{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"<p>This site contains the documentation for the <code>behavysis</code> program.</p>"},{"location":"examples/analysis.html","title":"Analysing a Folder of Experiments","text":"<p>All outcomes for experiment processing is stored in csv files in the <code>proj_dir/diagnostics</code> folder. These files store the outcome and process description (i.e. error explanations) of all experiments.</p>"},{"location":"examples/analysis.html#loading-in-all-relevant-packages","title":"Loading in all relevant packages","text":"<pre><code>from behavysis import Project\nfrom behavysis.processes import *\n</code></pre>"},{"location":"examples/analysis.html#making-the-project-and-importing-all-experiments","title":"Making the project and importing all experiments","text":"<p>The directory path of the project must be specified and must contain the experiment files you wish to analyse in a particular folder structure.</p> <p>For more information on how to structure a project directory, please see setup.</p> <p>For more information on how a <code>Experiment</code> works, please see behavysis.pipeline.project.Project.</p> <pre><code># Defining the project's folder\nproj_dir = \"./project\"\n# Initialising the project\nproj = Project(proj_dir)\n# Importing all the experiments (from the project folder)\nproj.importExperiments()\n</code></pre>"},{"location":"examples/analysis.html#checking-all-imported-experiments","title":"Checking all imported experiments","text":"<p>To see all imported experiments, see the <code>proj_dir/diagnostics/importExperiments.csv</code> file that has been generated.</p>"},{"location":"examples/analysis.html#updating-the-configurations-for-all-experiments","title":"Updating the configurations for all experiments","text":"<p>If you would like the configurations (which are stored in config files) to be updated new parameters, define the JSON style of configuration parameters you would like to add and run the following lines.</p> <p>For more information about how a configurations file works, please see here.</p> <pre><code># Defining the default configs json path\nconfigs_fp = \"path/to/default_configs.json\"\n# Overwriting the configs\nproj.update_configs(\n    configs_fp,\n    overwrite=\"user\",\n)\n</code></pre>"},{"location":"examples/analysis.html#get-animal-keypoints-in-videos","title":"Get Animal Keypoints in Videos","text":"<p>The following code processes and analyses all experiments that have been imported into a project. This is similar to analysing a single experiment.</p>"},{"location":"examples/analysis.html#downsample-videos","title":"Downsample videos","text":"<p>Formatting the raw mp4 videos so it can be fed through the DLC pose estimation algorithm.</p> <pre><code>proj.format_vid(\n    (\n        FormatVid.format_vid,\n        FormatVid.get_vid_metadata,\n    ),\n    overwrite=True,\n)\n</code></pre>"},{"location":"examples/analysis.html#run-keypoints-detection-deeplabcut","title":"Run Keypoints detection (DeepLabCut)","text":"<p>Running the DLC pose estimation algorithm on the formatted mp4 files.</p> <p>Note</p> <p>Make sure to change the <code>user.run_dlc.model_fp</code> to the DeepLabCut model's config file you'd like to use.</p> <pre><code>proj.run_dlc(\n    gputouse=None,\n    overwrite=True,\n)\n</code></pre>"},{"location":"examples/analysis.html#calculating-inherent-parameters-from-keypoints","title":"Calculating Inherent Parameters from Keypoints","text":"<p>Calculating relevant parameters to store in the <code>auto</code> section of the config file. The calculations performed are:</p> <pre><code>proj.calculate_params(\n    (\n        CalculateParams.start_frame,\n        CalculateParams.stop_frame,\n        CalculateParams.exp_dur,\n        CalculateParams.px_per_mm,\n    )\n)\n</code></pre> <p>And see a collation of all experiments' inherent parameters to spot any anomolies before continuing</p> <pre><code>proj.collate_configs_auto()\n</code></pre>"},{"location":"examples/analysis.html#postprocessing","title":"Postprocessing","text":"<p>Preprocessing the DLC csv data and output the preprocessed data to a <code>preprocessed_csv.&lt;exp_name&gt;.csv</code> file. The preprocessings performed are:</p> <pre><code>proj.preprocess(\n    (\n        Preprocess.start_stop_trim,\n        Preprocess.interpolate,\n        Preprocess.refine_ids,\n    ),\n    overwrite=overwrite,\n)\n</code></pre>"},{"location":"examples/analysis.html#make-simple-analysis","title":"Make Simple Analysis","text":"<p>Analysing the preprocessed csv data to extract useful analysis and results. The analyses performed are:</p> <pre><code>proj.analyse(\n    (\n        Analyse.thigmotaxis,\n        Analyse.center_crossing,\n        Analyse.in_roi,\n        Analyse.speed,\n        Analyse.social_distance,\n        Analyse.freezing,\n    )\n)\n</code></pre>"},{"location":"examples/analysis.html#automated-behaviour-detection","title":"Automated Behaviour Detection","text":""},{"location":"examples/analysis.html#extracting-features","title":"Extracting Features","text":"<p>Extracting derivative features from keypoints. For example - speed, bounding ellipse size, distance between points, etc.</p> <pre><code>proj.extract_features(overwrite)\n</code></pre>"},{"location":"examples/analysis.html#running-behaviour-classifiers","title":"Running Behaviour Classifiers","text":"<p>Note</p> <p>Make sure to change the <code>user.classify_behaviours</code> list to the behaviours classifiers you'd like to use.</p> <pre><code>proj.classify_behaviours(overwrite)\n</code></pre>"},{"location":"examples/analysis.html#exporting-the-behaviour-detection-results","title":"Exporting the Behaviour Detection Results","text":"<p>Exports to such a format, where a) <code>behavysis_viewer</code> can load it and perform semi-automated analysis, and b) after semi-automated verification, can be used to make a new/improve a current behaviour classifier (with behavysis.behav_classifier.behav_classifier.BehavClassifier)</p> <pre><code>proj.export_behaviours(overwrite)\n</code></pre>"},{"location":"examples/analysis.html#analyse-behaviours","title":"Analyse Behaviours","text":"<p>Similar to simple analysis, which calculates each experiment's a) the overall summary, and b) binned summary.</p> <pre><code>proj.behav_analyse()\n</code></pre>"},{"location":"examples/analysis.html#export-any-tables","title":"Export any Tables","text":"<p>Tables are stored as <code>.feather</code> files.</p> <p>To export these to csv files, run the following:</p> <pre><code>proj.export_feather(\"7_scored_behavs\", \"/path/to/csv_out\")\n</code></pre>"},{"location":"examples/analysis.html#evaluate","title":"Evaluate","text":"<p>Evaluates keypoints and behaviours accuracy by making annotated experiment videos.</p> <pre><code>proj.evaluate(\n    (\n        Evaluate.eval_vid,\n        Evaluate.keypoints_plot,\n    ),\n    overwrite=overwrite,\n)\n</code></pre>"},{"location":"examples/train.html","title":"Training a Behaviour Classifier","text":""},{"location":"examples/train.html#loading-in-all-relevant-packages","title":"Loading in all relevant packages","text":"<pre><code>import os\n\nfrom behavysis.mixins.behav_mixin import BehavMixin\nfrom behavysis.behav_classifier import BehavClassifier\nfrom behavysis.behav_classifier.clf_models.clf_templates import ClfTemplates\nfrom behavysis.pipeline import Project\n\nif __name__ == \"__main__\":\n    root_dir = \".\"\n    overwrite = True\n\n    # Option 1: From BORIS\n    # Define behaviours in BORIS\n    behavs_ls = [\"potential huddling\", \"huddling\"]\n    # Paths\n    configs_dir = os.path.join(root_dir, \"0_configs\")\n    boris_dir = os.path.join(root_dir, \"boris\")\n    dst_dir = os.path.join(root_dir, \"7_scored_behavs\")\n    # Getting names of all files\n    names = [os.path.splitext(i)[0] for i in os.listdir(boris_dir)]\n    for name in names:\n        # Paths\n        boris_fp = os.path.join(boris_dir, f\"{name}.tsv\")\n        configs_fp = os.path.join(configs_dir, f\"{name}.json\")\n        dst_fp = os.path.join(dst_dir, f\"{name}.parquet\")\n        # Making df from BORIS\n        df = BehavMixin.import_boris_tsv(boris_fp, configs_fp, behavs_ls)\n        # Saving df\n        df.to_feather(dst_fp)\n    # Making BehavClassifier objects\n    for behav in behavs_ls:\n        BehavClassifier.create_new_model(os.path.join(root_dir, \"behav_models\"), behav)\n\n    # Option 2: From previous behavysis project\n    proj = Project(root_dir)\n    proj.import_experiments()\n    # Making BehavClassifier objects\n    BehavClassifier.create_from_project(proj)\n\n    # Loading a BehavModel\n    behav = \"fight\"\n    model = BehavClassifier.load(\n        os.path.join(root_dir, \"behav_models\", f\"{behav}.json\")\n    )\n    # Testing all different classifiers\n    model.clf_eval_compare_all()\n    # MANUALLY LOOK AT THE BEST CLASSIFIER AND SELECT\n    # Example\n    model.pipeline_build(ClfTemplates.dnn_1)\n</code></pre>"},{"location":"installation/installing.html","title":"Installing","text":"<p>Step 1:</p> <p>Install conda by visiting the Miniconda downloads page and following the prompts to install on your system.</p> <p>Open the downloaded miniconda file and follow the installation prompts.</p> <p>Step 2:</p> <p>Open a terminal (Mac or Linux) or Anaconda PowerShell Prompt (Windows) and verify that conda has been installed with the following command.</p> <pre><code>conda --version\n</code></pre> <p>A response like <code>conda xx.xx.xx</code> indicates that it has been correctly installed.</p> <p>Step 3:</p> <p>Update conda and use the libmamba solver (makes downloading conda programs MUCH faster):</p> <pre><code>conda update -n base conda\nconda install -n base conda-libmamba-solver\nconda config --set solver libmamba\n</code></pre> <p>Step 4:</p> <p>Install packages that help Jupyter notebooks read conda environments:</p> <pre><code>conda install -n base nb_conda nb_conda_kernels\n</code></pre> <p>Step 5:</p> <p>Install the <code>behavysis</code> conda environment (download here).</p> <pre><code>conda env create -f path/to/conda_env.yaml\n</code></pre> <p>Step 6:</p> <p>Install the <code>DEEPLABCUT</code> conda environment (download here).</p> <pre><code>conda env create -f path/to/DEEPLABCUT.yaml\n</code></pre> <p>Step 7:</p> <p>Install the <code>simba</code> conda environment (download here).</p> <pre><code>conda env create -f path/to/simba_env.yaml\n</code></pre>"},{"location":"installation/running.html","title":"Running","text":"<p>Step 1:</p> <p>Open a terminal (Mac or Linux) or Anaconda PowerShell Prompt (Windows)</p> <p>Step 2:</p> <p>Activate the program environment with the following command:</p> <pre><code>conda activate behavysis\n</code></pre> <p>Step 3:</p> <p>You can now use the <code>behavysis</code> package in a Jupyter kernel or regular Python script.</p> <p>See here for examples of Jupyter notebooks to run behaviour analysis.</p> <p>See here for examples of Jupyter notebooks to train behaviour classifiers.</p> <p>See here for a tutorial of <code>behavysis</code>'s workflow.</p> <p>See here for API documentation.</p> <p>Note</p> <p>To run jupyter, run the following command in the terminal</p> <pre><code>jupyter-lab\n</code></pre> <p>This will open a browser to <code>http://127.0.0.1:8888/lab</code>, where you can run jupyter notebooks.</p> <p>You can also run jupyter notebooks in VS Code.</p>"},{"location":"installation/uninstalling.html","title":"Uninstalling","text":"<p>For more information about how to uninstall conda, see here.</p> <p>Step 1:</p> <p>Open a terminal (Mac or Linux) or Anaconda PowerShell Prompt (Windows)</p> <p>Step 2:</p> <p>To uninstall the <code>behavysis</code>, <code>DEEPLABCUT</code>, and <code>simba</code> conda envs, run the following commands:</p> <pre><code>conda env remove -n behavysis\nconda env remove -n DEEPLABCUT\nconda env remove -n simba\n</code></pre> <p>Step 3:</p> <p>To remove conda, enter the following commands in the terminal.</p> <pre><code>conda install anaconda-clean\nanaconda-clean --yes\n\nrm -rf ~/anaconda3\nrm -rf ~/opt/anaconda3\nrm -rf ~/.anaconda_backup\n</code></pre> <p>Step 5: Edit your bash or zsh profile so conda it does not look for conda anymore. Open each of these files (note that not all of them may exist on your computer), <code>~/.zshrc</code>, <code>~/.zprofile</code>, or <code>~/.bash_profile</code>, with the following command.</p> <pre><code>open ~/.zshrc\nopen ~/.zprofile\nopen ~/.bash_profile\n</code></pre>"},{"location":"installation/updating.html","title":"Updating","text":"<p>Step 1: Download the <code>conda_env.yaml</code> file from here</p> <p>Step 2: Run the following command to update <code>behavysis</code>:</p> <pre><code>conda env update -f conda_env.yaml --prune\n</code></pre>"},{"location":"reference/behav_classifier.html","title":"Behav classifier","text":""},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier","title":"<code>behavysis.behav_classifier.behav_classifier.BehavClassifier</code>","text":"<p>BehavClassifier abstract class peforms behav classifier model preparation, training, saving, evaluation, and inference.</p> Source code in <code>behavysis/behav_classifier/behav_classifier.py</code> <pre><code>class BehavClassifier:\n    \"\"\"\n    BehavClassifier abstract class peforms behav classifier model preparation, training, saving,\n    evaluation, and inference.\n    \"\"\"\n\n    logger = init_logger_file()\n\n    _proj_dir: str\n    _behav_name: str\n    _clf: BaseTorchModel\n\n    def __init__(self, proj_dir: str, behav_name: str) -&gt; None:\n        # Setting attributes\n        self._proj_dir = os.path.abspath(proj_dir)\n        self._behav_name = behav_name\n        # Assert that the behaviour is scored in the project (in the scored_behavs directory)\n        # Getting the list of behaviours in project to check against\n        y_df = self.wrangle_columns_y(self.combine_dfs(self.y_dir))\n        assert np.isin(behav_name, y_df.columns)\n        # Trying to load configs (or making new)\n        try:\n            configs = BehavClassifierConfigs.read_json(self.configs_fp)\n            self.logger.debug(\"Loaded existing configs\")\n        except FileNotFoundError:\n            configs = BehavClassifierConfigs()\n            self.logger.debug(\"Made new model configs\")\n        # Setting and saving configs\n        configs.proj_dir = self._proj_dir\n        configs.behav_name = self._behav_name\n        self.configs = configs\n        # Trying to load classifier (or making new)\n        try:\n            self.clf = joblib_load(self.clf_fp)\n            self.logger.debug(\"Loaded existing classifier\")\n        except FileNotFoundError:\n            self.clf = CNN1()\n            self.logger.debug(\"Made new classifier\")\n\n    #################################################\n    #            GETTER AND SETTERS\n    #################################################\n\n    @property\n    def proj_dir(self) -&gt; str:\n        return self._proj_dir\n\n    @property\n    def behav_name(self) -&gt; str:\n        return self._behav_name\n\n    @property\n    def clf(self) -&gt; BaseTorchModel:\n        return self._clf\n\n    @clf.setter\n    def clf(self, clf: BaseTorchModel | str) -&gt; None:\n        # If a str, then loading\n        if isinstance(clf, str):\n            clf_name = clf\n            self._clf = joblib_load(os.path.join(self.clfs_dir, clf, \"classifier.sav\"))\n            self.logger.debug(f\"Loaded classifier: {clf_name}\")\n        # If a BaseTorchModel, then setting\n        else:\n            clf_name = type(clf).__name__\n            self._clf = clf\n            self.logger.debug(f\"Initialised classifier: {clf_name}\")\n        # Updating in configs\n        configs = self.configs\n        configs.clf_struct = clf_name\n        self.configs = configs\n\n    @property\n    def model_dir(self) -&gt; str:\n        return os.path.join(self.proj_dir, \"behav_models\", self.behav_name)\n\n    @property\n    def configs_fp(self) -&gt; str:\n        return os.path.join(self.model_dir, \"configs.json\")\n\n    @property\n    def configs(self) -&gt; BehavClassifierConfigs:\n        return BehavClassifierConfigs.read_json(self.configs_fp)\n\n    @configs.setter\n    def configs(self, configs: BehavClassifierConfigs) -&gt; None:\n        try:\n            if self.configs == configs:\n                return\n        except FileNotFoundError:\n            pass\n        self.logger.debug(\"Configs have changed. Updating model configs on disk\")\n        configs.write_json(self.configs_fp)\n\n    @property\n    def clfs_dir(self) -&gt; str:\n        return os.path.join(self.model_dir, \"classifiers\")\n\n    @property\n    def clf_dir(self) -&gt; str:\n        return os.path.join(self.clfs_dir, self.configs.clf_struct)\n\n    @property\n    def clf_fp(self) -&gt; str:\n        return os.path.join(self.clf_dir, \"classifier.sav\")\n\n    @property\n    def preproc_fp(self) -&gt; str:\n        return os.path.join(self.clf_dir, \"preproc.sav\")\n\n    @property\n    def eval_dir(self) -&gt; str:\n        return os.path.join(self.clf_dir, \"evaluation\")\n\n    @property\n    def x_dir(self) -&gt; str:\n        \"\"\"\n        Returns the model's x directory.\n        It gets the features_extracted directory from the parent Behavysis model directory.\n        \"\"\"\n        return os.path.join(self.proj_dir, Folders.FEATURES_EXTRACTED.value)\n\n    @property\n    def y_dir(self) -&gt; str:\n        \"\"\"\n        Returns the model's y directory.\n        It gets the scored_behavs directory from the parent Behavysis model directory.\n        \"\"\"\n        return os.path.join(self.proj_dir, Folders.SCORED_BEHAVS.value)\n\n    #################################################\n    # CREATE/LOAD MODEL METHODS\n    #################################################\n\n    @classmethod\n    def create_from_project_dir(cls, proj_dir: str) -&gt; list:\n        \"\"\"\n        Loading classifier from given Behavysis project directory.\n        \"\"\"\n        # Getting the list of behaviours (after wrangling column names)\n        y_df = cls.wrangle_columns_y(cls.combine_dfs(os.path.join(proj_dir, Folders.SCORED_BEHAVS.value)))\n        behavs_ls = y_df.columns.to_list()\n        # For each behaviour, making a new BehavClassifier instance\n        models_ls = [cls(proj_dir, behav) for behav in behavs_ls]\n        return models_ls\n\n    @classmethod\n    def create_from_project(cls, proj: Project) -&gt; list[BehavClassifier]:\n        \"\"\"\n        Loading classifier from given Behavysis project instance.\n        Wraps the `create_from_project_dir` method.\n        \"\"\"\n        return cls.create_from_project_dir(proj.root_dir)\n\n    @classmethod\n    def load(cls, proj_dir: str, behav_name: str) -&gt; BehavClassifier:\n        \"\"\"\n        Reads the model from the expected model file.\n        \"\"\"\n        # Checking that the configs file exists and is valid\n        configs_fp = os.path.join(proj_dir, \"behav_models\", behav_name, \"configs.json\")\n        try:\n            BehavClassifierConfigs.read_json(configs_fp)\n        except (FileNotFoundError, OSError):\n            raise ValueError(\n                f'Model in project directory, \"{proj_dir}\", and behav name, \"{behav_name}\", not found.\\n'\n                \"Please check file path.\"\n            )\n        return cls(proj_dir, behav_name)\n\n    ###############################################################################################\n    #            COMBINING DFS TO SINGLE DF\n    ###############################################################################################\n\n    @classmethod\n    def combine_dfs(cls, src_dir):\n        \"\"\"\n        Combines the data in the given directory into a single dataframe.\n        Adds a MultiIndex level to the rows, with the values as the filenames in the directory.\n        \"\"\"\n        data_dict = {get_name(i): DFMixin.read(os.path.join(src_dir, i)) for i in os.listdir(os.path.join(src_dir))}\n        df = pd.concat(data_dict.values(), axis=0, keys=data_dict.keys())\n        df = BehavClassifierCombinedDf.basic_clean(df)\n        return df\n\n    ###############################################################################################\n    #            PREPROCESSING DFS\n    ###############################################################################################\n\n    @staticmethod\n    def _preproc_x_fit_select_cols(x: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Selects only the derived features (not the x-y-l columns).\n\n        Used in the preprocessing pipeline.\n        \"\"\"\n        return x[:, 48:]\n\n    @classmethod\n    def preproc_x_fit(cls, x: np.ndarray, preproc_fp: str) -&gt; None:\n        \"\"\"\n        The preprocessing steps are:\n        - Select only the derived features (not the x-y-l columns)\n            - 2 (indivs) * 8 (bpts) * 3 (coords) = 48 (columns) before derived features\n        - MinMax scaling (using previously fitted MinMaxScaler)\n        \"\"\"\n        preproc_pipe = Pipeline(\n            steps=[\n                (\"select_columns\", FunctionTransformer(cls._preproc_x_fit_select_cols)),\n                (\"min_max_scaler\", MinMaxScaler()),\n            ]\n        )\n        preproc_pipe.fit(x)\n        joblib_dump(preproc_pipe, preproc_fp)\n\n    @classmethod\n    def preproc_x_transform(cls, x: np.ndarray, preproc_fp: str) -&gt; np.ndarray:\n        \"\"\"\n        Runs the preprocessing steps fitted from `preproc_x_fit` on the given `x` data.\n        \"\"\"\n        preproc_pipe: Pipeline = joblib_load(preproc_fp)\n        x_preproc = preproc_pipe.transform(x)\n        return x_preproc\n\n    @classmethod\n    def wrangle_columns_y(cls, y: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Filters the `y` dataframe to only include the `behav` column and the specific outcome columns,\n        and rename the columns to be in the format `{behav}__{outcome}`.\n        \"\"\"\n        # Filtering out the pred columns (in the `outcomes` level)\n        columns_filter = np.isin(\n            y.columns.get_level_values(BehavScoredDf.CN.OUTCOMES.value),\n            [BehavScoredDf.OutcomesCols.PRED.value],\n            invert=True,\n        )\n        y = y.loc[:, columns_filter]\n        # Setting the column names from `(behav, outcome)` to `{behav}__{outcome}`\n        y.columns = [\n            f\"{behav_name}\"\n            if outcome_name == BehavScoredDf.OutcomesCols.ACTUAL.value\n            else f\"{behav_name}__{outcome_name}\"\n            for behav_name, outcome_name in y.columns\n        ]\n        return y\n\n    @classmethod\n    def oversample(cls, x: np.ndarray, y: np.ndarray, ratio: float) -&gt; np.ndarray:\n        assert x.shape[0] == y.shape[0]\n        # Getting index\n        index = np.arange(y.shape[0])\n        # Getting indices where y is True\n        t = index[y == BehavValues.BEHAV.value]\n        # Getting indices where y is False\n        f = index[y == BehavValues.NON_BEHAV.value]\n        # Getting intended size (as t_len / f_len = ratio)\n        new_t_size = int(np.round(f.shape[0] * ratio))\n        # Oversampling the True indices\n        t = np.random.choice(t, size=new_t_size, replace=True)\n        # Combining the True and False indices\n        new_index = np.concatenate([t, f])\n        # Returning the resampled x\n        return x[new_index]\n\n    @classmethod\n    def undersample(cls, x: np.ndarray, y: np.ndarray, ratio: float) -&gt; np.ndarray:\n        assert x.shape[0] == y.shape[0]\n        # Getting index\n        index = np.arange(y.shape[0])\n        # Getting indices where y is True\n        t = index[y == BehavValues.BEHAV.value]\n        # Getting indices where y is False\n        f = index[y == BehavValues.NON_BEHAV.value]\n        # Getting intended size (as t_len / f_len = ratio)\n        new_f_size = int(np.round(t.shape[0] / ratio))\n        # Undersampling the False indices\n        f = np.random.choice(f, size=new_f_size, replace=False)\n        # Combining the True and False indices\n        new_index = np.concatenate([t, f])\n        # Returning the resampled x\n        return x[new_index]\n\n    #################################################\n    #            PIPELINE FOR DATA PREP\n    #################################################\n\n    def preproc_training(\n        self,\n    ) -&gt; tuple[list[np.ndarray], list[np.ndarray], list[np.ndarray], list[np.ndarray]]:\n        \"\"\"\n        Prepares the data for the training pipeline.\n\n        Performs the following:\n        - Combining dfs from x and y directories (individual experiment data).\n        - Ensures the x and y dfs have the same index, and are in the same row order.\n        - Preprocesses x df. Refer to `preprocess_x` for details.\n        - Selects the y class (given in the configs file) from the y df.\n        - Preprocesses y df. Refer to `preprocess_y` for details.\n        - Splits into training and test indexes.\n            - The training indexes are undersampled to the ratio given in the configs.\n\n        Returns\n        -------\n        A tuple containing four numpy arrays:\n        - x_ls: list of each dataframe's input data.\n        - y_ls: list of each dataframe's target labels.\n        - index_train_ls: list of each dataframe's indexes for the training data.\n        - index_test_ls: list of each dataframe's indexes for the testing data.\n        \"\"\"\n        # Getting the lists of x and y dfs\n        x_fp_ls = [os.path.join(self.x_dir, i) for i in os.listdir(os.path.join(self.x_dir))]\n        y_fp_ls = [os.path.join(self.y_dir, i) for i in os.listdir(os.path.join(self.y_dir))]\n        x_df_ls = async_read_files_run(x_fp_ls, FeaturesDf.read)\n        y_df_ls = async_read_files_run(y_fp_ls, BehavScoredDf.read)\n        # Formatting y dfs (selecting column and replacing UNDETERMINED with NON_BEHAV values)\n        y_df_ls = [\n            y[(self.configs.behav_name, BehavScoredDf.OutcomesCols.ACTUAL.value)].replace(\n                BehavValues.UNDETERMINED.value, BehavValues.NON_BEHAV.value\n            )\n            for y in y_df_ls\n        ]\n        # Ensuring x and y dfs have the same index and are in the same row order\n        index_df_ls = [x.index.intersection(y.index) for x, y in zip(x_df_ls, y_df_ls)]\n        x_df_ls = [x.loc[index] for x, index in zip(x_df_ls, index_df_ls)]\n        y_df_ls = [y.loc[index] for y, index in zip(y_df_ls, index_df_ls)]\n        assert np.all([x.shape[0] == y.shape[0] for x, y in zip(x_df_ls, y_df_ls)])\n        # Converting to numpy arrays\n        x_ls = [x.values for x in x_df_ls]\n        y_ls = [y.values for y in y_df_ls]\n        index_ls = [np.arange(x.shape[0]) for x in x_ls]\n        # x preprocessing: fitting (across all x dfs) and transforming (for each x df)\n        self.preproc_x_fit(np.concatenate(x_ls, axis=0), self.preproc_fp)\n        x_ls = [self.preproc_x_transform(x, self.preproc_fp) for x in x_ls]\n        # Making a 2D array of (df_index, index, y) for train-test splitting, stratifying and sampling\n        index_flat = listofvects2array(index_ls, y_ls)\n        # Splitting into train and test indexes\n        index_train_flat, index_test_flat = train_test_split(\n            index_flat,\n            test_size=self.configs.test_split,\n            stratify=index_flat[:, 2],\n        )\n        # Oversampling and undersampling ONLY on training data\n        index_train_flat = self.oversample(index_train_flat, index_train_flat[:, 2], self.configs.oversample_ratio)\n        index_train_flat = self.undersample(index_train_flat, index_train_flat[:, 2], self.configs.undersample_ratio)\n        # Reshaping back to individual df index lists\n        index_train_ls = array2listofvect(index_train_flat, 1)\n        index_test_ls = array2listofvect(index_test_flat, 1)\n        return x_ls, y_ls, index_train_ls, index_test_ls\n\n    #################################################\n    # PIPELINE FOR CLASSIFIER TRAINING AND INFERENCE\n    #################################################\n\n    def pipeline_training(self) -&gt; None:\n        \"\"\"\n        Makes a classifier and saves it to the model's root directory.\n\n        Callable is a method from `ClfTemplates`.\n        \"\"\"\n        self.logger.info(f\"Training {self.configs.clf_struct}\")\n        # Preparing data\n        x_ls, y_ls, index_train_ls, index_test_ls = self.preproc_training()\n        # Training the model\n        history = self.clf.fit(\n            x_ls=x_ls,\n            y_ls=y_ls,\n            index_ls=index_train_ls,\n            batch_size=self.configs.batch_size,\n            epochs=self.configs.epochs,\n            val_split=self.configs.val_split,\n        )\n        # Saving history\n        self.clf_eval_save_history(history)\n        # Evaluating on train and test data\n        self.clf_eval_save_performance(x_ls, y_ls, index_train_ls, \"train\")\n        self.clf_eval_save_performance(x_ls, y_ls, index_test_ls, \"test\")\n        # Saving model\n        joblib_dump(self.clf, self.clf_fp)\n\n    def pipeline_training_all(self):\n        \"\"\"\n        Making classifier for all available templates.\n        \"\"\"\n        # Saving existing clf\n        clf = self.clf\n        for clf_cls in CLF_TEMPLATES:\n            # Initialising the model\n            self.clf = clf_cls()\n            # Building pipeline, which runs and saves evaluation\n            self.pipeline_training()\n        # Restoring clf\n        self.clf = clf\n\n    def pipeline_inference(self, x_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Given the unprocessed features dataframe, runs the model pipeline to make predictions.\n\n        Pipeline is:\n        - Preprocess `x` df. Refer to\n        [behavysis.behav_classifier.BehavClassifier.preproc_x][] for details.\n        - Makes predictions and returns the predicted behaviours.\n        \"\"\"\n        index = x_df.index\n        # Preprocessing features\n        x = self.preproc_x_transform(x_df.values, self.preproc_fp)\n        # Loading the model\n        self.clf = joblib_load(self.clf_fp)\n        # Getting probabilities\n        y_prob = self.clf.predict(\n            x=x,\n            index=np.arange(x.shape[0]),\n            batch_size=self.configs.batch_size,\n        )\n        # Making predictions from probabilities (and pcutoff)\n        y_pred = (y_prob &gt; self.configs.pcutoff).astype(int)\n        # Making df\n        pred_df = BehavPredictedDf.init_df(pd.Series(index))\n        pred_df[(self.configs.behav_name, BehavPredictedDf.OutcomesCols.PROB.value)] = y_prob\n        pred_df[(self.configs.behav_name, BehavPredictedDf.OutcomesCols.PRED.value)] = y_pred\n        return pred_df\n\n    #################################################\n    # COMPREHENSIVE EVALUATION FUNCTIONS\n    #################################################\n\n    def clf_eval_save_history(self, history: pd.DataFrame):\n        # Saving history df\n        DFMixin.write(history, os.path.join(self.eval_dir, f\"history.{DFMixin.IO}\"))\n        # Making and saving history figure\n        fig, ax = plt.subplots(figsize=(10, 7))\n        sns.lineplot(data=history, ax=ax)\n        fig.savefig(os.path.join(self.eval_dir, \"history.png\"))\n\n    def clf_eval_save_performance(\n        self,\n        x_ls: list[np.ndarray],\n        y_ls: list[np.ndarray],\n        index_ls: list[np.ndarray],\n        name: str,\n    ) -&gt; tuple[pd.DataFrame, dict, Figure, Figure, Figure]:\n        \"\"\"\n        Evaluates the classifier performance on the given x and y data.\n        Saves the `metrics_fig` and `pcutoffs_fig` to the model's root directory.\n\n        Returns\n        -------\n        y_eval : pd.DataFrame\n            Predicted behaviour classifications against the true labels.\n        metrics_fig : mpl.Figure\n            Figure showing the confusion matrix.\n        pcutoffs_fig : mpl.Figure\n            Figure showing the precision, recall, f1, and accuracy for different pcutoffs.\n        logc_fig : mpl.Figure\n            Figure showing the logistic curve for different predicted probabilities.\n        \"\"\"\n        # Getting predictions\n        y_true_ls = [y[index] for y, index in zip(y_ls, index_ls)]\n        y_prob_ls = [\n            self.clf.predict(x=x, index=index, batch_size=self.configs.batch_size) for x, index in zip(x_ls, index_ls)\n        ]\n        # Making eval vects\n        y_true = np.concatenate(y_true_ls)\n        y_prob = np.concatenate(y_prob_ls)\n        y_pred = (y_prob &gt; self.configs.pcutoff).astype(int)\n        # Making eval_df\n        eval_df = BehavPredictedDf.init_df(pd.Series(np.arange(np.concatenate(index_ls).shape[0])))\n        eval_df[(self.configs.behav_name, BehavPredictedDf.OutcomesCols.PROB.value)] = y_prob\n        eval_df[(self.configs.behav_name, BehavPredictedDf.OutcomesCols.PRED.value)] = y_pred\n        eval_df[(self.configs.behav_name, BehavScoredDf.OutcomesCols.ACTUAL.value)] = y_true\n        # Making classification report\n        report_dict = self.eval_report(y_true, y_pred)\n        # Making confusion matrix figure\n        metrics_fig = self.eval_conf_matr(y_true, y_pred)\n        # Making performance for different pcutoffs figure\n        pcutoffs_fig = self.eval_metrics_pcutoffs(y_true, y_prob)\n        # Logistic curve\n        logc_fig = self.eval_logc(y_true, y_prob)\n        # Saving data and figures\n        BehavClassifierEvalDf.write(eval_df, os.path.join(self.eval_dir, f\"{name}_eval.{BehavClassifierEvalDf.IO}\"))\n        write_json(os.path.join(self.eval_dir, f\"{name}_report.json\"), report_dict)\n        metrics_fig.savefig(os.path.join(self.eval_dir, f\"{name}_confm.png\"))\n        pcutoffs_fig.savefig(os.path.join(self.eval_dir, f\"{name}_pcutoffs.png\"))\n        logc_fig.savefig(os.path.join(self.eval_dir, f\"{name}_logc.png\"))\n        return eval_df, report_dict, metrics_fig, pcutoffs_fig, logc_fig\n\n    #################################################\n    # EVALUATION METRICS FUNCTIONS\n    #################################################\n\n    @classmethod\n    def eval_report(cls, y_true: np.ndarray, y_pred: np.ndarray) -&gt; dict:\n        \"\"\"\n        __summary__\n        \"\"\"\n        return classification_report(\n            y_true=y_true,\n            y_pred=y_pred,\n            target_names=enum2tuple(GenericBehavLabels),\n            output_dict=True,\n        )  # type: ignore\n\n    @classmethod\n    def eval_conf_matr(cls, y_true: np.ndarray, y_pred: np.ndarray) -&gt; Figure:\n        \"\"\"\n        __summary__\n        \"\"\"\n        # Making confusion matrix\n        fig, ax = plt.subplots(figsize=(7, 7))\n        sns.heatmap(\n            confusion_matrix(y_true, y_pred),\n            annot=True,\n            fmt=\"d\",\n            cmap=\"viridis\",\n            cbar=False,\n            xticklabels=enum2tuple(GenericBehavLabels),\n            yticklabels=enum2tuple(GenericBehavLabels),\n            ax=ax,\n        )\n        ax.set_xlabel(\"Predicted\")\n        ax.set_ylabel(\"True\")\n        return fig\n\n    @classmethod\n    def eval_metrics_pcutoffs(cls, y_true: np.ndarray, y_prob: np.ndarray) -&gt; Figure:\n        \"\"\"\n        __summary__\n        \"\"\"\n        # Getting precision, recall and accuracy for different cutoffs\n        pcutoffs = np.linspace(0, 1, 101)\n        # Measures\n        precisions = np.zeros(pcutoffs.shape[0])\n        recalls = np.zeros(pcutoffs.shape[0])\n        f1 = np.zeros(pcutoffs.shape[0])\n        accuracies = np.zeros(pcutoffs.shape[0])\n        for i, pcutoff in enumerate(pcutoffs):\n            y_pred = y_prob &gt; pcutoff\n            report = classification_report(\n                y_true,\n                y_pred,\n                target_names=enum2tuple(GenericBehavLabels),\n                output_dict=True,\n            )\n            precisions[i] = report[GenericBehavLabels.BEHAV.value][\"precision\"]  # type: ignore\n            recalls[i] = report[GenericBehavLabels.BEHAV.value][\"recall\"]  # type: ignore\n            f1[i] = report[GenericBehavLabels.BEHAV.value][\"f1-score\"]  # type: ignore\n            accuracies[i] = report[\"accuracy\"]  # type: ignore\n        # Making figure\n        fig, ax = plt.subplots(figsize=(10, 7))\n        sns.lineplot(x=pcutoffs, y=precisions, label=\"precision\", ax=ax)\n        sns.lineplot(x=pcutoffs, y=recalls, label=\"recall\", ax=ax)\n        sns.lineplot(x=pcutoffs, y=f1, label=\"f1\", ax=ax)\n        sns.lineplot(x=pcutoffs, y=accuracies, label=\"accuracy\", ax=ax)\n        return fig\n\n    @classmethod\n    def eval_logc(cls, y_true: np.ndarray, y_prob: np.ndarray) -&gt; Figure:\n        \"\"\"\n        __summary__\n        \"\"\"\n        y_eval = pd.DataFrame(\n            {\n                \"y_true\": y_true,\n                \"y_prob\": y_prob,\n                \"y_pred\": y_prob &gt; 0.4,\n                \"y_true_jitter\": y_true + (0.2 * (np.random.rand(len(y_prob)) - 0.5)),\n            }\n        )\n        fig, ax = plt.subplots(figsize=(10, 7))\n        sns.scatterplot(\n            data=y_eval,\n            x=\"y_prob\",\n            y=\"y_true_jitter\",\n            marker=\".\",\n            s=10,\n            linewidth=0,\n            alpha=0.2,\n            ax=ax,\n        )\n        # Making line of ratio of y_true outcomes for each y_prob\n        pcutoffs = np.linspace(0, 1, 101)\n        ratios = np.vectorize(lambda i: np.mean(i &gt; y_eval[\"y_prob\"]))(pcutoffs)\n        sns.lineplot(x=pcutoffs, y=ratios, ax=ax)\n        return fig\n\n    @classmethod\n    def eval_bouts(cls, y_true: np.ndarray, y_pred: np.ndarray) -&gt; pd.DataFrame:\n        \"\"\"\n        __summary__\n        \"\"\"\n        y_eval = pd.DataFrame({\"y_true\": y_true, \"y_pred\": y_pred})\n        y_eval[\"ids\"] = np.cumsum(y_eval[\"y_true\"] != y_eval[\"y_true\"].shift())\n        # Getting the proportion of correct predictions for each bout\n        y_eval_grouped = y_eval.groupby(\"ids\")\n        y_eval_summary = pd.DataFrame(\n            y_eval_grouped.apply(lambda x: (x[\"y_pred\"] == x[\"y_true\"]).mean()),\n            columns=[\"proportion\"],\n        )\n        y_eval_summary[\"actual_bout\"] = y_eval_grouped.apply(lambda x: x[\"y_true\"].mean())\n        y_eval_summary[\"bout_len\"] = y_eval_grouped.apply(lambda x: x.shape[0])\n        y_eval_summary = y_eval_summary.sort_values(\"proportion\")\n        # # Making figure\n        # fig, ax = plt.subplots(figsize=(10, 7))\n        # sns.scatterplot(\n        #     data=y_eval_summary,\n        #     x=\"proportion\",\n        #     y=\"bout_len\",\n        #     hue=\"actual_bout\",\n        #     alpha=0.4,\n        #     marker=\".\",\n        #     s=50,\n        #     linewidth=0,\n        #     ax=ax,\n        # )\n        return y_eval_summary\n</code></pre>"},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier.x_dir","title":"<code>x_dir</code>  <code>property</code>","text":"<p>Returns the model's x directory. It gets the features_extracted directory from the parent Behavysis model directory.</p>"},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier.y_dir","title":"<code>y_dir</code>  <code>property</code>","text":"<p>Returns the model's y directory. It gets the scored_behavs directory from the parent Behavysis model directory.</p>"},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier._preproc_x_fit_select_cols","title":"<code>_preproc_x_fit_select_cols(x)</code>  <code>staticmethod</code>","text":"<p>Selects only the derived features (not the x-y-l columns).</p> <p>Used in the preprocessing pipeline.</p> Source code in <code>behavysis/behav_classifier/behav_classifier.py</code> <pre><code>@staticmethod\ndef _preproc_x_fit_select_cols(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Selects only the derived features (not the x-y-l columns).\n\n    Used in the preprocessing pipeline.\n    \"\"\"\n    return x[:, 48:]\n</code></pre>"},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier.clf_eval_save_performance","title":"<code>clf_eval_save_performance(x_ls, y_ls, index_ls, name)</code>","text":"<p>Evaluates the classifier performance on the given x and y data. Saves the <code>metrics_fig</code> and <code>pcutoffs_fig</code> to the model's root directory.</p> <p>Returns:</p> Name Type Description <code>y_eval</code> <code>DataFrame</code> <p>Predicted behaviour classifications against the true labels.</p> <code>metrics_fig</code> <code>Figure</code> <p>Figure showing the confusion matrix.</p> <code>pcutoffs_fig</code> <code>Figure</code> <p>Figure showing the precision, recall, f1, and accuracy for different pcutoffs.</p> <code>logc_fig</code> <code>Figure</code> <p>Figure showing the logistic curve for different predicted probabilities.</p> Source code in <code>behavysis/behav_classifier/behav_classifier.py</code> <pre><code>def clf_eval_save_performance(\n    self,\n    x_ls: list[np.ndarray],\n    y_ls: list[np.ndarray],\n    index_ls: list[np.ndarray],\n    name: str,\n) -&gt; tuple[pd.DataFrame, dict, Figure, Figure, Figure]:\n    \"\"\"\n    Evaluates the classifier performance on the given x and y data.\n    Saves the `metrics_fig` and `pcutoffs_fig` to the model's root directory.\n\n    Returns\n    -------\n    y_eval : pd.DataFrame\n        Predicted behaviour classifications against the true labels.\n    metrics_fig : mpl.Figure\n        Figure showing the confusion matrix.\n    pcutoffs_fig : mpl.Figure\n        Figure showing the precision, recall, f1, and accuracy for different pcutoffs.\n    logc_fig : mpl.Figure\n        Figure showing the logistic curve for different predicted probabilities.\n    \"\"\"\n    # Getting predictions\n    y_true_ls = [y[index] for y, index in zip(y_ls, index_ls)]\n    y_prob_ls = [\n        self.clf.predict(x=x, index=index, batch_size=self.configs.batch_size) for x, index in zip(x_ls, index_ls)\n    ]\n    # Making eval vects\n    y_true = np.concatenate(y_true_ls)\n    y_prob = np.concatenate(y_prob_ls)\n    y_pred = (y_prob &gt; self.configs.pcutoff).astype(int)\n    # Making eval_df\n    eval_df = BehavPredictedDf.init_df(pd.Series(np.arange(np.concatenate(index_ls).shape[0])))\n    eval_df[(self.configs.behav_name, BehavPredictedDf.OutcomesCols.PROB.value)] = y_prob\n    eval_df[(self.configs.behav_name, BehavPredictedDf.OutcomesCols.PRED.value)] = y_pred\n    eval_df[(self.configs.behav_name, BehavScoredDf.OutcomesCols.ACTUAL.value)] = y_true\n    # Making classification report\n    report_dict = self.eval_report(y_true, y_pred)\n    # Making confusion matrix figure\n    metrics_fig = self.eval_conf_matr(y_true, y_pred)\n    # Making performance for different pcutoffs figure\n    pcutoffs_fig = self.eval_metrics_pcutoffs(y_true, y_prob)\n    # Logistic curve\n    logc_fig = self.eval_logc(y_true, y_prob)\n    # Saving data and figures\n    BehavClassifierEvalDf.write(eval_df, os.path.join(self.eval_dir, f\"{name}_eval.{BehavClassifierEvalDf.IO}\"))\n    write_json(os.path.join(self.eval_dir, f\"{name}_report.json\"), report_dict)\n    metrics_fig.savefig(os.path.join(self.eval_dir, f\"{name}_confm.png\"))\n    pcutoffs_fig.savefig(os.path.join(self.eval_dir, f\"{name}_pcutoffs.png\"))\n    logc_fig.savefig(os.path.join(self.eval_dir, f\"{name}_logc.png\"))\n    return eval_df, report_dict, metrics_fig, pcutoffs_fig, logc_fig\n</code></pre>"},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier.combine_dfs","title":"<code>combine_dfs(src_dir)</code>  <code>classmethod</code>","text":"<p>Combines the data in the given directory into a single dataframe. Adds a MultiIndex level to the rows, with the values as the filenames in the directory.</p> Source code in <code>behavysis/behav_classifier/behav_classifier.py</code> <pre><code>@classmethod\ndef combine_dfs(cls, src_dir):\n    \"\"\"\n    Combines the data in the given directory into a single dataframe.\n    Adds a MultiIndex level to the rows, with the values as the filenames in the directory.\n    \"\"\"\n    data_dict = {get_name(i): DFMixin.read(os.path.join(src_dir, i)) for i in os.listdir(os.path.join(src_dir))}\n    df = pd.concat(data_dict.values(), axis=0, keys=data_dict.keys())\n    df = BehavClassifierCombinedDf.basic_clean(df)\n    return df\n</code></pre>"},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier.create_from_project","title":"<code>create_from_project(proj)</code>  <code>classmethod</code>","text":"<p>Loading classifier from given Behavysis project instance. Wraps the <code>create_from_project_dir</code> method.</p> Source code in <code>behavysis/behav_classifier/behav_classifier.py</code> <pre><code>@classmethod\ndef create_from_project(cls, proj: Project) -&gt; list[BehavClassifier]:\n    \"\"\"\n    Loading classifier from given Behavysis project instance.\n    Wraps the `create_from_project_dir` method.\n    \"\"\"\n    return cls.create_from_project_dir(proj.root_dir)\n</code></pre>"},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier.create_from_project_dir","title":"<code>create_from_project_dir(proj_dir)</code>  <code>classmethod</code>","text":"<p>Loading classifier from given Behavysis project directory.</p> Source code in <code>behavysis/behav_classifier/behav_classifier.py</code> <pre><code>@classmethod\ndef create_from_project_dir(cls, proj_dir: str) -&gt; list:\n    \"\"\"\n    Loading classifier from given Behavysis project directory.\n    \"\"\"\n    # Getting the list of behaviours (after wrangling column names)\n    y_df = cls.wrangle_columns_y(cls.combine_dfs(os.path.join(proj_dir, Folders.SCORED_BEHAVS.value)))\n    behavs_ls = y_df.columns.to_list()\n    # For each behaviour, making a new BehavClassifier instance\n    models_ls = [cls(proj_dir, behav) for behav in behavs_ls]\n    return models_ls\n</code></pre>"},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier.eval_bouts","title":"<code>eval_bouts(y_true, y_pred)</code>  <code>classmethod</code>","text":"<p>summary</p> Source code in <code>behavysis/behav_classifier/behav_classifier.py</code> <pre><code>@classmethod\ndef eval_bouts(cls, y_true: np.ndarray, y_pred: np.ndarray) -&gt; pd.DataFrame:\n    \"\"\"\n    __summary__\n    \"\"\"\n    y_eval = pd.DataFrame({\"y_true\": y_true, \"y_pred\": y_pred})\n    y_eval[\"ids\"] = np.cumsum(y_eval[\"y_true\"] != y_eval[\"y_true\"].shift())\n    # Getting the proportion of correct predictions for each bout\n    y_eval_grouped = y_eval.groupby(\"ids\")\n    y_eval_summary = pd.DataFrame(\n        y_eval_grouped.apply(lambda x: (x[\"y_pred\"] == x[\"y_true\"]).mean()),\n        columns=[\"proportion\"],\n    )\n    y_eval_summary[\"actual_bout\"] = y_eval_grouped.apply(lambda x: x[\"y_true\"].mean())\n    y_eval_summary[\"bout_len\"] = y_eval_grouped.apply(lambda x: x.shape[0])\n    y_eval_summary = y_eval_summary.sort_values(\"proportion\")\n    # # Making figure\n    # fig, ax = plt.subplots(figsize=(10, 7))\n    # sns.scatterplot(\n    #     data=y_eval_summary,\n    #     x=\"proportion\",\n    #     y=\"bout_len\",\n    #     hue=\"actual_bout\",\n    #     alpha=0.4,\n    #     marker=\".\",\n    #     s=50,\n    #     linewidth=0,\n    #     ax=ax,\n    # )\n    return y_eval_summary\n</code></pre>"},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier.eval_conf_matr","title":"<code>eval_conf_matr(y_true, y_pred)</code>  <code>classmethod</code>","text":"<p>summary</p> Source code in <code>behavysis/behav_classifier/behav_classifier.py</code> <pre><code>@classmethod\ndef eval_conf_matr(cls, y_true: np.ndarray, y_pred: np.ndarray) -&gt; Figure:\n    \"\"\"\n    __summary__\n    \"\"\"\n    # Making confusion matrix\n    fig, ax = plt.subplots(figsize=(7, 7))\n    sns.heatmap(\n        confusion_matrix(y_true, y_pred),\n        annot=True,\n        fmt=\"d\",\n        cmap=\"viridis\",\n        cbar=False,\n        xticklabels=enum2tuple(GenericBehavLabels),\n        yticklabels=enum2tuple(GenericBehavLabels),\n        ax=ax,\n    )\n    ax.set_xlabel(\"Predicted\")\n    ax.set_ylabel(\"True\")\n    return fig\n</code></pre>"},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier.eval_logc","title":"<code>eval_logc(y_true, y_prob)</code>  <code>classmethod</code>","text":"<p>summary</p> Source code in <code>behavysis/behav_classifier/behav_classifier.py</code> <pre><code>@classmethod\ndef eval_logc(cls, y_true: np.ndarray, y_prob: np.ndarray) -&gt; Figure:\n    \"\"\"\n    __summary__\n    \"\"\"\n    y_eval = pd.DataFrame(\n        {\n            \"y_true\": y_true,\n            \"y_prob\": y_prob,\n            \"y_pred\": y_prob &gt; 0.4,\n            \"y_true_jitter\": y_true + (0.2 * (np.random.rand(len(y_prob)) - 0.5)),\n        }\n    )\n    fig, ax = plt.subplots(figsize=(10, 7))\n    sns.scatterplot(\n        data=y_eval,\n        x=\"y_prob\",\n        y=\"y_true_jitter\",\n        marker=\".\",\n        s=10,\n        linewidth=0,\n        alpha=0.2,\n        ax=ax,\n    )\n    # Making line of ratio of y_true outcomes for each y_prob\n    pcutoffs = np.linspace(0, 1, 101)\n    ratios = np.vectorize(lambda i: np.mean(i &gt; y_eval[\"y_prob\"]))(pcutoffs)\n    sns.lineplot(x=pcutoffs, y=ratios, ax=ax)\n    return fig\n</code></pre>"},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier.eval_metrics_pcutoffs","title":"<code>eval_metrics_pcutoffs(y_true, y_prob)</code>  <code>classmethod</code>","text":"<p>summary</p> Source code in <code>behavysis/behav_classifier/behav_classifier.py</code> <pre><code>@classmethod\ndef eval_metrics_pcutoffs(cls, y_true: np.ndarray, y_prob: np.ndarray) -&gt; Figure:\n    \"\"\"\n    __summary__\n    \"\"\"\n    # Getting precision, recall and accuracy for different cutoffs\n    pcutoffs = np.linspace(0, 1, 101)\n    # Measures\n    precisions = np.zeros(pcutoffs.shape[0])\n    recalls = np.zeros(pcutoffs.shape[0])\n    f1 = np.zeros(pcutoffs.shape[0])\n    accuracies = np.zeros(pcutoffs.shape[0])\n    for i, pcutoff in enumerate(pcutoffs):\n        y_pred = y_prob &gt; pcutoff\n        report = classification_report(\n            y_true,\n            y_pred,\n            target_names=enum2tuple(GenericBehavLabels),\n            output_dict=True,\n        )\n        precisions[i] = report[GenericBehavLabels.BEHAV.value][\"precision\"]  # type: ignore\n        recalls[i] = report[GenericBehavLabels.BEHAV.value][\"recall\"]  # type: ignore\n        f1[i] = report[GenericBehavLabels.BEHAV.value][\"f1-score\"]  # type: ignore\n        accuracies[i] = report[\"accuracy\"]  # type: ignore\n    # Making figure\n    fig, ax = plt.subplots(figsize=(10, 7))\n    sns.lineplot(x=pcutoffs, y=precisions, label=\"precision\", ax=ax)\n    sns.lineplot(x=pcutoffs, y=recalls, label=\"recall\", ax=ax)\n    sns.lineplot(x=pcutoffs, y=f1, label=\"f1\", ax=ax)\n    sns.lineplot(x=pcutoffs, y=accuracies, label=\"accuracy\", ax=ax)\n    return fig\n</code></pre>"},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier.eval_report","title":"<code>eval_report(y_true, y_pred)</code>  <code>classmethod</code>","text":"<p>summary</p> Source code in <code>behavysis/behav_classifier/behav_classifier.py</code> <pre><code>@classmethod\ndef eval_report(cls, y_true: np.ndarray, y_pred: np.ndarray) -&gt; dict:\n    \"\"\"\n    __summary__\n    \"\"\"\n    return classification_report(\n        y_true=y_true,\n        y_pred=y_pred,\n        target_names=enum2tuple(GenericBehavLabels),\n        output_dict=True,\n    )  # type: ignore\n</code></pre>"},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier.load","title":"<code>load(proj_dir, behav_name)</code>  <code>classmethod</code>","text":"<p>Reads the model from the expected model file.</p> Source code in <code>behavysis/behav_classifier/behav_classifier.py</code> <pre><code>@classmethod\ndef load(cls, proj_dir: str, behav_name: str) -&gt; BehavClassifier:\n    \"\"\"\n    Reads the model from the expected model file.\n    \"\"\"\n    # Checking that the configs file exists and is valid\n    configs_fp = os.path.join(proj_dir, \"behav_models\", behav_name, \"configs.json\")\n    try:\n        BehavClassifierConfigs.read_json(configs_fp)\n    except (FileNotFoundError, OSError):\n        raise ValueError(\n            f'Model in project directory, \"{proj_dir}\", and behav name, \"{behav_name}\", not found.\\n'\n            \"Please check file path.\"\n        )\n    return cls(proj_dir, behav_name)\n</code></pre>"},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier.pipeline_inference","title":"<code>pipeline_inference(x_df)</code>","text":"<p>Given the unprocessed features dataframe, runs the model pipeline to make predictions.</p> <p>Pipeline is: - Preprocess <code>x</code> df. Refer to behavysis.behav_classifier.BehavClassifier.preproc_x for details. - Makes predictions and returns the predicted behaviours.</p> Source code in <code>behavysis/behav_classifier/behav_classifier.py</code> <pre><code>def pipeline_inference(self, x_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Given the unprocessed features dataframe, runs the model pipeline to make predictions.\n\n    Pipeline is:\n    - Preprocess `x` df. Refer to\n    [behavysis.behav_classifier.BehavClassifier.preproc_x][] for details.\n    - Makes predictions and returns the predicted behaviours.\n    \"\"\"\n    index = x_df.index\n    # Preprocessing features\n    x = self.preproc_x_transform(x_df.values, self.preproc_fp)\n    # Loading the model\n    self.clf = joblib_load(self.clf_fp)\n    # Getting probabilities\n    y_prob = self.clf.predict(\n        x=x,\n        index=np.arange(x.shape[0]),\n        batch_size=self.configs.batch_size,\n    )\n    # Making predictions from probabilities (and pcutoff)\n    y_pred = (y_prob &gt; self.configs.pcutoff).astype(int)\n    # Making df\n    pred_df = BehavPredictedDf.init_df(pd.Series(index))\n    pred_df[(self.configs.behav_name, BehavPredictedDf.OutcomesCols.PROB.value)] = y_prob\n    pred_df[(self.configs.behav_name, BehavPredictedDf.OutcomesCols.PRED.value)] = y_pred\n    return pred_df\n</code></pre>"},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier.pipeline_training","title":"<code>pipeline_training()</code>","text":"<p>Makes a classifier and saves it to the model's root directory.</p> <p>Callable is a method from <code>ClfTemplates</code>.</p> Source code in <code>behavysis/behav_classifier/behav_classifier.py</code> <pre><code>def pipeline_training(self) -&gt; None:\n    \"\"\"\n    Makes a classifier and saves it to the model's root directory.\n\n    Callable is a method from `ClfTemplates`.\n    \"\"\"\n    self.logger.info(f\"Training {self.configs.clf_struct}\")\n    # Preparing data\n    x_ls, y_ls, index_train_ls, index_test_ls = self.preproc_training()\n    # Training the model\n    history = self.clf.fit(\n        x_ls=x_ls,\n        y_ls=y_ls,\n        index_ls=index_train_ls,\n        batch_size=self.configs.batch_size,\n        epochs=self.configs.epochs,\n        val_split=self.configs.val_split,\n    )\n    # Saving history\n    self.clf_eval_save_history(history)\n    # Evaluating on train and test data\n    self.clf_eval_save_performance(x_ls, y_ls, index_train_ls, \"train\")\n    self.clf_eval_save_performance(x_ls, y_ls, index_test_ls, \"test\")\n    # Saving model\n    joblib_dump(self.clf, self.clf_fp)\n</code></pre>"},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier.pipeline_training_all","title":"<code>pipeline_training_all()</code>","text":"<p>Making classifier for all available templates.</p> Source code in <code>behavysis/behav_classifier/behav_classifier.py</code> <pre><code>def pipeline_training_all(self):\n    \"\"\"\n    Making classifier for all available templates.\n    \"\"\"\n    # Saving existing clf\n    clf = self.clf\n    for clf_cls in CLF_TEMPLATES:\n        # Initialising the model\n        self.clf = clf_cls()\n        # Building pipeline, which runs and saves evaluation\n        self.pipeline_training()\n    # Restoring clf\n    self.clf = clf\n</code></pre>"},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier.preproc_training","title":"<code>preproc_training()</code>","text":"<p>Prepares the data for the training pipeline.</p> <p>Performs the following: - Combining dfs from x and y directories (individual experiment data). - Ensures the x and y dfs have the same index, and are in the same row order. - Preprocesses x df. Refer to <code>preprocess_x</code> for details. - Selects the y class (given in the configs file) from the y df. - Preprocesses y df. Refer to <code>preprocess_y</code> for details. - Splits into training and test indexes.     - The training indexes are undersampled to the ratio given in the configs.</p> <p>Returns:</p> Type Description <code>A tuple containing four numpy arrays:</code> <code>- x_ls: list of each dataframe's input data.</code> <code>- y_ls: list of each dataframe's target labels.</code> <code>- index_train_ls: list of each dataframe's indexes for the training data.</code> <code>- index_test_ls: list of each dataframe's indexes for the testing data.</code> Source code in <code>behavysis/behav_classifier/behav_classifier.py</code> <pre><code>def preproc_training(\n    self,\n) -&gt; tuple[list[np.ndarray], list[np.ndarray], list[np.ndarray], list[np.ndarray]]:\n    \"\"\"\n    Prepares the data for the training pipeline.\n\n    Performs the following:\n    - Combining dfs from x and y directories (individual experiment data).\n    - Ensures the x and y dfs have the same index, and are in the same row order.\n    - Preprocesses x df. Refer to `preprocess_x` for details.\n    - Selects the y class (given in the configs file) from the y df.\n    - Preprocesses y df. Refer to `preprocess_y` for details.\n    - Splits into training and test indexes.\n        - The training indexes are undersampled to the ratio given in the configs.\n\n    Returns\n    -------\n    A tuple containing four numpy arrays:\n    - x_ls: list of each dataframe's input data.\n    - y_ls: list of each dataframe's target labels.\n    - index_train_ls: list of each dataframe's indexes for the training data.\n    - index_test_ls: list of each dataframe's indexes for the testing data.\n    \"\"\"\n    # Getting the lists of x and y dfs\n    x_fp_ls = [os.path.join(self.x_dir, i) for i in os.listdir(os.path.join(self.x_dir))]\n    y_fp_ls = [os.path.join(self.y_dir, i) for i in os.listdir(os.path.join(self.y_dir))]\n    x_df_ls = async_read_files_run(x_fp_ls, FeaturesDf.read)\n    y_df_ls = async_read_files_run(y_fp_ls, BehavScoredDf.read)\n    # Formatting y dfs (selecting column and replacing UNDETERMINED with NON_BEHAV values)\n    y_df_ls = [\n        y[(self.configs.behav_name, BehavScoredDf.OutcomesCols.ACTUAL.value)].replace(\n            BehavValues.UNDETERMINED.value, BehavValues.NON_BEHAV.value\n        )\n        for y in y_df_ls\n    ]\n    # Ensuring x and y dfs have the same index and are in the same row order\n    index_df_ls = [x.index.intersection(y.index) for x, y in zip(x_df_ls, y_df_ls)]\n    x_df_ls = [x.loc[index] for x, index in zip(x_df_ls, index_df_ls)]\n    y_df_ls = [y.loc[index] for y, index in zip(y_df_ls, index_df_ls)]\n    assert np.all([x.shape[0] == y.shape[0] for x, y in zip(x_df_ls, y_df_ls)])\n    # Converting to numpy arrays\n    x_ls = [x.values for x in x_df_ls]\n    y_ls = [y.values for y in y_df_ls]\n    index_ls = [np.arange(x.shape[0]) for x in x_ls]\n    # x preprocessing: fitting (across all x dfs) and transforming (for each x df)\n    self.preproc_x_fit(np.concatenate(x_ls, axis=0), self.preproc_fp)\n    x_ls = [self.preproc_x_transform(x, self.preproc_fp) for x in x_ls]\n    # Making a 2D array of (df_index, index, y) for train-test splitting, stratifying and sampling\n    index_flat = listofvects2array(index_ls, y_ls)\n    # Splitting into train and test indexes\n    index_train_flat, index_test_flat = train_test_split(\n        index_flat,\n        test_size=self.configs.test_split,\n        stratify=index_flat[:, 2],\n    )\n    # Oversampling and undersampling ONLY on training data\n    index_train_flat = self.oversample(index_train_flat, index_train_flat[:, 2], self.configs.oversample_ratio)\n    index_train_flat = self.undersample(index_train_flat, index_train_flat[:, 2], self.configs.undersample_ratio)\n    # Reshaping back to individual df index lists\n    index_train_ls = array2listofvect(index_train_flat, 1)\n    index_test_ls = array2listofvect(index_test_flat, 1)\n    return x_ls, y_ls, index_train_ls, index_test_ls\n</code></pre>"},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier.preproc_x_fit","title":"<code>preproc_x_fit(x, preproc_fp)</code>  <code>classmethod</code>","text":"<p>The preprocessing steps are: - Select only the derived features (not the x-y-l columns)     - 2 (indivs) * 8 (bpts) * 3 (coords) = 48 (columns) before derived features - MinMax scaling (using previously fitted MinMaxScaler)</p> Source code in <code>behavysis/behav_classifier/behav_classifier.py</code> <pre><code>@classmethod\ndef preproc_x_fit(cls, x: np.ndarray, preproc_fp: str) -&gt; None:\n    \"\"\"\n    The preprocessing steps are:\n    - Select only the derived features (not the x-y-l columns)\n        - 2 (indivs) * 8 (bpts) * 3 (coords) = 48 (columns) before derived features\n    - MinMax scaling (using previously fitted MinMaxScaler)\n    \"\"\"\n    preproc_pipe = Pipeline(\n        steps=[\n            (\"select_columns\", FunctionTransformer(cls._preproc_x_fit_select_cols)),\n            (\"min_max_scaler\", MinMaxScaler()),\n        ]\n    )\n    preproc_pipe.fit(x)\n    joblib_dump(preproc_pipe, preproc_fp)\n</code></pre>"},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier.preproc_x_transform","title":"<code>preproc_x_transform(x, preproc_fp)</code>  <code>classmethod</code>","text":"<p>Runs the preprocessing steps fitted from <code>preproc_x_fit</code> on the given <code>x</code> data.</p> Source code in <code>behavysis/behav_classifier/behav_classifier.py</code> <pre><code>@classmethod\ndef preproc_x_transform(cls, x: np.ndarray, preproc_fp: str) -&gt; np.ndarray:\n    \"\"\"\n    Runs the preprocessing steps fitted from `preproc_x_fit` on the given `x` data.\n    \"\"\"\n    preproc_pipe: Pipeline = joblib_load(preproc_fp)\n    x_preproc = preproc_pipe.transform(x)\n    return x_preproc\n</code></pre>"},{"location":"reference/behav_classifier.html#behavysis.behav_classifier.behav_classifier.BehavClassifier.wrangle_columns_y","title":"<code>wrangle_columns_y(y)</code>  <code>classmethod</code>","text":"<p>Filters the <code>y</code> dataframe to only include the <code>behav</code> column and the specific outcome columns, and rename the columns to be in the format <code>{behav}__{outcome}</code>.</p> Source code in <code>behavysis/behav_classifier/behav_classifier.py</code> <pre><code>@classmethod\ndef wrangle_columns_y(cls, y: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the `y` dataframe to only include the `behav` column and the specific outcome columns,\n    and rename the columns to be in the format `{behav}__{outcome}`.\n    \"\"\"\n    # Filtering out the pred columns (in the `outcomes` level)\n    columns_filter = np.isin(\n        y.columns.get_level_values(BehavScoredDf.CN.OUTCOMES.value),\n        [BehavScoredDf.OutcomesCols.PRED.value],\n        invert=True,\n    )\n    y = y.loc[:, columns_filter]\n    # Setting the column names from `(behav, outcome)` to `{behav}__{outcome}`\n    y.columns = [\n        f\"{behav_name}\"\n        if outcome_name == BehavScoredDf.OutcomesCols.ACTUAL.value\n        else f\"{behav_name}__{outcome_name}\"\n        for behav_name, outcome_name in y.columns\n    ]\n    return y\n</code></pre>"},{"location":"reference/behav_classifier.html#behavysis.pydantic_models.behav_classifier_configs.BehavClassifierConfigs","title":"<code>behavysis.pydantic_models.behav_classifier_configs.BehavClassifierConfigs</code>","text":"<p>               Bases: <code>PydanticBaseModel</code></p> Source code in <code>behavysis/pydantic_models/behav_classifier_configs.py</code> <pre><code>class BehavClassifierConfigs(PydanticBaseModel):\n    proj_dir: str = \"project_dir\"\n    behav_name: str = \"behav_name\"\n    seed: int = 42\n    oversample_ratio: float = 0.2\n    undersample_ratio: float = 0.4\n\n    clf_struct: str = \"clf\"  # Classifier type (defined in ClfTemplates)\n    pcutoff: float = 0.2\n    test_split: float = 0.2\n    val_split: float = 0.2\n    batch_size: int = 256\n    epochs: int = 100\n</code></pre>"},{"location":"reference/behav_classifier_templates.html","title":"Behav classifier templates","text":""},{"location":"reference/behav_classifier_templates.html#behavysis.behav_classifier.clf_models.clf_templates","title":"<code>behavysis.behav_classifier.clf_models.clf_templates</code>","text":"<p>summary</p>"},{"location":"reference/behav_classifier_templates.html#behavysis.behav_classifier.clf_models.clf_templates.CNN1","title":"<code>CNN1</code>","text":"<p>               Bases: <code>BaseTorchModel</code></p> <p>x features is (samples, window, features). y outcome is (samples, class).</p> Source code in <code>behavysis/behav_classifier/clf_models/clf_templates.py</code> <pre><code>class CNN1(BaseTorchModel):\n    \"\"\"\n    x features is (samples, window, features).\n    y outcome is (samples, class).\n    \"\"\"\n\n    def __init__(self):\n        # Initialising the parent class\n        super().__init__(498, 10)  # 546\n        # Define the layers\n        self.conv1 = nn.Conv1d(self.nfeatures, 64, kernel_size=2)\n        self.relu1 = nn.ReLU()\n        # self.maxpool1 = nn.MaxPool1d(kernel_size=2)\n        self.flatten = nn.Flatten()\n\n        flat_size = self.window_frames * 2 + 1\n        flat_size = flat_size - 1\n        # flat_size = (flat_size - 2) // 2\n        flat_size = flat_size * 64\n\n        self.fc1 = nn.Linear(flat_size, 64)\n        self.relu3 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 1)\n        self.sigmoid1 = nn.Sigmoid()\n        # Define the loss function and optimizer\n        self.criterion: nn.Module = nn.BCELoss()\n        self.optimizer: optim.Optimizer = optim.Adam(self.parameters())\n        # Setting the device (GPU or CPU)\n        self.device_to_gpu()\n\n    def forward(self, x):\n        out = x\n        out = self.conv1(out)\n        out = self.relu1(out)\n        # out = self.maxpool1(out)\n        out = self.flatten(out)\n        out = self.fc1(out)\n        out = self.relu3(out)\n        out = self.dropout1(out)\n        out = self.fc2(out)\n        out = self.sigmoid1(out)\n        return out\n</code></pre>"},{"location":"reference/behav_classifier_templates.html#behavysis.behav_classifier.clf_models.clf_templates.CNN2","title":"<code>CNN2</code>","text":"<p>               Bases: <code>BaseTorchModel</code></p> <p>x features is (samples, window, features). y outcome is (samples, class).</p> Source code in <code>behavysis/behav_classifier/clf_models/clf_templates.py</code> <pre><code>class CNN2(BaseTorchModel):\n    \"\"\"\n    x features is (samples, window, features).\n    y outcome is (samples, class).\n    \"\"\"\n\n    def __init__(self):\n        # Initialising the parent class\n        super().__init__(498, 10)  # 546\n        # Define the layers\n        self.conv1 = nn.Conv1d(self.nfeatures, 64, kernel_size=3)\n        self.relu1 = nn.ReLU()\n        self.maxpool1 = nn.MaxPool1d(kernel_size=2)\n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3)\n        self.relu2 = nn.ReLU()\n        self.maxpool2 = nn.MaxPool1d(kernel_size=2)\n        self.flatten = nn.Flatten()\n\n        flat_size = self.window_frames * 2 + 1\n        flat_size = (flat_size - 2) // 2\n        flat_size = (flat_size - 2) // 2\n        flat_size = flat_size * 32\n\n        self.fc1 = nn.Linear(flat_size, 64)\n        self.relu3 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 1)\n        self.sigmoid1 = nn.Sigmoid()\n        # Define the loss function and optimizer\n        self.criterion: nn.Module = nn.BCELoss()\n        self.optimizer: optim.Optimizer = optim.Adam(self.parameters())\n        # Setting the device (GPU or CPU)\n        self.device_to_gpu()\n\n    def forward(self, x):\n        out = x\n        out = self.conv1(out)\n        out = self.relu1(out)\n        out = self.maxpool1(out)\n        out = self.conv2(out)\n        out = self.relu2(out)\n        out = self.maxpool2(out)\n        out = self.flatten(out)\n        out = self.fc1(out)\n        out = self.relu3(out)\n        out = self.dropout1(out)\n        out = self.fc2(out)\n        out = self.sigmoid1(out)\n        return out\n</code></pre>"},{"location":"reference/behav_classifier_templates.html#behavysis.behav_classifier.clf_models.clf_templates.DNN1","title":"<code>DNN1</code>","text":"<p>               Bases: <code>BaseTorchModel</code></p> <p>x features is (samples, window, features). y outcome is (samples, class).</p> Source code in <code>behavysis/behav_classifier/clf_models/clf_templates.py</code> <pre><code>class DNN1(BaseTorchModel):\n    \"\"\"\n    x features is (samples, window, features).\n    y outcome is (samples, class).\n    \"\"\"\n\n    def __init__(self):\n        # Initialising the parent class\n        super().__init__(498, 0)  # 546\n        # Input shape\n        flat_size = self.window_frames * 2 + 1\n        flat_size = flat_size * self.nfeatures\n        # Define the layers\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(flat_size, 64)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 1)\n        self.sigmoid1 = nn.Sigmoid()\n        # Define the loss function and optimizer\n        self.criterion: nn.Module = nn.BCELoss()\n        self.optimizer: optim.Optimizer = optim.Adam(self.parameters())\n        # Setting the device (GPU or CPU)\n        self.device_to_gpu()\n\n    def forward(self, x):\n        out = x\n        out = self.flatten(out)\n        out = self.fc1(out)\n        out = self.relu1(out)\n        out = self.dropout1(out)\n        out = self.fc2(out)\n        out = self.sigmoid1(out)\n        return out\n</code></pre>"},{"location":"reference/behav_classifier_templates.html#behavysis.behav_classifier.clf_models.clf_templates.DNN2","title":"<code>DNN2</code>","text":"<p>               Bases: <code>BaseTorchModel</code></p> <p>x features is (samples, window, features). y outcome is (samples, class).</p> Source code in <code>behavysis/behav_classifier/clf_models/clf_templates.py</code> <pre><code>class DNN2(BaseTorchModel):\n    \"\"\"\n    x features is (samples, window, features).\n    y outcome is (samples, class).\n    \"\"\"\n\n    def __init__(self):\n        # Initialising the parent class\n        super().__init__(498, 0)  # 546\n        # Input shape\n        flat_size = self.window_frames * 2 + 1\n        flat_size = flat_size * self.nfeatures\n        # Define the layers\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(flat_size, 32)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(32, 1)\n        self.sigmoid1 = nn.Sigmoid()\n        # Define the loss function and optimizer\n        self.criterion: nn.Module = nn.BCELoss()\n        self.optimizer: optim.Optimizer = optim.Adam(self.parameters())\n        # Setting the device (GPU or CPU)\n        self.device_to_gpu()\n\n    def forward(self, x):\n        out = x\n        out = self.flatten(out)\n        out = self.fc1(out)\n        out = self.relu1(out)\n        out = self.dropout1(out)\n        out = self.fc2(out)\n        out = self.sigmoid1(out)\n        return out\n</code></pre>"},{"location":"reference/behav_classifier_templates.html#behavysis.behav_classifier.clf_models.clf_templates.DNN3","title":"<code>DNN3</code>","text":"<p>               Bases: <code>BaseTorchModel</code></p> <p>x features is (samples, window, features). y outcome is (samples, class).</p> Source code in <code>behavysis/behav_classifier/clf_models/clf_templates.py</code> <pre><code>class DNN3(BaseTorchModel):\n    \"\"\"\n    x features is (samples, window, features).\n    y outcome is (samples, class).\n    \"\"\"\n\n    def __init__(self):\n        # Initialising the parent class\n        super().__init__(498, 0)  # 546\n        # Input shape\n        flat_size = self.window_frames * 2 + 1\n        flat_size = flat_size * self.nfeatures\n        # Define the layers\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(flat_size, 256)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(256, 64)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(64, 1)\n        self.sigmoid1 = nn.Sigmoid()\n        # Define the loss function and optimizer\n        self.criterion: nn.Module = nn.BCELoss()\n        self.optimizer: optim.Optimizer = optim.Adam(self.parameters())\n        # Setting the device (GPU or CPU)\n        self.device_to_gpu()\n\n    def forward(self, x):\n        out = x\n        out = self.flatten(out)\n        out = self.fc1(out)\n        out = self.relu1(out)\n        out = self.dropout1(out)\n        out = self.fc2(out)\n        out = self.relu2(out)\n        out = self.dropout2(out)\n        out = self.fc3(out)\n        out = self.sigmoid1(out)\n        return out\n</code></pre>"},{"location":"reference/behav_classifier_templates.html#behavysis.behav_classifier.clf_models.clf_templates.RF1","title":"<code>RF1</code>","text":"<p>               Bases: <code>RandomForestClassifier</code></p> <p>x features is (samples, features). y outcome is (samples, class).</p> Source code in <code>behavysis/behav_classifier/clf_models/clf_templates.py</code> <pre><code>class RF1(RandomForestClassifier):\n    \"\"\"\n    x features is (samples, features).\n    y outcome is (samples, class).\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            n_estimators=2000,\n            max_depth=3,\n            random_state=0,\n            n_jobs=16,\n            verbose=1,\n        )\n        self.window_frames = 0\n\n    def fit(self, x_ls: list[np.ndarray], y_ls: list[np.ndarray], index_ls: list[np.ndarray], *args, **kwargs):\n        # Filtering data\n        x_ls = [x[index] for x, index in zip(x_ls, index_ls)]\n        y_ls = [y[index] for y, index in zip(y_ls, index_ls)]\n        # Concatenating dataframes\n        x = np.concatenate(x_ls, axis=0)\n        y = np.concatenate(y_ls, axis=0)\n        # Fitting\n        super().fit(x, y)\n        return pd.DataFrame(\n            index=pd.Index([], name=\"epoch\"),\n            columns=[\"loss\", \"vloss\"],\n        )\n\n    def predict(self, x: np.ndarray, index: None | np.ndarray = None, *args, **kwargs):\n        index = index if index is not None else np.arange(x.shape[0])\n        return super().predict_proba(x[index])[:, 1]\n</code></pre>"},{"location":"reference/behavysis.html","title":"Behavysis","text":""},{"location":"reference/behavysis.html#behavysis","title":"<code>behavysis</code>","text":"<p>This package is used to interprets and interprets lab mice behaviour using computer vision. The package allows users to perform the entire analytics pipeline from raw lab footage to interpretable plotted and tabulated data for different analysises. This pipeline includes:</p> <ul> <li>Formatting raw videos to a desired mp4 format (e.g. user defined fps and resolution)</li> <li>Performing stance detection on the mp4 file to generate an annotated mp4 file and file that tabulates the x-y coordinates of the subject's body points in each video frame. DeepLabCut is used to perform this.</li> <li>Preprocessing the coordinates file</li> <li>Extracting meaningful data analysis from the preprocessed coordinates file</li> </ul>"},{"location":"reference/experiment.html","title":"Experiment","text":""},{"location":"reference/experiment.html#behavysis.pipeline.experiment.Experiment","title":"<code>behavysis.pipeline.experiment.Experiment</code>","text":"<p>Behavysis Pipeline class for a single experiment.</p> <p>Encompasses the entire process including: - Raw mp4 file import. - mp4 file formatting (px and fps). - DLC keypoints inference. - Feature wrangling (start time detection, more features like average body position). - Interpretable behaviour results. - Other quantitative analysis.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>description</p> required <code>root_dir</code> <code>str</code> <p>description</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>ValueError: <code>root_dir</code> does not exist or <code>name</code> does not exist in the <code>root_dir</code> folder.</p> Source code in <code>behavysis/pipeline/experiment.py</code> <pre><code>class Experiment:\n    \"\"\"\n    Behavysis Pipeline class for a single experiment.\n\n    Encompasses the entire process including:\n    - Raw mp4 file import.\n    - mp4 file formatting (px and fps).\n    - DLC keypoints inference.\n    - Feature wrangling (start time detection, more features like average body position).\n    - Interpretable behaviour results.\n    - Other quantitative analysis.\n\n    Parameters\n    ----------\n    name : str\n        _description_\n    root_dir : str\n        _description_\n\n    Raises\n    ------\n    ValueError\n        ValueError: `root_dir` does not exist or `name` does not exist in the `root_dir` folder.\n    \"\"\"\n\n    logger = init_logger_file()\n\n    def __init__(self, name: str, root_dir: str) -&gt; None:\n        \"\"\"\n        Make a Experiment instance.\n        \"\"\"\n        # Assertion: root_dir mus\u2020 exist\n        if not os.path.isdir(root_dir):\n            raise ValueError(\n                f'Cannot find the project folder named \"{root_dir}\".\\nPlease specify a folder that exists.'\n            )\n        # Setting up instance variables\n        self.name = name\n        self.root_dir = os.path.abspath(root_dir)\n        # Assertion: name must correspond to at least one file in root_dir\n        file_exists_ls = [os.path.isfile(self.get_fp(f)) for f in Folders]\n        if not np.any(file_exists_ls):\n            folders_ls_msg = \"\".join([f\"\\n    - {f.value}\" for f in Folders])\n            raise ValueError(\n                f'No files named \"{name}\" exist in \"{root_dir}\".\\n'\n                f'Please specify a file that exists in \"{root_dir}\", '\n                f\"in one of the following folder WITH the correct file extension name:{folders_ls_msg}\"\n            )\n\n    #####################################################################\n    #               GET/CHECK FILEPATH METHODS\n    #####################################################################\n\n    def get_fp(self, _folder: Folders | str) -&gt; str:\n        \"\"\"\n        Returns the experiment's file path from the given folder.\n\n        Parameters\n        ----------\n        folder_str : str\n            The folder to return the experiment document's filepath for.\n\n        Returns\n        -------\n        str\n            The experiment document's filepath.\n\n        Raises\n        ------\n        ValueError\n            ValueError: Folder name is not valid. Refer to Folders Enum for valid folder names.\n        \"\"\"\n        # Getting Folder item\n        if isinstance(_folder, str):\n            try:\n                folder = Folders(_folder)\n            except ValueError:\n                folders_ls_msg = \"\".join([f\"\\n    - {f.value}\" for f in Folders])\n                raise ValueError(\n                    f\"{_folder} is not a valid experiment folder name.\\n\"\n                    f\"Please only specify one of the following folders:{folders_ls_msg}\"\n                )\n        else:\n            folder = _folder\n        # Getting file extension from enum\n        file_ext: FileExts = getattr(FileExts, folder.name)\n        # Getting experiment filepath for given folder\n        fp = os.path.join(self.root_dir, folder.value, f\"{self.name}.{file_ext.value}\")\n        return fp\n\n    #####################################################################\n    #               EXPERIMENT PROCESSING SCAFFOLD METHODS\n    #####################################################################\n\n    def _proc_scaff(\n        self,\n        funcs: tuple[Callable, ...],\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; dict[str, str]:\n        \"\"\"\n        All processing runs through here.\n        This method ensures that the stdout and diagnostics dict are correctly generated.\n\n        Parameters\n        ----------\n        funcs : tuple[Callable, ...]\n            List of functions.\n\n        Returns\n        -------\n        dict[str, str]\n            Diagnostics dictionary, with description of each function's outcome.\n\n        Notes\n        -----\n        Each func in `funcs` is called in the form:\n        ```\n        func(*args, **kwargs)\n        ```\n        \"\"\"\n        f_names_ls_msg = \"\".join([f\"\\n    - {f.__name__}\" for f in funcs])\n        self.logger.info(f\"Processing experiment, {self.name}, with:{f_names_ls_msg}\")\n        # Setting up diagnostics dict\n        dd = {\"experiment\": self.name}\n        # Running functions and saving outcome to diagnostics dict\n        for f in funcs:\n            f_name = f.__name__\n            # Getting logger and corresponding io object\n            f_logger, f_io_obj = init_logger_io_obj(f_name)\n            # Running each func and saving outcome\n            try:\n                f(*args, **kwargs)\n                # f_logger.info(success_msg())\n            except Exception as e:\n                f_logger.error(e)\n                self.logger.debug(traceback.format_exc())\n            # Adding to diagnostics dict\n            dd[f_name] = get_io_obj_content(f_io_obj)\n            # Clearing io object\n            f_io_obj.truncate(0)\n        self.logger.info(f\"Finished processing experiment, {self.name}, with:{f_names_ls_msg}\")\n        return dd\n\n    #####################################################################\n    #                        CONFIG FILE METHODS\n    #####################################################################\n\n    def update_configs(self, default_configs_fp: str, overwrite: str) -&gt; dict:\n        \"\"\"\n        Initialises the JSON config files with the given configurations in `configs`.\n        It can be specified whether or not to overwrite existing configuration values.\n\n        Parameters\n        ----------\n        default_configs_fp : str\n            The JSON configs filepath to add/overwrite to the experiment's current configs file.\n        overwrite : {\"set\", \"reset\"}\n            Specifies how to overwrite existing configurations.\n            If `add`, only parameters in `configs` not already in the config files are added.\n            If `set`, all parameters in `configs` are set in the config files (overwriting).\n            If `reset`, the config files are completely replaced by `configs`.\n\n        Returns\n        -------\n        dict\n            Diagnostics dictionary, with description of each function's outcome.\n        \"\"\"\n        return self._proc_scaff(\n            (UpdateConfigs.update_configs,),\n            configs_fp=self.get_fp(Folders.CONFIGS),\n            default_configs_fp=default_configs_fp,\n            overwrite=overwrite,\n        )\n\n    #####################################################################\n    #                    FORMATTING VIDEO METHODS\n    #####################################################################\n\n    def format_vid(self, overwrite: bool) -&gt; dict:\n        \"\"\"\n        Formats the video with ffmpeg to fit the formatted configs (e.g. fps and resolution_px).\n        Once the formatted video is produced, the configs dict and *configs.json file are\n        updated with the formatted video's metadata.\n\n        Parameters\n        ----------\n        funcs : tuple[Callable, ...]\n            _description_\n        overwrite : bool\n            Whether to overwrite the output file (if it exists).\n\n        Returns\n        -------\n        dict\n            Diagnostics dictionary, with description of each function's outcome.\n\n        Notes\n        -----\n        Can call any methods from `FormatVid`.\n        \"\"\"\n        return self._proc_scaff(\n            (FormatVid.format_vid,),\n            raw_vid_fp=self.get_fp(Folders.RAW_VID),\n            formatted_vid_fp=self.get_fp(Folders.FORMATTED_VID),\n            configs_fp=self.get_fp(Folders.CONFIGS),\n            overwrite=overwrite,\n        )\n\n    def get_vid_metadata(self) -&gt; dict:\n        \"\"\"\n        Gets the video metadata for the raw and formatted video files.\n\n        Parameters\n        ----------\n        overwrite : bool\n            _description_\n\n        Returns\n        -------\n        dict\n            _description_\n        \"\"\"\n        return self._proc_scaff(\n            (FormatVid.get_vids_metadata,),\n            raw_vid_fp=self.get_fp(Folders.RAW_VID),\n            formatted_vid_fp=self.get_fp(Folders.FORMATTED_VID),\n            configs_fp=self.get_fp(Folders.CONFIGS),\n        )\n\n    #####################################################################\n    #                      DLC KEYPOINTS METHODS\n    #####################################################################\n\n    def run_dlc(self, gputouse: int | None, overwrite: bool) -&gt; dict:\n        \"\"\"\n        Run the DLC model on the formatted video to generate a DLC annotated video\n        and DLC h5 file for all experiments.\n\n        Parameters\n        ----------\n        gputouse : int\n            _description_\n        overwrite : bool\n            Whether to overwrite the output file (if it exists).\n\n        Returns\n        -------\n        dict\n            Diagnostics dictionary, with description of each function's outcome.\n\n        Notes\n        -----\n        Can call any methods from `RunDLC`.\n        \"\"\"\n        return self._proc_scaff(\n            (RunDLC.ma_dlc_run_single,),\n            formatted_vid_fp=self.get_fp(Folders.FORMATTED_VID),\n            keypoints_fp=self.get_fp(Folders.KEYPOINTS),\n            configs_fp=self.get_fp(Folders.CONFIGS),\n            gputouse=gputouse,\n            overwrite=overwrite,\n        )\n\n    def calculate_parameters(self, funcs: tuple[Callable, ...]) -&gt; dict:\n        \"\"\"\n        A pipeline to calculate the parameters of the keypoints file, which will\n        assist in preprocessing the keypoints data.\n\n        Parameters\n        ----------\n        funcs : tuple[Callable, ...]\n            _description_\n\n        Returns\n        -------\n        Dict\n            Diagnostics dictionary, with description of each function's outcome.\n\n        Notes\n        -----\n        Can call any methods from `CalculateParams`.\n        \"\"\"\n        return self._proc_scaff(\n            funcs,\n            keypoints_fp=self.get_fp(Folders.KEYPOINTS),\n            configs_fp=self.get_fp(Folders.CONFIGS),\n        )\n\n    def collate_auto_configs(self) -&gt; Dict:\n        \"\"\"\n        Collates the auto-configs of the experiment into the main configs file.\n        \"\"\"\n        dd = {\"experiment\": self.name}\n        # Reading the experiment's configs file\n        f_logger, f_io_obj = init_logger_io_obj()\n        try:\n            configs = ExperimentConfigs.read_json(self.get_fp(Folders.CONFIGS))\n            f_logger.debug(\"Reading configs file.\")\n            # f_logger.info(success_msg())\n            dd[\"reading_configs\"] = get_io_obj_content(f_io_obj)\n        except FileNotFoundError:\n            f_logger.error(\"no configs file found.\")\n            dd[\"reading_configs\"] = get_io_obj_content(f_io_obj)\n            return dd\n        # Getting all the auto fields from the configs file\n        configs_auto_field_keys = AutoConfigs.get_field_names()\n        for field_key_ls in configs_auto_field_keys:\n            value = configs.auto\n            for key in field_key_ls:\n                value = getattr(value, key)\n            dd[\"_\".join(field_key_ls)] = value  # type: ignore\n        return dd\n\n    def preprocess(self, funcs: tuple[Callable, ...], overwrite: bool) -&gt; dict:\n        \"\"\"\n        A preprocessing pipeline method to convert raw keypoints data into preprocessed\n        keypoints data that is ready for ML analysis.\n        All functs passed in must have the format func(df, dict) -&gt; df. Possible funcs\n        are given in preprocessing.py\n        The preprocessed data is saved to the project's preprocessed folder.\n\n        Parameters\n        ----------\n        funcs : tuple[Callable, ...]\n            _description_\n        overwrite : bool\n            Whether to overwrite the output file (if it exists).\n\n        Returns\n        -------\n        dict\n            Diagnostics dictionary, with description of each function's outcome.\n\n        Notes\n        -----\n        Can call any methods from `Preprocess`.\n        \"\"\"\n        # Exporting keypoints df to preprocessed folder\n        dd0 = self._proc_scaff(\n            (Export.df2df,),\n            src_fp=self.get_fp(Folders.KEYPOINTS),\n            dst_fp=self.get_fp(Folders.PREPROCESSED),\n            overwrite=overwrite,\n        )\n        # If there is an error or warning (indicates not to ovewrite) in logger, return early\n        if \"ERROR\" in dd0[Export.df2df.__name__] or \"WARNING\" in dd0[Export.df2df.__name__]:\n            return dd0\n        # Feeding through preprocessing functions\n        dd1 = self._proc_scaff(\n            funcs,\n            src_fp=self.get_fp(Folders.PREPROCESSED),\n            dst_fp=self.get_fp(Folders.PREPROCESSED),\n            configs_fp=self.get_fp(Folders.CONFIGS),\n            overwrite=True,\n        )\n        return {**dd0, **dd1}\n\n    #####################################################################\n    #                 SIMBA BEHAVIOUR CLASSIFICATION METHODS\n    #####################################################################\n\n    def extract_features(self, overwrite: bool) -&gt; dict:\n        \"\"\"\n        Extracts features from the preprocessed dlc file to generate many more features.\n        This dataframe of derived features will be input for a ML classifier to detect\n        particularly trained behaviours.\n\n        Parameters\n        ----------\n        overwrite : bool\n            Whether to overwrite the output file (if it exists).\n\n        Returns\n        -------\n        dict\n            Diagnostics dictionary, with description of each function's outcome.\n        \"\"\"\n        return self._proc_scaff(\n            (ExtractFeatures.extract_features,),\n            keypoints_fp=self.get_fp(Folders.PREPROCESSED),\n            features_fp=self.get_fp(Folders.FEATURES_EXTRACTED),\n            configs_fp=self.get_fp(Folders.CONFIGS),\n            overwrite=overwrite,\n        )\n\n    def classify_behavs(self, overwrite: bool) -&gt; dict:\n        \"\"\"\n        Given model config files in the BehavClassifier format, generates beahviour predidctions\n        on the given extracted features dataframe.\n\n        Parameters\n        ----------\n        overwrite : bool\n            Whether to overwrite the output file (if it exists).\n\n        Returns\n        -------\n        dict\n            Diagnostics dictionary, with description of each function's outcome.\n        \"\"\"\n        return self._proc_scaff(\n            (ClassifyBehavs.classify_behavs,),\n            features_fp=self.get_fp(Folders.FEATURES_EXTRACTED),\n            behavs_fp=self.get_fp(Folders.PREDICTED_BEHAVS),\n            configs_fp=self.get_fp(Folders.CONFIGS),\n            overwrite=overwrite,\n        )\n\n    def export_behavs(self, overwrite: bool) -&gt; dict:\n        \"\"\"\n        _summary_\n\n        Parameters\n        ----------\n        overwrite : bool\n            _description_\n\n        Returns\n        -------\n        dict\n            _description_\n        \"\"\"\n        # Exporting 6_predicted_behavs df to 7_scored_behavs folder\n        return self._proc_scaff(\n            (Export.predictedbehavs2scoredbehavs,),\n            src_fp=self.get_fp(Folders.PREDICTED_BEHAVS),\n            dst_fp=self.get_fp(Folders.SCORED_BEHAVS),\n            configs_fp=self.get_fp(Folders.CONFIGS),\n            overwrite=overwrite,\n        )\n\n    #####################################################################\n    #                     SIMPLE ANALYSIS METHODS\n    #####################################################################\n\n    def analyse(self, funcs: tuple[Callable, ...]) -&gt; dict:\n        \"\"\"\n        An ML pipeline method to analyse the preprocessed DLC data.\n        Possible funcs are given in analysis.py.\n        The preprocessed data is saved to the project's analysis folder.\n\n        Parameters\n        ----------\n        funcs : tuple[Callable, ...]\n            _description_\n\n        Returns\n        -------\n        dict\n            Diagnostics dictionary, with description of each function's outcome.\n\n        Notes\n        -----\n        Can call any methods from `Analyse`.\n        \"\"\"\n        return self._proc_scaff(\n            funcs,\n            keypoints_fp=self.get_fp(Folders.PREPROCESSED),\n            dst_dir=os.path.join(self.root_dir, ANALYSIS_DIR),\n            configs_fp=self.get_fp(Folders.CONFIGS),\n        )\n\n    def analyse_behavs(self) -&gt; dict:\n        \"\"\"\n        An ML pipeline method to analyse the preprocessed DLC data.\n        Possible funcs are given in analysis.py.\n        The preprocessed data is saved to the project's analysis folder.\n\n        Returns\n        -------\n        dict\n            Diagnostics dictionary, with description of each function's outcome.\n\n        Notes\n        -----\n        Can call any methods from `Analyse`.\n        \"\"\"\n        return self._proc_scaff(\n            (AnalyseBehavs.analyse_behavs,),\n            behavs_fp=self.get_fp(Folders.SCORED_BEHAVS),\n            dst_dir=os.path.join(self.root_dir, ANALYSIS_DIR),\n            configs_fp=self.get_fp(Folders.CONFIGS),\n        )\n\n    def combine_analysis(self) -&gt; dict:\n        \"\"\"\n        Combine the experiment's analysis in each fbf into a single df\n        \"\"\"\n        # TODO: make new subfolder called combined_analysis and make ONLY(??) fbf analysis.\n        return self._proc_scaff(\n            (CombineAnalysis.combine_analysis,),\n            analysis_dir=os.path.join(self.root_dir, ANALYSIS_DIR),\n            analysis_combined_fp=self.get_fp(Folders.ANALYSIS_COMBINED),\n            configs_fp=self.get_fp(Folders.CONFIGS),\n            overwrite=True,  # TODO: remove overwrite\n        )\n\n    #####################################################################\n    #           EVALUATING DLC ANALYSIS AND BEHAV CLASSIFICATION\n    #####################################################################\n\n    def evaluate_vid(self, overwrite: bool) -&gt; dict:\n        \"\"\"\n        Evaluating preprocessed DLC data and scored_behavs data.\n\n        Parameters\n        ----------\n        funcs : _type_\n            _description_\n        overwrite : bool\n            Whether to overwrite the output file (if it exists).\n\n        Returns\n        -------\n        dict\n            Diagnostics dictionary, with description of each function's outcome.\n        \"\"\"\n        return self._proc_scaff(\n            (EvaluateVid.evaluate_vid,),\n            formatted_vid_fp=self.get_fp(Folders.FORMATTED_VID),\n            keypoints_fp=self.get_fp(Folders.PREPROCESSED),\n            analysis_combined_fp=self.get_fp(Folders.ANALYSIS_COMBINED),\n            eval_vid_fp=self.get_fp(Folders.EVALUATE_VID),\n            configs_fp=self.get_fp(Folders.CONFIGS),\n            overwrite=overwrite,\n        )\n\n    def export2csv(self, src_dir: str, dst_dir: str, overwrite: bool) -&gt; dict:\n        \"\"\"\n        _summary_\n\n        Parameters\n        ----------\n        src_dir : str\n            _description_\n        dst_dir : str\n            _description_\n\n        Returns\n        -------\n        dict\n            _description_\n        \"\"\"\n        return self._proc_scaff(\n            (Export.df2csv,),\n            src_fp=self.get_fp(src_dir),\n            dst_fp=os.path.join(dst_dir, f\"{self.name}.csv\"),\n            overwrite=overwrite,\n        )\n</code></pre>"},{"location":"reference/experiment.html#behavysis.pipeline.experiment.Experiment.__init__","title":"<code>__init__(name, root_dir)</code>","text":"<p>Make a Experiment instance.</p> Source code in <code>behavysis/pipeline/experiment.py</code> <pre><code>def __init__(self, name: str, root_dir: str) -&gt; None:\n    \"\"\"\n    Make a Experiment instance.\n    \"\"\"\n    # Assertion: root_dir mus\u2020 exist\n    if not os.path.isdir(root_dir):\n        raise ValueError(\n            f'Cannot find the project folder named \"{root_dir}\".\\nPlease specify a folder that exists.'\n        )\n    # Setting up instance variables\n    self.name = name\n    self.root_dir = os.path.abspath(root_dir)\n    # Assertion: name must correspond to at least one file in root_dir\n    file_exists_ls = [os.path.isfile(self.get_fp(f)) for f in Folders]\n    if not np.any(file_exists_ls):\n        folders_ls_msg = \"\".join([f\"\\n    - {f.value}\" for f in Folders])\n        raise ValueError(\n            f'No files named \"{name}\" exist in \"{root_dir}\".\\n'\n            f'Please specify a file that exists in \"{root_dir}\", '\n            f\"in one of the following folder WITH the correct file extension name:{folders_ls_msg}\"\n        )\n</code></pre>"},{"location":"reference/experiment.html#behavysis.pipeline.experiment.Experiment._proc_scaff","title":"<code>_proc_scaff(funcs, *args, **kwargs)</code>","text":"<p>All processing runs through here. This method ensures that the stdout and diagnostics dict are correctly generated.</p> <p>Parameters:</p> Name Type Description Default <code>funcs</code> <code>tuple[Callable, ...]</code> <p>List of functions.</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Diagnostics dictionary, with description of each function's outcome.</p> Notes <p>Each func in <code>funcs</code> is called in the form: <pre><code>func(*args, **kwargs)\n</code></pre></p> Source code in <code>behavysis/pipeline/experiment.py</code> <pre><code>def _proc_scaff(\n    self,\n    funcs: tuple[Callable, ...],\n    *args: Any,\n    **kwargs: Any,\n) -&gt; dict[str, str]:\n    \"\"\"\n    All processing runs through here.\n    This method ensures that the stdout and diagnostics dict are correctly generated.\n\n    Parameters\n    ----------\n    funcs : tuple[Callable, ...]\n        List of functions.\n\n    Returns\n    -------\n    dict[str, str]\n        Diagnostics dictionary, with description of each function's outcome.\n\n    Notes\n    -----\n    Each func in `funcs` is called in the form:\n    ```\n    func(*args, **kwargs)\n    ```\n    \"\"\"\n    f_names_ls_msg = \"\".join([f\"\\n    - {f.__name__}\" for f in funcs])\n    self.logger.info(f\"Processing experiment, {self.name}, with:{f_names_ls_msg}\")\n    # Setting up diagnostics dict\n    dd = {\"experiment\": self.name}\n    # Running functions and saving outcome to diagnostics dict\n    for f in funcs:\n        f_name = f.__name__\n        # Getting logger and corresponding io object\n        f_logger, f_io_obj = init_logger_io_obj(f_name)\n        # Running each func and saving outcome\n        try:\n            f(*args, **kwargs)\n            # f_logger.info(success_msg())\n        except Exception as e:\n            f_logger.error(e)\n            self.logger.debug(traceback.format_exc())\n        # Adding to diagnostics dict\n        dd[f_name] = get_io_obj_content(f_io_obj)\n        # Clearing io object\n        f_io_obj.truncate(0)\n    self.logger.info(f\"Finished processing experiment, {self.name}, with:{f_names_ls_msg}\")\n    return dd\n</code></pre>"},{"location":"reference/experiment.html#behavysis.pipeline.experiment.Experiment.analyse","title":"<code>analyse(funcs)</code>","text":"<p>An ML pipeline method to analyse the preprocessed DLC data. Possible funcs are given in analysis.py. The preprocessed data is saved to the project's analysis folder.</p> <p>Parameters:</p> Name Type Description Default <code>funcs</code> <code>tuple[Callable, ...]</code> <p>description</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Diagnostics dictionary, with description of each function's outcome.</p> Notes <p>Can call any methods from <code>Analyse</code>.</p> Source code in <code>behavysis/pipeline/experiment.py</code> <pre><code>def analyse(self, funcs: tuple[Callable, ...]) -&gt; dict:\n    \"\"\"\n    An ML pipeline method to analyse the preprocessed DLC data.\n    Possible funcs are given in analysis.py.\n    The preprocessed data is saved to the project's analysis folder.\n\n    Parameters\n    ----------\n    funcs : tuple[Callable, ...]\n        _description_\n\n    Returns\n    -------\n    dict\n        Diagnostics dictionary, with description of each function's outcome.\n\n    Notes\n    -----\n    Can call any methods from `Analyse`.\n    \"\"\"\n    return self._proc_scaff(\n        funcs,\n        keypoints_fp=self.get_fp(Folders.PREPROCESSED),\n        dst_dir=os.path.join(self.root_dir, ANALYSIS_DIR),\n        configs_fp=self.get_fp(Folders.CONFIGS),\n    )\n</code></pre>"},{"location":"reference/experiment.html#behavysis.pipeline.experiment.Experiment.analyse_behavs","title":"<code>analyse_behavs()</code>","text":"<p>An ML pipeline method to analyse the preprocessed DLC data. Possible funcs are given in analysis.py. The preprocessed data is saved to the project's analysis folder.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Diagnostics dictionary, with description of each function's outcome.</p> Notes <p>Can call any methods from <code>Analyse</code>.</p> Source code in <code>behavysis/pipeline/experiment.py</code> <pre><code>def analyse_behavs(self) -&gt; dict:\n    \"\"\"\n    An ML pipeline method to analyse the preprocessed DLC data.\n    Possible funcs are given in analysis.py.\n    The preprocessed data is saved to the project's analysis folder.\n\n    Returns\n    -------\n    dict\n        Diagnostics dictionary, with description of each function's outcome.\n\n    Notes\n    -----\n    Can call any methods from `Analyse`.\n    \"\"\"\n    return self._proc_scaff(\n        (AnalyseBehavs.analyse_behavs,),\n        behavs_fp=self.get_fp(Folders.SCORED_BEHAVS),\n        dst_dir=os.path.join(self.root_dir, ANALYSIS_DIR),\n        configs_fp=self.get_fp(Folders.CONFIGS),\n    )\n</code></pre>"},{"location":"reference/experiment.html#behavysis.pipeline.experiment.Experiment.calculate_parameters","title":"<code>calculate_parameters(funcs)</code>","text":"<p>A pipeline to calculate the parameters of the keypoints file, which will assist in preprocessing the keypoints data.</p> <p>Parameters:</p> Name Type Description Default <code>funcs</code> <code>tuple[Callable, ...]</code> <p>description</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>Diagnostics dictionary, with description of each function's outcome.</p> Notes <p>Can call any methods from <code>CalculateParams</code>.</p> Source code in <code>behavysis/pipeline/experiment.py</code> <pre><code>def calculate_parameters(self, funcs: tuple[Callable, ...]) -&gt; dict:\n    \"\"\"\n    A pipeline to calculate the parameters of the keypoints file, which will\n    assist in preprocessing the keypoints data.\n\n    Parameters\n    ----------\n    funcs : tuple[Callable, ...]\n        _description_\n\n    Returns\n    -------\n    Dict\n        Diagnostics dictionary, with description of each function's outcome.\n\n    Notes\n    -----\n    Can call any methods from `CalculateParams`.\n    \"\"\"\n    return self._proc_scaff(\n        funcs,\n        keypoints_fp=self.get_fp(Folders.KEYPOINTS),\n        configs_fp=self.get_fp(Folders.CONFIGS),\n    )\n</code></pre>"},{"location":"reference/experiment.html#behavysis.pipeline.experiment.Experiment.classify_behavs","title":"<code>classify_behavs(overwrite)</code>","text":"<p>Given model config files in the BehavClassifier format, generates beahviour predidctions on the given extracted features dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file (if it exists).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Diagnostics dictionary, with description of each function's outcome.</p> Source code in <code>behavysis/pipeline/experiment.py</code> <pre><code>def classify_behavs(self, overwrite: bool) -&gt; dict:\n    \"\"\"\n    Given model config files in the BehavClassifier format, generates beahviour predidctions\n    on the given extracted features dataframe.\n\n    Parameters\n    ----------\n    overwrite : bool\n        Whether to overwrite the output file (if it exists).\n\n    Returns\n    -------\n    dict\n        Diagnostics dictionary, with description of each function's outcome.\n    \"\"\"\n    return self._proc_scaff(\n        (ClassifyBehavs.classify_behavs,),\n        features_fp=self.get_fp(Folders.FEATURES_EXTRACTED),\n        behavs_fp=self.get_fp(Folders.PREDICTED_BEHAVS),\n        configs_fp=self.get_fp(Folders.CONFIGS),\n        overwrite=overwrite,\n    )\n</code></pre>"},{"location":"reference/experiment.html#behavysis.pipeline.experiment.Experiment.collate_auto_configs","title":"<code>collate_auto_configs()</code>","text":"<p>Collates the auto-configs of the experiment into the main configs file.</p> Source code in <code>behavysis/pipeline/experiment.py</code> <pre><code>def collate_auto_configs(self) -&gt; Dict:\n    \"\"\"\n    Collates the auto-configs of the experiment into the main configs file.\n    \"\"\"\n    dd = {\"experiment\": self.name}\n    # Reading the experiment's configs file\n    f_logger, f_io_obj = init_logger_io_obj()\n    try:\n        configs = ExperimentConfigs.read_json(self.get_fp(Folders.CONFIGS))\n        f_logger.debug(\"Reading configs file.\")\n        # f_logger.info(success_msg())\n        dd[\"reading_configs\"] = get_io_obj_content(f_io_obj)\n    except FileNotFoundError:\n        f_logger.error(\"no configs file found.\")\n        dd[\"reading_configs\"] = get_io_obj_content(f_io_obj)\n        return dd\n    # Getting all the auto fields from the configs file\n    configs_auto_field_keys = AutoConfigs.get_field_names()\n    for field_key_ls in configs_auto_field_keys:\n        value = configs.auto\n        for key in field_key_ls:\n            value = getattr(value, key)\n        dd[\"_\".join(field_key_ls)] = value  # type: ignore\n    return dd\n</code></pre>"},{"location":"reference/experiment.html#behavysis.pipeline.experiment.Experiment.combine_analysis","title":"<code>combine_analysis()</code>","text":"<p>Combine the experiment's analysis in each fbf into a single df</p> Source code in <code>behavysis/pipeline/experiment.py</code> <pre><code>def combine_analysis(self) -&gt; dict:\n    \"\"\"\n    Combine the experiment's analysis in each fbf into a single df\n    \"\"\"\n    # TODO: make new subfolder called combined_analysis and make ONLY(??) fbf analysis.\n    return self._proc_scaff(\n        (CombineAnalysis.combine_analysis,),\n        analysis_dir=os.path.join(self.root_dir, ANALYSIS_DIR),\n        analysis_combined_fp=self.get_fp(Folders.ANALYSIS_COMBINED),\n        configs_fp=self.get_fp(Folders.CONFIGS),\n        overwrite=True,  # TODO: remove overwrite\n    )\n</code></pre>"},{"location":"reference/experiment.html#behavysis.pipeline.experiment.Experiment.evaluate_vid","title":"<code>evaluate_vid(overwrite)</code>","text":"<p>Evaluating preprocessed DLC data and scored_behavs data.</p> <p>Parameters:</p> Name Type Description Default <code>funcs</code> <code>_type_</code> <p>description</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file (if it exists).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Diagnostics dictionary, with description of each function's outcome.</p> Source code in <code>behavysis/pipeline/experiment.py</code> <pre><code>def evaluate_vid(self, overwrite: bool) -&gt; dict:\n    \"\"\"\n    Evaluating preprocessed DLC data and scored_behavs data.\n\n    Parameters\n    ----------\n    funcs : _type_\n        _description_\n    overwrite : bool\n        Whether to overwrite the output file (if it exists).\n\n    Returns\n    -------\n    dict\n        Diagnostics dictionary, with description of each function's outcome.\n    \"\"\"\n    return self._proc_scaff(\n        (EvaluateVid.evaluate_vid,),\n        formatted_vid_fp=self.get_fp(Folders.FORMATTED_VID),\n        keypoints_fp=self.get_fp(Folders.PREPROCESSED),\n        analysis_combined_fp=self.get_fp(Folders.ANALYSIS_COMBINED),\n        eval_vid_fp=self.get_fp(Folders.EVALUATE_VID),\n        configs_fp=self.get_fp(Folders.CONFIGS),\n        overwrite=overwrite,\n    )\n</code></pre>"},{"location":"reference/experiment.html#behavysis.pipeline.experiment.Experiment.export2csv","title":"<code>export2csv(src_dir, dst_dir, overwrite)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>src_dir</code> <code>str</code> <p>description</p> required <code>dst_dir</code> <code>str</code> <p>description</p> required <p>Returns:</p> Type Description <code>dict</code> <p>description</p> Source code in <code>behavysis/pipeline/experiment.py</code> <pre><code>def export2csv(self, src_dir: str, dst_dir: str, overwrite: bool) -&gt; dict:\n    \"\"\"\n    _summary_\n\n    Parameters\n    ----------\n    src_dir : str\n        _description_\n    dst_dir : str\n        _description_\n\n    Returns\n    -------\n    dict\n        _description_\n    \"\"\"\n    return self._proc_scaff(\n        (Export.df2csv,),\n        src_fp=self.get_fp(src_dir),\n        dst_fp=os.path.join(dst_dir, f\"{self.name}.csv\"),\n        overwrite=overwrite,\n    )\n</code></pre>"},{"location":"reference/experiment.html#behavysis.pipeline.experiment.Experiment.export_behavs","title":"<code>export_behavs(overwrite)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>description</p> required <p>Returns:</p> Type Description <code>dict</code> <p>description</p> Source code in <code>behavysis/pipeline/experiment.py</code> <pre><code>def export_behavs(self, overwrite: bool) -&gt; dict:\n    \"\"\"\n    _summary_\n\n    Parameters\n    ----------\n    overwrite : bool\n        _description_\n\n    Returns\n    -------\n    dict\n        _description_\n    \"\"\"\n    # Exporting 6_predicted_behavs df to 7_scored_behavs folder\n    return self._proc_scaff(\n        (Export.predictedbehavs2scoredbehavs,),\n        src_fp=self.get_fp(Folders.PREDICTED_BEHAVS),\n        dst_fp=self.get_fp(Folders.SCORED_BEHAVS),\n        configs_fp=self.get_fp(Folders.CONFIGS),\n        overwrite=overwrite,\n    )\n</code></pre>"},{"location":"reference/experiment.html#behavysis.pipeline.experiment.Experiment.extract_features","title":"<code>extract_features(overwrite)</code>","text":"<p>Extracts features from the preprocessed dlc file to generate many more features. This dataframe of derived features will be input for a ML classifier to detect particularly trained behaviours.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file (if it exists).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Diagnostics dictionary, with description of each function's outcome.</p> Source code in <code>behavysis/pipeline/experiment.py</code> <pre><code>def extract_features(self, overwrite: bool) -&gt; dict:\n    \"\"\"\n    Extracts features from the preprocessed dlc file to generate many more features.\n    This dataframe of derived features will be input for a ML classifier to detect\n    particularly trained behaviours.\n\n    Parameters\n    ----------\n    overwrite : bool\n        Whether to overwrite the output file (if it exists).\n\n    Returns\n    -------\n    dict\n        Diagnostics dictionary, with description of each function's outcome.\n    \"\"\"\n    return self._proc_scaff(\n        (ExtractFeatures.extract_features,),\n        keypoints_fp=self.get_fp(Folders.PREPROCESSED),\n        features_fp=self.get_fp(Folders.FEATURES_EXTRACTED),\n        configs_fp=self.get_fp(Folders.CONFIGS),\n        overwrite=overwrite,\n    )\n</code></pre>"},{"location":"reference/experiment.html#behavysis.pipeline.experiment.Experiment.format_vid","title":"<code>format_vid(overwrite)</code>","text":"<p>Formats the video with ffmpeg to fit the formatted configs (e.g. fps and resolution_px). Once the formatted video is produced, the configs dict and *configs.json file are updated with the formatted video's metadata.</p> <p>Parameters:</p> Name Type Description Default <code>funcs</code> <code>tuple[Callable, ...]</code> <p>description</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file (if it exists).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Diagnostics dictionary, with description of each function's outcome.</p> Notes <p>Can call any methods from <code>FormatVid</code>.</p> Source code in <code>behavysis/pipeline/experiment.py</code> <pre><code>def format_vid(self, overwrite: bool) -&gt; dict:\n    \"\"\"\n    Formats the video with ffmpeg to fit the formatted configs (e.g. fps and resolution_px).\n    Once the formatted video is produced, the configs dict and *configs.json file are\n    updated with the formatted video's metadata.\n\n    Parameters\n    ----------\n    funcs : tuple[Callable, ...]\n        _description_\n    overwrite : bool\n        Whether to overwrite the output file (if it exists).\n\n    Returns\n    -------\n    dict\n        Diagnostics dictionary, with description of each function's outcome.\n\n    Notes\n    -----\n    Can call any methods from `FormatVid`.\n    \"\"\"\n    return self._proc_scaff(\n        (FormatVid.format_vid,),\n        raw_vid_fp=self.get_fp(Folders.RAW_VID),\n        formatted_vid_fp=self.get_fp(Folders.FORMATTED_VID),\n        configs_fp=self.get_fp(Folders.CONFIGS),\n        overwrite=overwrite,\n    )\n</code></pre>"},{"location":"reference/experiment.html#behavysis.pipeline.experiment.Experiment.get_fp","title":"<code>get_fp(_folder)</code>","text":"<p>Returns the experiment's file path from the given folder.</p> <p>Parameters:</p> Name Type Description Default <code>folder_str</code> <code>str</code> <p>The folder to return the experiment document's filepath for.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The experiment document's filepath.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>ValueError: Folder name is not valid. Refer to Folders Enum for valid folder names.</p> Source code in <code>behavysis/pipeline/experiment.py</code> <pre><code>def get_fp(self, _folder: Folders | str) -&gt; str:\n    \"\"\"\n    Returns the experiment's file path from the given folder.\n\n    Parameters\n    ----------\n    folder_str : str\n        The folder to return the experiment document's filepath for.\n\n    Returns\n    -------\n    str\n        The experiment document's filepath.\n\n    Raises\n    ------\n    ValueError\n        ValueError: Folder name is not valid. Refer to Folders Enum for valid folder names.\n    \"\"\"\n    # Getting Folder item\n    if isinstance(_folder, str):\n        try:\n            folder = Folders(_folder)\n        except ValueError:\n            folders_ls_msg = \"\".join([f\"\\n    - {f.value}\" for f in Folders])\n            raise ValueError(\n                f\"{_folder} is not a valid experiment folder name.\\n\"\n                f\"Please only specify one of the following folders:{folders_ls_msg}\"\n            )\n    else:\n        folder = _folder\n    # Getting file extension from enum\n    file_ext: FileExts = getattr(FileExts, folder.name)\n    # Getting experiment filepath for given folder\n    fp = os.path.join(self.root_dir, folder.value, f\"{self.name}.{file_ext.value}\")\n    return fp\n</code></pre>"},{"location":"reference/experiment.html#behavysis.pipeline.experiment.Experiment.get_vid_metadata","title":"<code>get_vid_metadata()</code>","text":"<p>Gets the video metadata for the raw and formatted video files.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>description</p> required <p>Returns:</p> Type Description <code>dict</code> <p>description</p> Source code in <code>behavysis/pipeline/experiment.py</code> <pre><code>def get_vid_metadata(self) -&gt; dict:\n    \"\"\"\n    Gets the video metadata for the raw and formatted video files.\n\n    Parameters\n    ----------\n    overwrite : bool\n        _description_\n\n    Returns\n    -------\n    dict\n        _description_\n    \"\"\"\n    return self._proc_scaff(\n        (FormatVid.get_vids_metadata,),\n        raw_vid_fp=self.get_fp(Folders.RAW_VID),\n        formatted_vid_fp=self.get_fp(Folders.FORMATTED_VID),\n        configs_fp=self.get_fp(Folders.CONFIGS),\n    )\n</code></pre>"},{"location":"reference/experiment.html#behavysis.pipeline.experiment.Experiment.preprocess","title":"<code>preprocess(funcs, overwrite)</code>","text":"<p>A preprocessing pipeline method to convert raw keypoints data into preprocessed keypoints data that is ready for ML analysis. All functs passed in must have the format func(df, dict) -&gt; df. Possible funcs are given in preprocessing.py The preprocessed data is saved to the project's preprocessed folder.</p> <p>Parameters:</p> Name Type Description Default <code>funcs</code> <code>tuple[Callable, ...]</code> <p>description</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file (if it exists).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Diagnostics dictionary, with description of each function's outcome.</p> Notes <p>Can call any methods from <code>Preprocess</code>.</p> Source code in <code>behavysis/pipeline/experiment.py</code> <pre><code>def preprocess(self, funcs: tuple[Callable, ...], overwrite: bool) -&gt; dict:\n    \"\"\"\n    A preprocessing pipeline method to convert raw keypoints data into preprocessed\n    keypoints data that is ready for ML analysis.\n    All functs passed in must have the format func(df, dict) -&gt; df. Possible funcs\n    are given in preprocessing.py\n    The preprocessed data is saved to the project's preprocessed folder.\n\n    Parameters\n    ----------\n    funcs : tuple[Callable, ...]\n        _description_\n    overwrite : bool\n        Whether to overwrite the output file (if it exists).\n\n    Returns\n    -------\n    dict\n        Diagnostics dictionary, with description of each function's outcome.\n\n    Notes\n    -----\n    Can call any methods from `Preprocess`.\n    \"\"\"\n    # Exporting keypoints df to preprocessed folder\n    dd0 = self._proc_scaff(\n        (Export.df2df,),\n        src_fp=self.get_fp(Folders.KEYPOINTS),\n        dst_fp=self.get_fp(Folders.PREPROCESSED),\n        overwrite=overwrite,\n    )\n    # If there is an error or warning (indicates not to ovewrite) in logger, return early\n    if \"ERROR\" in dd0[Export.df2df.__name__] or \"WARNING\" in dd0[Export.df2df.__name__]:\n        return dd0\n    # Feeding through preprocessing functions\n    dd1 = self._proc_scaff(\n        funcs,\n        src_fp=self.get_fp(Folders.PREPROCESSED),\n        dst_fp=self.get_fp(Folders.PREPROCESSED),\n        configs_fp=self.get_fp(Folders.CONFIGS),\n        overwrite=True,\n    )\n    return {**dd0, **dd1}\n</code></pre>"},{"location":"reference/experiment.html#behavysis.pipeline.experiment.Experiment.run_dlc","title":"<code>run_dlc(gputouse, overwrite)</code>","text":"<p>Run the DLC model on the formatted video to generate a DLC annotated video and DLC h5 file for all experiments.</p> <p>Parameters:</p> Name Type Description Default <code>gputouse</code> <code>int</code> <p>description</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file (if it exists).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Diagnostics dictionary, with description of each function's outcome.</p> Notes <p>Can call any methods from <code>RunDLC</code>.</p> Source code in <code>behavysis/pipeline/experiment.py</code> <pre><code>def run_dlc(self, gputouse: int | None, overwrite: bool) -&gt; dict:\n    \"\"\"\n    Run the DLC model on the formatted video to generate a DLC annotated video\n    and DLC h5 file for all experiments.\n\n    Parameters\n    ----------\n    gputouse : int\n        _description_\n    overwrite : bool\n        Whether to overwrite the output file (if it exists).\n\n    Returns\n    -------\n    dict\n        Diagnostics dictionary, with description of each function's outcome.\n\n    Notes\n    -----\n    Can call any methods from `RunDLC`.\n    \"\"\"\n    return self._proc_scaff(\n        (RunDLC.ma_dlc_run_single,),\n        formatted_vid_fp=self.get_fp(Folders.FORMATTED_VID),\n        keypoints_fp=self.get_fp(Folders.KEYPOINTS),\n        configs_fp=self.get_fp(Folders.CONFIGS),\n        gputouse=gputouse,\n        overwrite=overwrite,\n    )\n</code></pre>"},{"location":"reference/experiment.html#behavysis.pipeline.experiment.Experiment.update_configs","title":"<code>update_configs(default_configs_fp, overwrite)</code>","text":"<p>Initialises the JSON config files with the given configurations in <code>configs</code>. It can be specified whether or not to overwrite existing configuration values.</p> <p>Parameters:</p> Name Type Description Default <code>default_configs_fp</code> <code>str</code> <p>The JSON configs filepath to add/overwrite to the experiment's current configs file.</p> required <code>overwrite</code> <code>(set, reset)</code> <p>Specifies how to overwrite existing configurations. If <code>add</code>, only parameters in <code>configs</code> not already in the config files are added. If <code>set</code>, all parameters in <code>configs</code> are set in the config files (overwriting). If <code>reset</code>, the config files are completely replaced by <code>configs</code>.</p> <code>\"set\"</code> <p>Returns:</p> Type Description <code>dict</code> <p>Diagnostics dictionary, with description of each function's outcome.</p> Source code in <code>behavysis/pipeline/experiment.py</code> <pre><code>def update_configs(self, default_configs_fp: str, overwrite: str) -&gt; dict:\n    \"\"\"\n    Initialises the JSON config files with the given configurations in `configs`.\n    It can be specified whether or not to overwrite existing configuration values.\n\n    Parameters\n    ----------\n    default_configs_fp : str\n        The JSON configs filepath to add/overwrite to the experiment's current configs file.\n    overwrite : {\"set\", \"reset\"}\n        Specifies how to overwrite existing configurations.\n        If `add`, only parameters in `configs` not already in the config files are added.\n        If `set`, all parameters in `configs` are set in the config files (overwriting).\n        If `reset`, the config files are completely replaced by `configs`.\n\n    Returns\n    -------\n    dict\n        Diagnostics dictionary, with description of each function's outcome.\n    \"\"\"\n    return self._proc_scaff(\n        (UpdateConfigs.update_configs,),\n        configs_fp=self.get_fp(Folders.CONFIGS),\n        default_configs_fp=default_configs_fp,\n        overwrite=overwrite,\n    )\n</code></pre>"},{"location":"reference/processes.html","title":"Processes","text":""},{"location":"reference/processes.html#behavysis.processes.update_configs.UpdateConfigs","title":"<code>behavysis.processes.update_configs.UpdateConfigs</code>","text":"<p>summary</p> Source code in <code>behavysis/processes/update_configs.py</code> <pre><code>class UpdateConfigs:\n    \"\"\"_summary_\"\"\"\n\n    @staticmethod\n    def update_configs(\n        configs_fp: str,\n        default_configs_fp: str,\n        overwrite: Literal[\"user\", \"all\"],\n    ) -&gt; str:\n        \"\"\"\n        Initialises the config files with the given `default_configs`.\n        The different types of overwriting are:\n        - \"user\": Only the user parameters are updated.\n        - \"all\": All parameters are updated.\n\n        Parameters\n        ----------\n        configs_fp : str\n            The filepath of the existing config file.\n        default_configs_fp : str\n            The filepath of the default config file to use.\n        overwrite : Literal[\"user\", \"all\"]\n            Specifies how to update the config files.\n\n        Returns\n        -------\n        str\n            Description of the function's outcome.\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        # Parsing in the experiment's existing JSON configs\n        try:\n            configs = ExperimentConfigs.read_json(configs_fp)\n        except (FileNotFoundError, ValidationError):\n            configs = ExperimentConfigs()\n        # Reading in the new configs from the given configs_fp\n        default_configs = ExperimentConfigs.read_json(default_configs_fp)\n        # Overwriting the configs file (with given method)\n        if overwrite == \"user\":\n            configs.user = default_configs.user\n            configs.ref = default_configs.ref\n            logger.info(\"Updating user and ref configs.\")\n        elif overwrite == \"all\":\n            configs = default_configs\n            logger.info(\"Updating all configs.\")\n        else:\n            raise ValueError(\n                f'Invalid value \"{overwrite}\" passed to function. ' 'The value must be either \"user\", or \"all\".'\n            )\n        # Writing new configs to JSON file\n        configs.write_json(configs_fp)\n        return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.update_configs.UpdateConfigs.update_configs","title":"<code>update_configs(configs_fp, default_configs_fp, overwrite)</code>  <code>staticmethod</code>","text":"<p>Initialises the config files with the given <code>default_configs</code>. The different types of overwriting are: - \"user\": Only the user parameters are updated. - \"all\": All parameters are updated.</p> <p>Parameters:</p> Name Type Description Default <code>configs_fp</code> <code>str</code> <p>The filepath of the existing config file.</p> required <code>default_configs_fp</code> <code>str</code> <p>The filepath of the default config file to use.</p> required <code>overwrite</code> <code>Literal['user', 'all']</code> <p>Specifies how to update the config files.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Description of the function's outcome.</p> Source code in <code>behavysis/processes/update_configs.py</code> <pre><code>@staticmethod\ndef update_configs(\n    configs_fp: str,\n    default_configs_fp: str,\n    overwrite: Literal[\"user\", \"all\"],\n) -&gt; str:\n    \"\"\"\n    Initialises the config files with the given `default_configs`.\n    The different types of overwriting are:\n    - \"user\": Only the user parameters are updated.\n    - \"all\": All parameters are updated.\n\n    Parameters\n    ----------\n    configs_fp : str\n        The filepath of the existing config file.\n    default_configs_fp : str\n        The filepath of the default config file to use.\n    overwrite : Literal[\"user\", \"all\"]\n        Specifies how to update the config files.\n\n    Returns\n    -------\n    str\n        Description of the function's outcome.\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    # Parsing in the experiment's existing JSON configs\n    try:\n        configs = ExperimentConfigs.read_json(configs_fp)\n    except (FileNotFoundError, ValidationError):\n        configs = ExperimentConfigs()\n    # Reading in the new configs from the given configs_fp\n    default_configs = ExperimentConfigs.read_json(default_configs_fp)\n    # Overwriting the configs file (with given method)\n    if overwrite == \"user\":\n        configs.user = default_configs.user\n        configs.ref = default_configs.ref\n        logger.info(\"Updating user and ref configs.\")\n    elif overwrite == \"all\":\n        configs = default_configs\n        logger.info(\"Updating all configs.\")\n    else:\n        raise ValueError(\n            f'Invalid value \"{overwrite}\" passed to function. ' 'The value must be either \"user\", or \"all\".'\n        )\n    # Writing new configs to JSON file\n    configs.write_json(configs_fp)\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.format_vid.FormatVid","title":"<code>behavysis.processes.format_vid.FormatVid</code>","text":"<p>Class for formatting videos based on given parameters.</p> Source code in <code>behavysis/processes/format_vid.py</code> <pre><code>class FormatVid:\n    \"\"\"\n    Class for formatting videos based on given parameters.\n    \"\"\"\n\n    @classmethod\n    def format_vid(cls, raw_vid_fp: str, formatted_vid_fp: str, configs_fp: str, overwrite: bool) -&gt; str:\n        \"\"\"\n        Formats the input video with the given parameters.\n\n        Parameters\n        ----------\n        raw_fp : str\n            The input video filepath.\n        formatted_fp : str\n            The output video filepath.\n        configs_fp : str\n            The JSON configs filepath.\n        overwrite : bool\n            Whether to overwrite the output file (if it exists).\n\n        Returns\n        -------\n        str\n            Description of the function's outcome.\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        if not overwrite and os.path.exists(formatted_vid_fp):\n            logger.warning(file_exists_msg(formatted_vid_fp))\n            return get_io_obj_content(io_obj)\n        # Finding all necessary config parameters for video formatting\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs_filt = configs.user.format_vid\n        # Processing the video\n        ffmpeg_process_vid(\n            in_fp=raw_vid_fp,\n            dst_fp=formatted_vid_fp,\n            logger=logger,\n            width_px=configs.get_ref(configs_filt.width_px),\n            height_px=configs.get_ref(configs_filt.height_px),\n            fps=configs.get_ref(configs_filt.fps),\n            start_sec=configs.get_ref(configs_filt.start_sec),\n            stop_sec=configs.get_ref(configs_filt.stop_sec),\n        )\n        cls.get_vids_metadata(raw_vid_fp, formatted_vid_fp, configs_fp)\n        return get_io_obj_content(io_obj)\n\n    @classmethod\n    def get_vids_metadata(cls, raw_vid_fp: str, formatted_vid_fp: str, configs_fp: str) -&gt; str:\n        \"\"\"\n        Finds the video metadata/parameters for either the raw or formatted video,\n        and stores this data in the experiment's config file.\n\n        Parameters\n        ----------\n        raw_fp : str\n            The input video filepath.\n        formatted_fp : str\n            The output video filepath.\n        configs_fp : str\n            The JSON configs filepath.\n\n        Returns\n        -------\n        str\n            Description of the function's outcome.\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        # Saving video metadata to configs dict\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs.auto.raw_vid = get_vid_metadata(raw_vid_fp, logger)\n        configs.auto.formatted_vid = get_vid_metadata(formatted_vid_fp, logger)\n        logger.info(\"Video metadata stored in config file.\")\n        configs.write_json(configs_fp)\n        return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.format_vid.FormatVid.format_vid","title":"<code>format_vid(raw_vid_fp, formatted_vid_fp, configs_fp, overwrite)</code>  <code>classmethod</code>","text":"<p>Formats the input video with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>raw_fp</code> <code>str</code> <p>The input video filepath.</p> required <code>formatted_fp</code> <code>str</code> <p>The output video filepath.</p> required <code>configs_fp</code> <code>str</code> <p>The JSON configs filepath.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file (if it exists).</p> required <p>Returns:</p> Type Description <code>str</code> <p>Description of the function's outcome.</p> Source code in <code>behavysis/processes/format_vid.py</code> <pre><code>@classmethod\ndef format_vid(cls, raw_vid_fp: str, formatted_vid_fp: str, configs_fp: str, overwrite: bool) -&gt; str:\n    \"\"\"\n    Formats the input video with the given parameters.\n\n    Parameters\n    ----------\n    raw_fp : str\n        The input video filepath.\n    formatted_fp : str\n        The output video filepath.\n    configs_fp : str\n        The JSON configs filepath.\n    overwrite : bool\n        Whether to overwrite the output file (if it exists).\n\n    Returns\n    -------\n    str\n        Description of the function's outcome.\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    if not overwrite and os.path.exists(formatted_vid_fp):\n        logger.warning(file_exists_msg(formatted_vid_fp))\n        return get_io_obj_content(io_obj)\n    # Finding all necessary config parameters for video formatting\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs_filt = configs.user.format_vid\n    # Processing the video\n    ffmpeg_process_vid(\n        in_fp=raw_vid_fp,\n        dst_fp=formatted_vid_fp,\n        logger=logger,\n        width_px=configs.get_ref(configs_filt.width_px),\n        height_px=configs.get_ref(configs_filt.height_px),\n        fps=configs.get_ref(configs_filt.fps),\n        start_sec=configs.get_ref(configs_filt.start_sec),\n        stop_sec=configs.get_ref(configs_filt.stop_sec),\n    )\n    cls.get_vids_metadata(raw_vid_fp, formatted_vid_fp, configs_fp)\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.format_vid.FormatVid.get_vids_metadata","title":"<code>get_vids_metadata(raw_vid_fp, formatted_vid_fp, configs_fp)</code>  <code>classmethod</code>","text":"<p>Finds the video metadata/parameters for either the raw or formatted video, and stores this data in the experiment's config file.</p> <p>Parameters:</p> Name Type Description Default <code>raw_fp</code> <code>str</code> <p>The input video filepath.</p> required <code>formatted_fp</code> <code>str</code> <p>The output video filepath.</p> required <code>configs_fp</code> <code>str</code> <p>The JSON configs filepath.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Description of the function's outcome.</p> Source code in <code>behavysis/processes/format_vid.py</code> <pre><code>@classmethod\ndef get_vids_metadata(cls, raw_vid_fp: str, formatted_vid_fp: str, configs_fp: str) -&gt; str:\n    \"\"\"\n    Finds the video metadata/parameters for either the raw or formatted video,\n    and stores this data in the experiment's config file.\n\n    Parameters\n    ----------\n    raw_fp : str\n        The input video filepath.\n    formatted_fp : str\n        The output video filepath.\n    configs_fp : str\n        The JSON configs filepath.\n\n    Returns\n    -------\n    str\n        Description of the function's outcome.\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    # Saving video metadata to configs dict\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs.auto.raw_vid = get_vid_metadata(raw_vid_fp, logger)\n    configs.auto.formatted_vid = get_vid_metadata(formatted_vid_fp, logger)\n    logger.info(\"Video metadata stored in config file.\")\n    configs.write_json(configs_fp)\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.run_dlc.RunDLC","title":"<code>behavysis.processes.run_dlc.RunDLC</code>","text":"<p>summary</p> Source code in <code>behavysis/processes/run_dlc.py</code> <pre><code>class RunDLC:\n    \"\"\"_summary_\"\"\"\n\n    @classmethod\n    def ma_dlc_run_single(\n        cls,\n        formatted_vid_fp: str,\n        keypoints_fp: str,\n        configs_fp: str,\n        gputouse: int | None,\n        overwrite: bool,\n    ) -&gt; str:\n        \"\"\"\n        Running custom DLC script to generate a DLC keypoints dataframe from a single video.\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        if not overwrite and os.path.exists(keypoints_fp):\n            logger.warning(file_exists_msg(keypoints_fp))\n            return get_io_obj_content(io_obj)\n        # Getting model_fp\n        configs = ExperimentConfigs.read_json(configs_fp)\n        model_fp = configs.get_ref(configs.user.run_dlc.model_fp)\n        # Derive more parameters\n        temp_dlc_dir = os.path.join(CACHE_DIR, f\"dlc_{gputouse}\")\n        keypoints_dir = os.path.dirname(keypoints_fp)\n        # Making output directories\n        os.makedirs(temp_dlc_dir, exist_ok=True)\n\n        # Assertion: the config.yaml file must exist.\n        if not os.path.isfile(model_fp):\n            raise ValueError(\n                f'The given model_fp file does not exist: \"{model_fp}\".\\n'\n                + 'Check this file and specify a DLC \".yaml\" config file.'\n            )\n\n        # Running the DLC subprocess (in a separate conda env)\n        run_dlc_subproc(model_fp, [formatted_vid_fp], temp_dlc_dir, CACHE_DIR, gputouse, logger)\n\n        # Exporting the h5 to chosen file format\n        export2df(formatted_vid_fp, temp_dlc_dir, keypoints_dir, logger)\n        silent_remove(temp_dlc_dir)\n\n        return get_io_obj_content(io_obj)\n\n    @staticmethod\n    def ma_dlc_run_batch(\n        vid_fp_ls: list[str],\n        keypoints_dir: str,\n        configs_dir: str,\n        gputouse: int | None,\n        overwrite: bool,\n    ) -&gt; str:\n        \"\"\"\n        Running custom DLC script to generate a DLC keypoints dataframe from a single video.\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n\n        # Specifying the GPU to use and making the output directory\n        # Making output directories\n        temp_dlc_dir = os.path.join(CACHE_DIR, f\"dlc_{gputouse}\")\n        os.makedirs(temp_dlc_dir, exist_ok=True)\n\n        # If overwrite is False, filtering for only experiments that need processing\n        if not overwrite:\n            # Getting only the vid_fp_ls elements that do not exist in keypoints_dir\n            vid_fp_ls = [\n                vid_fp\n                for vid_fp in vid_fp_ls\n                if not os.path.exists(os.path.join(keypoints_dir, f\"{get_name(vid_fp)}.{KeypointsDf.IO}\"))\n            ]\n\n        # If there are no videos to process, return\n        if len(vid_fp_ls) == 0:\n            return get_io_obj_content(io_obj)\n\n        # Getting the DLC model config path\n        # Getting the names of the files that need processing\n        dlc_fp_ls = [get_name(i) for i in vid_fp_ls]\n        # Getting their corresponding configs_fp\n        dlc_fp_ls = [os.path.join(configs_dir, f\"{i}.json\") for i in dlc_fp_ls]\n        # Reading their configs\n        dlc_fp_ls = [ExperimentConfigs.read_json(i) for i in dlc_fp_ls]\n        # Getting their model_fp\n        dlc_fp_ls = [i.user.run_dlc.model_fp for i in dlc_fp_ls]\n        # Converting to a set\n        dlc_fp_set = set(dlc_fp_ls)\n        # Assertion: all model_fp must be the same\n        assert len(dlc_fp_set) == 1\n        # Getting the model_fp\n        model_fp = dlc_fp_set.pop()\n        # Assertion: the config.yaml file must exist.\n        assert os.path.isfile(model_fp), (\n            f'The given model_fp file does not exist: \"{model_fp}\".\\n'\n            + 'Check this file and specify a DLC \".yaml\" config file.'\n        )\n\n        # Running the DLC subprocess (in a separate conda env)\n        run_dlc_subproc(model_fp, vid_fp_ls, temp_dlc_dir, CACHE_DIR, gputouse, logger)\n\n        # Exporting the h5 to chosen file format\n        for vid_fp in vid_fp_ls:\n            export2df(vid_fp, temp_dlc_dir, keypoints_dir, logger)\n        silent_remove(temp_dlc_dir)\n        return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.run_dlc.RunDLC.ma_dlc_run_batch","title":"<code>ma_dlc_run_batch(vid_fp_ls, keypoints_dir, configs_dir, gputouse, overwrite)</code>  <code>staticmethod</code>","text":"<p>Running custom DLC script to generate a DLC keypoints dataframe from a single video.</p> Source code in <code>behavysis/processes/run_dlc.py</code> <pre><code>@staticmethod\ndef ma_dlc_run_batch(\n    vid_fp_ls: list[str],\n    keypoints_dir: str,\n    configs_dir: str,\n    gputouse: int | None,\n    overwrite: bool,\n) -&gt; str:\n    \"\"\"\n    Running custom DLC script to generate a DLC keypoints dataframe from a single video.\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n\n    # Specifying the GPU to use and making the output directory\n    # Making output directories\n    temp_dlc_dir = os.path.join(CACHE_DIR, f\"dlc_{gputouse}\")\n    os.makedirs(temp_dlc_dir, exist_ok=True)\n\n    # If overwrite is False, filtering for only experiments that need processing\n    if not overwrite:\n        # Getting only the vid_fp_ls elements that do not exist in keypoints_dir\n        vid_fp_ls = [\n            vid_fp\n            for vid_fp in vid_fp_ls\n            if not os.path.exists(os.path.join(keypoints_dir, f\"{get_name(vid_fp)}.{KeypointsDf.IO}\"))\n        ]\n\n    # If there are no videos to process, return\n    if len(vid_fp_ls) == 0:\n        return get_io_obj_content(io_obj)\n\n    # Getting the DLC model config path\n    # Getting the names of the files that need processing\n    dlc_fp_ls = [get_name(i) for i in vid_fp_ls]\n    # Getting their corresponding configs_fp\n    dlc_fp_ls = [os.path.join(configs_dir, f\"{i}.json\") for i in dlc_fp_ls]\n    # Reading their configs\n    dlc_fp_ls = [ExperimentConfigs.read_json(i) for i in dlc_fp_ls]\n    # Getting their model_fp\n    dlc_fp_ls = [i.user.run_dlc.model_fp for i in dlc_fp_ls]\n    # Converting to a set\n    dlc_fp_set = set(dlc_fp_ls)\n    # Assertion: all model_fp must be the same\n    assert len(dlc_fp_set) == 1\n    # Getting the model_fp\n    model_fp = dlc_fp_set.pop()\n    # Assertion: the config.yaml file must exist.\n    assert os.path.isfile(model_fp), (\n        f'The given model_fp file does not exist: \"{model_fp}\".\\n'\n        + 'Check this file and specify a DLC \".yaml\" config file.'\n    )\n\n    # Running the DLC subprocess (in a separate conda env)\n    run_dlc_subproc(model_fp, vid_fp_ls, temp_dlc_dir, CACHE_DIR, gputouse, logger)\n\n    # Exporting the h5 to chosen file format\n    for vid_fp in vid_fp_ls:\n        export2df(vid_fp, temp_dlc_dir, keypoints_dir, logger)\n    silent_remove(temp_dlc_dir)\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.run_dlc.RunDLC.ma_dlc_run_single","title":"<code>ma_dlc_run_single(formatted_vid_fp, keypoints_fp, configs_fp, gputouse, overwrite)</code>  <code>classmethod</code>","text":"<p>Running custom DLC script to generate a DLC keypoints dataframe from a single video.</p> Source code in <code>behavysis/processes/run_dlc.py</code> <pre><code>@classmethod\ndef ma_dlc_run_single(\n    cls,\n    formatted_vid_fp: str,\n    keypoints_fp: str,\n    configs_fp: str,\n    gputouse: int | None,\n    overwrite: bool,\n) -&gt; str:\n    \"\"\"\n    Running custom DLC script to generate a DLC keypoints dataframe from a single video.\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    if not overwrite and os.path.exists(keypoints_fp):\n        logger.warning(file_exists_msg(keypoints_fp))\n        return get_io_obj_content(io_obj)\n    # Getting model_fp\n    configs = ExperimentConfigs.read_json(configs_fp)\n    model_fp = configs.get_ref(configs.user.run_dlc.model_fp)\n    # Derive more parameters\n    temp_dlc_dir = os.path.join(CACHE_DIR, f\"dlc_{gputouse}\")\n    keypoints_dir = os.path.dirname(keypoints_fp)\n    # Making output directories\n    os.makedirs(temp_dlc_dir, exist_ok=True)\n\n    # Assertion: the config.yaml file must exist.\n    if not os.path.isfile(model_fp):\n        raise ValueError(\n            f'The given model_fp file does not exist: \"{model_fp}\".\\n'\n            + 'Check this file and specify a DLC \".yaml\" config file.'\n        )\n\n    # Running the DLC subprocess (in a separate conda env)\n    run_dlc_subproc(model_fp, [formatted_vid_fp], temp_dlc_dir, CACHE_DIR, gputouse, logger)\n\n    # Exporting the h5 to chosen file format\n    export2df(formatted_vid_fp, temp_dlc_dir, keypoints_dir, logger)\n    silent_remove(temp_dlc_dir)\n\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.calculate_params.CalculateParams","title":"<code>behavysis.processes.calculate_params.CalculateParams</code>","text":"Source code in <code>behavysis/processes/calculate_params.py</code> <pre><code>class CalculateParams:\n    @staticmethod\n    def start_frame_from_likelihood(\n        keypoints_fp: str,\n        configs_fp: str,\n    ) -&gt; str:\n        \"\"\"\n        Determines the starting frame of the experiment based on\n        when the subject \"likely\" entered the frame of view.\n\n        This is done by looking at a sliding window of time. If the median likelihood of the subject\n        existing in each frame across the sliding window is greater than the defined pcutoff, then\n        the determine this as the start time.\n\n        Notes\n        -----\n        The config file must contain the following parameters:\n        ```\n        - user\n            - calculate_params\n                - start_frame\n                    - bodyparts: list[str]\n                    - window_sec: float\n                    - pcutoff: float\n        ```\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        start_frame, stop_frame = calc_exists_from_likelihood(keypoints_fp, configs_fp, logger)\n        # Writing to configs\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs.auto.start_frame = start_frame\n        configs.write_json(configs_fp)\n        return get_io_obj_content(io_obj)\n\n    @staticmethod\n    def start_frame_from_csv(keypoints_fp: str, configs_fp: str) -&gt; str:\n        \"\"\"\n        Reads the start time of the experiment from a given CSV file\n        (filepath specified in config file).\n\n        Expects value to be in seconds (so will convert to frames).\n        Also expects the csv_fp to be a csv file,\n        where the first column is the name of the video and the second column\n        is the start time.\n\n        Notes\n        -----\n        The config file must contain the following parameters:\n        ```\n        - user\n            - calculate_params\n                - start_frame_from_csv\n                    - csv_fp: str\n                    - name: None | str\n        ```\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        # Getting necessary config parameters\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs_filt = configs.user.calculate_params.start_frame_from_csv\n        fps = configs.auto.formatted_vid.fps\n        csv_fp = configs.get_ref(configs_filt.csv_fp)\n        name = configs.get_ref(configs_filt.name)\n        assert fps != -1, \"fps not yet set. Please calculate fps first with `proj.get_vid_metadata`.\"\n        # Using the name of the video as the name of the experiment if not specified\n        if name is None:\n            name = get_name(keypoints_fp)\n        # Reading csv_fp\n        start_times_df = pd.read_csv(csv_fp, index_col=0)\n        start_times_df.index = start_times_df.index.astype(str)\n        assert name in start_times_df.index.values, (\n            f\"{name} not in {csv_fp}.\\n\"\n            \"Update the `name` parameter in the configs file or check the start_frames csv file.\"\n        )\n        # Getting start time in seconds\n        start_sec = start_times_df.loc[name][0]\n        # Converting to start frame\n        start_frame = int(np.round(start_sec * fps, 0))\n        # Writing to configs\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs.auto.start_frame = start_frame\n        configs.write_json(configs_fp)\n        return get_io_obj_content(io_obj)\n\n    @staticmethod\n    def stop_frame_from_likelihood(keypoints_fp: str, configs_fp: str) -&gt; str:\n        \"\"\"\n        Determines the starting frame of the experiment based on\n        when the subject \"likely\" entered the frame of view.\n\n        This is done by looking at a sliding window of time. If the median likelihood of the subject\n        existing in each frame across the sliding window is greater than the defined pcutoff, then\n        the determine this as the start time.\n\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        start_frame, stop_frame = calc_exists_from_likelihood(keypoints_fp, configs_fp, logger)\n        # Writing to configs\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs.auto.stop_frame = stop_frame\n        configs.write_json(configs_fp)\n        return get_io_obj_content(io_obj)\n\n    @staticmethod\n    def stop_frame_from_dur(keypoints_fp: str, configs_fp: str) -&gt; str:\n        \"\"\"\n        Calculates the end time according to the following equation:\n\n        ```\n        stop_frame = start_frame + experiment_duration\n        ```\n\n        Notes\n        -----\n        The config file must contain the following parameters:\n        ```\n        - user\n            - calculate_params\n                - stop_frame_from_dur\n                    - dur_sec: float\n        ```\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        # Getting necessary config parameters\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs_filt = configs.user.calculate_params.stop_frame_from_dur\n        dur_sec = configs.get_ref(configs_filt.dur_sec)\n        start_frame = configs.auto.start_frame\n        fps = configs.auto.formatted_vid.fps\n        total_frames = configs.auto.formatted_vid.total_frames\n        assert start_frame != -1, \"start_frame is None. Please calculate start_frame first.\"\n        assert fps != -1, \"fps not yet set. Please calculate fps first with `proj.get_vid_metadata`.\"\n        # Calculating stop_frame\n        dur_frames = int(dur_sec * fps)\n        stop_frame = start_frame + dur_frames\n        # Make a warning if the use-specified dur_sec is larger than the duration of the video.\n        if total_frames is None:\n            logger.warning(\"The length of the video itself has not been calculated yet.\")\n        elif stop_frame &gt; total_frames:\n            logger.warning(\n                \"The user specified dur_sec in the configs file is greater \"\n                \"than the actual length of the video. Please check to see if this video is \"\n                \"too short or if the dur_sec value is incorrect.\"\n            )\n        # Writing to config\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs.auto.stop_frame = stop_frame\n        configs.write_json(configs_fp)\n        return get_io_obj_content(io_obj)\n\n    @staticmethod\n    def dur_frames_from_likelihood(keypoints_fp: str, configs_fp: str) -&gt; str:\n        \"\"\"\n        Calculates the duration in seconds, from the time the specified bodyparts appeared\n        to the time they disappeared.\n        Appear/disappear is calculated from likelihood.\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        start_frame, stop_frame = calc_exists_from_likelihood(keypoints_fp, configs_fp, logger)\n        # Writing to configs\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs.auto.dur_frames = stop_frame - start_frame\n        configs.write_json(configs_fp)\n        return get_io_obj_content(io_obj)\n\n    @staticmethod\n    def px_per_mm(keypoints_fp: str, configs_fp: str) -&gt; str:\n        \"\"\"\n        Calculates the pixels per mm conversion for the video.\n\n        This is done by averaging the (x, y) coordinates of each corner,\n        finding the average x difference for the widths in pixels and y distance\n        for the heights in pixels,\n        dividing these pixel distances by their respective mm distances\n        (from the *config.json file),\n        and taking the average of these width and height conversions to estimate\n        the px to mm\n        conversion.\n\n        Notes\n        -----\n        The config file must contain the following parameters:\n        ```\n        - user\n            - calculate_params\n                - px_per_mm\n                    - point_a: str\n                    - point_b: str\n                    - dist_mm: float\n        ```\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        # Getting necessary config parameters\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs_filt = configs.user.calculate_params.px_per_mm\n        pt_a = configs.get_ref(configs_filt.pt_a)\n        pt_b = configs.get_ref(configs_filt.pt_b)\n        pcutoff = configs.get_ref(configs_filt.pcutoff)\n        dist_mm = configs.get_ref(configs_filt.dist_mm)\n        # Loading dataframe\n        keypoints_df = KeypointsDf.clean_headings(KeypointsDf.read(keypoints_fp))\n        # Imputing missing values with 0 (only really relevant for `likelihood` columns)\n        keypoints_df = keypoints_df.fillna(0)\n        # Checking that the two reference points are valid\n        KeypointsDf.check_bpts_exist(keypoints_df, [pt_a, pt_b])\n        # Getting calibration points (x, y, likelihood) values\n        pt_a_df = keypoints_df[IndivCols.SINGLE.value, pt_a]\n        pt_b_df = keypoints_df[IndivCols.SINGLE.value, pt_b]\n        for pt_df, pt in ([pt_a_df, pt_a], [pt_b_df, pt_b]):\n            assert np.any(pt_df[CoordsCols.LIKELIHOOD.value] &gt; pcutoff), (\n                f'No points for \"{pt}\" are above the pcutoff of {pcutoff}.\\n'\n                \"Consider lowering the pcutoff in the configs file.\\n\"\n                f'The highest likelihood value in \"{pt}\" is {np.nanmax(pt_df[CoordsCols.LIKELIHOOD.value])}.'\n            )\n        # Interpolating points which are below a likelihood threshold (linear)\n        pt_a_df.loc[pt_a_df[CoordsCols.LIKELIHOOD.value] &lt; pcutoff] = np.nan\n        pt_a_df = pt_a_df.interpolate(method=\"linear\", axis=0).bfill().ffill()\n        pt_b_df.loc[pt_b_df[CoordsCols.LIKELIHOOD.value] &lt; pcutoff] = np.nan\n        pt_b_df = pt_b_df.interpolate(method=\"linear\", axis=0).bfill().ffill()\n        # Getting distance between calibration points\n        # TODO: use variable names for x and y\n        dist_px = np.nanmean(np.sqrt(np.square(pt_a_df[\"x\"] - pt_b_df[\"x\"]) + np.square(pt_a_df[\"y\"] - pt_b_df[\"y\"])))\n        # Finding pixels per mm conversion, using the given arena width and height as calibration\n        px_per_mm = dist_px / dist_mm\n        # Saving to configs file\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs.auto.px_per_mm = px_per_mm\n        configs.write_json(configs_fp)\n        return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.calculate_params.CalculateParams.dur_frames_from_likelihood","title":"<code>dur_frames_from_likelihood(keypoints_fp, configs_fp)</code>  <code>staticmethod</code>","text":"<p>Calculates the duration in seconds, from the time the specified bodyparts appeared to the time they disappeared. Appear/disappear is calculated from likelihood.</p> Source code in <code>behavysis/processes/calculate_params.py</code> <pre><code>@staticmethod\ndef dur_frames_from_likelihood(keypoints_fp: str, configs_fp: str) -&gt; str:\n    \"\"\"\n    Calculates the duration in seconds, from the time the specified bodyparts appeared\n    to the time they disappeared.\n    Appear/disappear is calculated from likelihood.\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    start_frame, stop_frame = calc_exists_from_likelihood(keypoints_fp, configs_fp, logger)\n    # Writing to configs\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs.auto.dur_frames = stop_frame - start_frame\n    configs.write_json(configs_fp)\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.calculate_params.CalculateParams.px_per_mm","title":"<code>px_per_mm(keypoints_fp, configs_fp)</code>  <code>staticmethod</code>","text":"<p>Calculates the pixels per mm conversion for the video.</p> <p>This is done by averaging the (x, y) coordinates of each corner, finding the average x difference for the widths in pixels and y distance for the heights in pixels, dividing these pixel distances by their respective mm distances (from the *config.json file), and taking the average of these width and height conversions to estimate the px to mm conversion.</p> Notes <p>The config file must contain the following parameters: <pre><code>- user\n    - calculate_params\n        - px_per_mm\n            - point_a: str\n            - point_b: str\n            - dist_mm: float\n</code></pre></p> Source code in <code>behavysis/processes/calculate_params.py</code> <pre><code>@staticmethod\ndef px_per_mm(keypoints_fp: str, configs_fp: str) -&gt; str:\n    \"\"\"\n    Calculates the pixels per mm conversion for the video.\n\n    This is done by averaging the (x, y) coordinates of each corner,\n    finding the average x difference for the widths in pixels and y distance\n    for the heights in pixels,\n    dividing these pixel distances by their respective mm distances\n    (from the *config.json file),\n    and taking the average of these width and height conversions to estimate\n    the px to mm\n    conversion.\n\n    Notes\n    -----\n    The config file must contain the following parameters:\n    ```\n    - user\n        - calculate_params\n            - px_per_mm\n                - point_a: str\n                - point_b: str\n                - dist_mm: float\n    ```\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    # Getting necessary config parameters\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs_filt = configs.user.calculate_params.px_per_mm\n    pt_a = configs.get_ref(configs_filt.pt_a)\n    pt_b = configs.get_ref(configs_filt.pt_b)\n    pcutoff = configs.get_ref(configs_filt.pcutoff)\n    dist_mm = configs.get_ref(configs_filt.dist_mm)\n    # Loading dataframe\n    keypoints_df = KeypointsDf.clean_headings(KeypointsDf.read(keypoints_fp))\n    # Imputing missing values with 0 (only really relevant for `likelihood` columns)\n    keypoints_df = keypoints_df.fillna(0)\n    # Checking that the two reference points are valid\n    KeypointsDf.check_bpts_exist(keypoints_df, [pt_a, pt_b])\n    # Getting calibration points (x, y, likelihood) values\n    pt_a_df = keypoints_df[IndivCols.SINGLE.value, pt_a]\n    pt_b_df = keypoints_df[IndivCols.SINGLE.value, pt_b]\n    for pt_df, pt in ([pt_a_df, pt_a], [pt_b_df, pt_b]):\n        assert np.any(pt_df[CoordsCols.LIKELIHOOD.value] &gt; pcutoff), (\n            f'No points for \"{pt}\" are above the pcutoff of {pcutoff}.\\n'\n            \"Consider lowering the pcutoff in the configs file.\\n\"\n            f'The highest likelihood value in \"{pt}\" is {np.nanmax(pt_df[CoordsCols.LIKELIHOOD.value])}.'\n        )\n    # Interpolating points which are below a likelihood threshold (linear)\n    pt_a_df.loc[pt_a_df[CoordsCols.LIKELIHOOD.value] &lt; pcutoff] = np.nan\n    pt_a_df = pt_a_df.interpolate(method=\"linear\", axis=0).bfill().ffill()\n    pt_b_df.loc[pt_b_df[CoordsCols.LIKELIHOOD.value] &lt; pcutoff] = np.nan\n    pt_b_df = pt_b_df.interpolate(method=\"linear\", axis=0).bfill().ffill()\n    # Getting distance between calibration points\n    # TODO: use variable names for x and y\n    dist_px = np.nanmean(np.sqrt(np.square(pt_a_df[\"x\"] - pt_b_df[\"x\"]) + np.square(pt_a_df[\"y\"] - pt_b_df[\"y\"])))\n    # Finding pixels per mm conversion, using the given arena width and height as calibration\n    px_per_mm = dist_px / dist_mm\n    # Saving to configs file\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs.auto.px_per_mm = px_per_mm\n    configs.write_json(configs_fp)\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.calculate_params.CalculateParams.start_frame_from_csv","title":"<code>start_frame_from_csv(keypoints_fp, configs_fp)</code>  <code>staticmethod</code>","text":"<p>Reads the start time of the experiment from a given CSV file (filepath specified in config file).</p> <p>Expects value to be in seconds (so will convert to frames). Also expects the csv_fp to be a csv file, where the first column is the name of the video and the second column is the start time.</p> Notes <p>The config file must contain the following parameters: <pre><code>- user\n    - calculate_params\n        - start_frame_from_csv\n            - csv_fp: str\n            - name: None | str\n</code></pre></p> Source code in <code>behavysis/processes/calculate_params.py</code> <pre><code>@staticmethod\ndef start_frame_from_csv(keypoints_fp: str, configs_fp: str) -&gt; str:\n    \"\"\"\n    Reads the start time of the experiment from a given CSV file\n    (filepath specified in config file).\n\n    Expects value to be in seconds (so will convert to frames).\n    Also expects the csv_fp to be a csv file,\n    where the first column is the name of the video and the second column\n    is the start time.\n\n    Notes\n    -----\n    The config file must contain the following parameters:\n    ```\n    - user\n        - calculate_params\n            - start_frame_from_csv\n                - csv_fp: str\n                - name: None | str\n    ```\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    # Getting necessary config parameters\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs_filt = configs.user.calculate_params.start_frame_from_csv\n    fps = configs.auto.formatted_vid.fps\n    csv_fp = configs.get_ref(configs_filt.csv_fp)\n    name = configs.get_ref(configs_filt.name)\n    assert fps != -1, \"fps not yet set. Please calculate fps first with `proj.get_vid_metadata`.\"\n    # Using the name of the video as the name of the experiment if not specified\n    if name is None:\n        name = get_name(keypoints_fp)\n    # Reading csv_fp\n    start_times_df = pd.read_csv(csv_fp, index_col=0)\n    start_times_df.index = start_times_df.index.astype(str)\n    assert name in start_times_df.index.values, (\n        f\"{name} not in {csv_fp}.\\n\"\n        \"Update the `name` parameter in the configs file or check the start_frames csv file.\"\n    )\n    # Getting start time in seconds\n    start_sec = start_times_df.loc[name][0]\n    # Converting to start frame\n    start_frame = int(np.round(start_sec * fps, 0))\n    # Writing to configs\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs.auto.start_frame = start_frame\n    configs.write_json(configs_fp)\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.calculate_params.CalculateParams.start_frame_from_likelihood","title":"<code>start_frame_from_likelihood(keypoints_fp, configs_fp)</code>  <code>staticmethod</code>","text":"<p>Determines the starting frame of the experiment based on when the subject \"likely\" entered the frame of view.</p> <p>This is done by looking at a sliding window of time. If the median likelihood of the subject existing in each frame across the sliding window is greater than the defined pcutoff, then the determine this as the start time.</p> Notes <p>The config file must contain the following parameters: <pre><code>- user\n    - calculate_params\n        - start_frame\n            - bodyparts: list[str]\n            - window_sec: float\n            - pcutoff: float\n</code></pre></p> Source code in <code>behavysis/processes/calculate_params.py</code> <pre><code>@staticmethod\ndef start_frame_from_likelihood(\n    keypoints_fp: str,\n    configs_fp: str,\n) -&gt; str:\n    \"\"\"\n    Determines the starting frame of the experiment based on\n    when the subject \"likely\" entered the frame of view.\n\n    This is done by looking at a sliding window of time. If the median likelihood of the subject\n    existing in each frame across the sliding window is greater than the defined pcutoff, then\n    the determine this as the start time.\n\n    Notes\n    -----\n    The config file must contain the following parameters:\n    ```\n    - user\n        - calculate_params\n            - start_frame\n                - bodyparts: list[str]\n                - window_sec: float\n                - pcutoff: float\n    ```\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    start_frame, stop_frame = calc_exists_from_likelihood(keypoints_fp, configs_fp, logger)\n    # Writing to configs\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs.auto.start_frame = start_frame\n    configs.write_json(configs_fp)\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.calculate_params.CalculateParams.stop_frame_from_dur","title":"<code>stop_frame_from_dur(keypoints_fp, configs_fp)</code>  <code>staticmethod</code>","text":"<p>Calculates the end time according to the following equation:</p> <pre><code>stop_frame = start_frame + experiment_duration\n</code></pre> Notes <p>The config file must contain the following parameters: <pre><code>- user\n    - calculate_params\n        - stop_frame_from_dur\n            - dur_sec: float\n</code></pre></p> Source code in <code>behavysis/processes/calculate_params.py</code> <pre><code>@staticmethod\ndef stop_frame_from_dur(keypoints_fp: str, configs_fp: str) -&gt; str:\n    \"\"\"\n    Calculates the end time according to the following equation:\n\n    ```\n    stop_frame = start_frame + experiment_duration\n    ```\n\n    Notes\n    -----\n    The config file must contain the following parameters:\n    ```\n    - user\n        - calculate_params\n            - stop_frame_from_dur\n                - dur_sec: float\n    ```\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    # Getting necessary config parameters\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs_filt = configs.user.calculate_params.stop_frame_from_dur\n    dur_sec = configs.get_ref(configs_filt.dur_sec)\n    start_frame = configs.auto.start_frame\n    fps = configs.auto.formatted_vid.fps\n    total_frames = configs.auto.formatted_vid.total_frames\n    assert start_frame != -1, \"start_frame is None. Please calculate start_frame first.\"\n    assert fps != -1, \"fps not yet set. Please calculate fps first with `proj.get_vid_metadata`.\"\n    # Calculating stop_frame\n    dur_frames = int(dur_sec * fps)\n    stop_frame = start_frame + dur_frames\n    # Make a warning if the use-specified dur_sec is larger than the duration of the video.\n    if total_frames is None:\n        logger.warning(\"The length of the video itself has not been calculated yet.\")\n    elif stop_frame &gt; total_frames:\n        logger.warning(\n            \"The user specified dur_sec in the configs file is greater \"\n            \"than the actual length of the video. Please check to see if this video is \"\n            \"too short or if the dur_sec value is incorrect.\"\n        )\n    # Writing to config\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs.auto.stop_frame = stop_frame\n    configs.write_json(configs_fp)\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.calculate_params.CalculateParams.stop_frame_from_likelihood","title":"<code>stop_frame_from_likelihood(keypoints_fp, configs_fp)</code>  <code>staticmethod</code>","text":"<p>Determines the starting frame of the experiment based on when the subject \"likely\" entered the frame of view.</p> <p>This is done by looking at a sliding window of time. If the median likelihood of the subject existing in each frame across the sliding window is greater than the defined pcutoff, then the determine this as the start time.</p> Source code in <code>behavysis/processes/calculate_params.py</code> <pre><code>@staticmethod\ndef stop_frame_from_likelihood(keypoints_fp: str, configs_fp: str) -&gt; str:\n    \"\"\"\n    Determines the starting frame of the experiment based on\n    when the subject \"likely\" entered the frame of view.\n\n    This is done by looking at a sliding window of time. If the median likelihood of the subject\n    existing in each frame across the sliding window is greater than the defined pcutoff, then\n    the determine this as the start time.\n\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    start_frame, stop_frame = calc_exists_from_likelihood(keypoints_fp, configs_fp, logger)\n    # Writing to configs\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs.auto.stop_frame = stop_frame\n    configs.write_json(configs_fp)\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.preprocess.Preprocess","title":"<code>behavysis.processes.preprocess.Preprocess</code>","text":"<p>summary</p> Source code in <code>behavysis/processes/preprocess.py</code> <pre><code>class Preprocess:\n    \"\"\"_summary_\"\"\"\n\n    @classmethod\n    def start_stop_trim(cls, src_fp: str, dst_fp: str, configs_fp: str, overwrite: bool) -&gt; str:\n        \"\"\"\n        Filters the rows of a DLC formatted dataframe to include only rows within the start\n        and end time of the experiment, given a corresponding configs dict.\n\n        Parameters\n        ----------\n        dlc_fp : str\n            The file path of the input DLC formatted dataframe.\n        dst_fp : str\n            The file path of the output trimmed dataframe.\n        configs_fp : str\n            The file path of the configs dict.\n        overwrite : bool\n            If True, overwrite the output file if it already exists. If False, skip processing\n            if the output file already exists.\n\n        Returns\n        -------\n        str\n            An outcome message indicating the result of the trimming process.\n\n        Notes\n        -----\n        The config file must contain the following parameters:\n        ```\n        - user\n            - preprocess\n                - start_stop_trim\n                    - start_frame: int\n                    - stop_frame: int\n        ```\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        if not overwrite and os.path.exists(dst_fp):\n            logger.warning(file_exists_msg(dst_fp))\n            return get_io_obj_content(io_obj)\n        # Getting necessary config parameters\n        configs = ExperimentConfigs.read_json(configs_fp)\n        start_frame = configs.auto.start_frame\n        stop_frame = configs.auto.stop_frame\n        # Reading file\n        keypoints_df = KeypointsDf.read(src_fp)\n        # Trimming dataframe between start and stop frames\n        keypoints_df = keypoints_df.loc[start_frame:stop_frame, :]\n        KeypointsDf.write(keypoints_df, dst_fp)\n        return get_io_obj_content(io_obj)\n\n    @classmethod\n    def interpolate_stationary(cls, src_fp: str, dst_fp: str, configs_fp: str, overwrite: bool) -&gt; str:\n        \"\"\"\n        If the point detection (above a certain threshold) is below a certain proportion, then the x and y coordinates are set to the given values (usually corners).\n        Otherwise, does nothing (encouraged to run Preprocess.interpolate afterwards).\n\n        Notes\n        -----\n        The config file must contain the following parameters:\n        ```\n        - user\n            - preprocess\n                - interpolate_stationary\n                    [\n                        - bodypart: str (assumed to be the \"single\" individual)\n                        - pcutoff: float (between 0 and 1)\n                        - pcutoff_all: float (between 0 and 1)\n                        - x: float (between 0 and 1 - proportion of the video width)\n                        - y: float (between 0 and 1 - proportion of the video height)\n                    ]\n        ```\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        if not overwrite and os.path.exists(dst_fp):\n            logger.warning(file_exists_msg(dst_fp))\n            return get_io_obj_content(io_obj)\n        # Getting necessary config parameters list\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs_filt_ls = configs.user.preprocess.interpolate_stationary\n        # scorer = configs.auto.scorer_name\n        width_px = configs.auto.formatted_vid.width_px\n        height_px = configs.auto.formatted_vid.height_px\n        if width_px is None or height_px is None:\n            raise ValueError(\n                \"Width and height must be provided in the formatted video. Try running FormatVid.format_vid.\"\n            )\n        # Reading file\n        keypoints_df = KeypointsDf.read(src_fp)\n        # Getting the scorer name\n        scorer = keypoints_df.columns.unique(KeypointsDf.CN.SCORER.value)[0]\n        # For each bodypart, filling in the given point\n        for configs_filt in configs_filt_ls:\n            # Getting config parameters\n            bodypart = configs_filt.bodypart\n            pcutoff = configs_filt.pcutoff\n            pcutoff_all = configs_filt.pcutoff_all\n            x = configs_filt.x\n            y = configs_filt.y\n            # Converting x and y from video proportions to pixel coordinates\n            x = x * width_px\n            y = y * height_px\n            # Getting \"is_detected\" for each frame for the bodypart\n            is_detected = keypoints_df[(scorer, \"single\", bodypart, CoordsCols.LIKELIHOOD.value)] &gt;= pcutoff\n            # If the bodypart is detected in less than the given proportion of the video, then set the x and y coordinates to the given values\n            if is_detected.mean() &lt; pcutoff_all:\n                keypoints_df[(scorer, \"single\", bodypart, CoordsCols.X.value)] = x\n                keypoints_df[(scorer, \"single\", bodypart, CoordsCols.Y.value)] = y\n                keypoints_df[(scorer, \"single\", bodypart, CoordsCols.LIKELIHOOD.value)] = pcutoff\n                logger.info(\n                    f\"{bodypart} is detected in less than {pcutoff_all} of the video.\"\n                    f\" Setting x and y coordinates to ({x}, {y}).\"\n                )\n            else:\n                logger.info(\n                    f\"{bodypart} is detected in more than {pcutoff_all} of the video.\"\n                    \" No need for stationary interpolation.\"\n                )\n        # Saving\n        KeypointsDf.write(keypoints_df, dst_fp)\n        return get_io_obj_content(io_obj)\n\n    @classmethod\n    def interpolate(cls, src_fp: str, dst_fp: str, configs_fp: str, overwrite: bool) -&gt; str:\n        \"\"\"\n        \"Smooths\" out noticeable jitter of points, where the likelihood (and accuracy) of\n        a point's coordinates are low (e.g., when the subject's head goes out of view). It\n        does this by linearly interpolating the frames of a body part that are below a given\n        likelihood pcutoff.\n\n        Notes\n        -----\n        The config file must contain the following parameters:\n        ```\n        - user\n            - preprocess\n                - interpolate\n                    - pcutoff: float\n        ```\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        if not overwrite and os.path.exists(dst_fp):\n            logger.warning(file_exists_msg(dst_fp))\n            return get_io_obj_content(io_obj)\n        # Getting necessary config parameters\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs_filt = configs.user.preprocess.interpolate\n        # Reading file\n        keypoints_df = KeypointsDf.read(src_fp)\n        # Gettings the unique groups of (individual, bodypart) groups.\n        unique_cols = keypoints_df.columns.droplevel([\"coords\"]).unique()\n        # Setting low-likelihood points to Nan to later interpolate\n        for scorer, indiv, bp in unique_cols:\n            # Imputing Nan likelihood points with 0\n            keypoints_df[(scorer, indiv, bp, CoordsCols.LIKELIHOOD.value)].fillna(value=0, inplace=True)\n            # Setting x and y coordinates of points that have low likelihood to Nan\n            to_remove = keypoints_df[(scorer, indiv, bp, CoordsCols.LIKELIHOOD.value)] &lt; configs_filt.pcutoff\n            keypoints_df.loc[to_remove, (scorer, indiv, bp, CoordsCols.X.value)] = np.nan\n            keypoints_df.loc[to_remove, (scorer, indiv, bp, CoordsCols.Y.value)] = np.nan\n        # linearly interpolating Nan x and y points.\n        # Also backfilling points at the start.\n        # Also forward filling points at the end.\n        # Also imputing nan points with 0 (if the ENTIRE column is nan, then it's imputed)\n        keypoints_df = keypoints_df.interpolate(method=\"linear\").bfill().ffill()\n        # if df.isnull().values.any() then the entire column is nan (log warning)\n        # keypoints_df = keypoints_df.fillna(0)\n        KeypointsDf.write(keypoints_df, dst_fp)\n        return get_io_obj_content(io_obj)\n\n    @classmethod\n    def refine_ids(cls, src_fp: str, dst_fp: str, configs_fp: str, overwrite: bool) -&gt; str:\n        \"\"\"\n        Ensures that the identity is correctly tracked for maDLC.\n        Assumes interpolate_points has already been run.\n\n        Notes\n        -----\n        The config file must contain the following parameters:\n        ```\n        - user\n            - preprocess\n                - refine_ids\n                    - marked: str\n                    - unmarked: str\n                    - marking: str\n                    - window_sec: float\n                    - metric: [\"current\", \"rolling\", \"binned\"]\n        ```\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        if not overwrite and os.path.exists(dst_fp):\n            logger.warning(file_exists_msg(dst_fp))\n            return get_io_obj_content(io_obj)\n        # Reading file\n        keypoints_df = KeypointsDf.read(src_fp)\n        # Getting necessary config parameters\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs_filt = configs.user.preprocess.refine_ids\n        marked = configs.get_ref(configs_filt.marked)\n        unmarked = configs.get_ref(configs_filt.unmarked)\n        marking = configs.get_ref(configs_filt.marking)\n        window_sec = configs.get_ref(configs_filt.window_sec)\n        bpts = configs.get_ref(configs_filt.bodyparts)\n        metric = configs.get_ref(configs_filt.metric)\n        fps = configs.auto.formatted_vid.fps\n        # Calculating more parameters\n        window_frames = int(np.round(fps * window_sec, 0))\n        # Error checking for invalid/non-existent column names marked, unmarked, and marking\n        for column, level in [\n            (marked, \"individuals\"),\n            (unmarked, \"individuals\"),\n            (marking, \"bodyparts\"),\n        ]:\n            if column not in keypoints_df.columns.unique(level):\n                raise ValueError(\n                    f'The marking value in the config file, \"{column}\", is not a column name in the DLC file.'\n                )\n        # Checking that bodyparts are all valid\n        KeypointsDf.check_bpts_exist(keypoints_df, bpts)\n        # Calculating the distances between the averaged bodycentres and the marking\n        mark_dists_df = get_mark_dists_df(keypoints_df, marked, unmarked, [marking], bpts, logger)\n        # Getting \"to_switch\" decision series for each frame\n        switch_df = get_id_switch_df(mark_dists_df, window_frames, marked, unmarked, logger)\n        # Updating df with the switched values\n        switched_keypoints_df = switch_identities(keypoints_df, switch_df[metric], marked, unmarked, logger)\n        KeypointsDf.write(switched_keypoints_df, dst_fp)\n        return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.preprocess.Preprocess.interpolate","title":"<code>interpolate(src_fp, dst_fp, configs_fp, overwrite)</code>  <code>classmethod</code>","text":"<p>\"Smooths\" out noticeable jitter of points, where the likelihood (and accuracy) of a point's coordinates are low (e.g., when the subject's head goes out of view). It does this by linearly interpolating the frames of a body part that are below a given likelihood pcutoff.</p> Notes <p>The config file must contain the following parameters: <pre><code>- user\n    - preprocess\n        - interpolate\n            - pcutoff: float\n</code></pre></p> Source code in <code>behavysis/processes/preprocess.py</code> <pre><code>@classmethod\ndef interpolate(cls, src_fp: str, dst_fp: str, configs_fp: str, overwrite: bool) -&gt; str:\n    \"\"\"\n    \"Smooths\" out noticeable jitter of points, where the likelihood (and accuracy) of\n    a point's coordinates are low (e.g., when the subject's head goes out of view). It\n    does this by linearly interpolating the frames of a body part that are below a given\n    likelihood pcutoff.\n\n    Notes\n    -----\n    The config file must contain the following parameters:\n    ```\n    - user\n        - preprocess\n            - interpolate\n                - pcutoff: float\n    ```\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    if not overwrite and os.path.exists(dst_fp):\n        logger.warning(file_exists_msg(dst_fp))\n        return get_io_obj_content(io_obj)\n    # Getting necessary config parameters\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs_filt = configs.user.preprocess.interpolate\n    # Reading file\n    keypoints_df = KeypointsDf.read(src_fp)\n    # Gettings the unique groups of (individual, bodypart) groups.\n    unique_cols = keypoints_df.columns.droplevel([\"coords\"]).unique()\n    # Setting low-likelihood points to Nan to later interpolate\n    for scorer, indiv, bp in unique_cols:\n        # Imputing Nan likelihood points with 0\n        keypoints_df[(scorer, indiv, bp, CoordsCols.LIKELIHOOD.value)].fillna(value=0, inplace=True)\n        # Setting x and y coordinates of points that have low likelihood to Nan\n        to_remove = keypoints_df[(scorer, indiv, bp, CoordsCols.LIKELIHOOD.value)] &lt; configs_filt.pcutoff\n        keypoints_df.loc[to_remove, (scorer, indiv, bp, CoordsCols.X.value)] = np.nan\n        keypoints_df.loc[to_remove, (scorer, indiv, bp, CoordsCols.Y.value)] = np.nan\n    # linearly interpolating Nan x and y points.\n    # Also backfilling points at the start.\n    # Also forward filling points at the end.\n    # Also imputing nan points with 0 (if the ENTIRE column is nan, then it's imputed)\n    keypoints_df = keypoints_df.interpolate(method=\"linear\").bfill().ffill()\n    # if df.isnull().values.any() then the entire column is nan (log warning)\n    # keypoints_df = keypoints_df.fillna(0)\n    KeypointsDf.write(keypoints_df, dst_fp)\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.preprocess.Preprocess.interpolate_stationary","title":"<code>interpolate_stationary(src_fp, dst_fp, configs_fp, overwrite)</code>  <code>classmethod</code>","text":"<p>If the point detection (above a certain threshold) is below a certain proportion, then the x and y coordinates are set to the given values (usually corners). Otherwise, does nothing (encouraged to run Preprocess.interpolate afterwards).</p> Notes <p>The config file must contain the following parameters: <pre><code>- user\n    - preprocess\n        - interpolate_stationary\n            [\n                - bodypart: str (assumed to be the \"single\" individual)\n                - pcutoff: float (between 0 and 1)\n                - pcutoff_all: float (between 0 and 1)\n                - x: float (between 0 and 1 - proportion of the video width)\n                - y: float (between 0 and 1 - proportion of the video height)\n            ]\n</code></pre></p> Source code in <code>behavysis/processes/preprocess.py</code> <pre><code>@classmethod\ndef interpolate_stationary(cls, src_fp: str, dst_fp: str, configs_fp: str, overwrite: bool) -&gt; str:\n    \"\"\"\n    If the point detection (above a certain threshold) is below a certain proportion, then the x and y coordinates are set to the given values (usually corners).\n    Otherwise, does nothing (encouraged to run Preprocess.interpolate afterwards).\n\n    Notes\n    -----\n    The config file must contain the following parameters:\n    ```\n    - user\n        - preprocess\n            - interpolate_stationary\n                [\n                    - bodypart: str (assumed to be the \"single\" individual)\n                    - pcutoff: float (between 0 and 1)\n                    - pcutoff_all: float (between 0 and 1)\n                    - x: float (between 0 and 1 - proportion of the video width)\n                    - y: float (between 0 and 1 - proportion of the video height)\n                ]\n    ```\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    if not overwrite and os.path.exists(dst_fp):\n        logger.warning(file_exists_msg(dst_fp))\n        return get_io_obj_content(io_obj)\n    # Getting necessary config parameters list\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs_filt_ls = configs.user.preprocess.interpolate_stationary\n    # scorer = configs.auto.scorer_name\n    width_px = configs.auto.formatted_vid.width_px\n    height_px = configs.auto.formatted_vid.height_px\n    if width_px is None or height_px is None:\n        raise ValueError(\n            \"Width and height must be provided in the formatted video. Try running FormatVid.format_vid.\"\n        )\n    # Reading file\n    keypoints_df = KeypointsDf.read(src_fp)\n    # Getting the scorer name\n    scorer = keypoints_df.columns.unique(KeypointsDf.CN.SCORER.value)[0]\n    # For each bodypart, filling in the given point\n    for configs_filt in configs_filt_ls:\n        # Getting config parameters\n        bodypart = configs_filt.bodypart\n        pcutoff = configs_filt.pcutoff\n        pcutoff_all = configs_filt.pcutoff_all\n        x = configs_filt.x\n        y = configs_filt.y\n        # Converting x and y from video proportions to pixel coordinates\n        x = x * width_px\n        y = y * height_px\n        # Getting \"is_detected\" for each frame for the bodypart\n        is_detected = keypoints_df[(scorer, \"single\", bodypart, CoordsCols.LIKELIHOOD.value)] &gt;= pcutoff\n        # If the bodypart is detected in less than the given proportion of the video, then set the x and y coordinates to the given values\n        if is_detected.mean() &lt; pcutoff_all:\n            keypoints_df[(scorer, \"single\", bodypart, CoordsCols.X.value)] = x\n            keypoints_df[(scorer, \"single\", bodypart, CoordsCols.Y.value)] = y\n            keypoints_df[(scorer, \"single\", bodypart, CoordsCols.LIKELIHOOD.value)] = pcutoff\n            logger.info(\n                f\"{bodypart} is detected in less than {pcutoff_all} of the video.\"\n                f\" Setting x and y coordinates to ({x}, {y}).\"\n            )\n        else:\n            logger.info(\n                f\"{bodypart} is detected in more than {pcutoff_all} of the video.\"\n                \" No need for stationary interpolation.\"\n            )\n    # Saving\n    KeypointsDf.write(keypoints_df, dst_fp)\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.preprocess.Preprocess.refine_ids","title":"<code>refine_ids(src_fp, dst_fp, configs_fp, overwrite)</code>  <code>classmethod</code>","text":"<p>Ensures that the identity is correctly tracked for maDLC. Assumes interpolate_points has already been run.</p> Notes <p>The config file must contain the following parameters: <pre><code>- user\n    - preprocess\n        - refine_ids\n            - marked: str\n            - unmarked: str\n            - marking: str\n            - window_sec: float\n            - metric: [\"current\", \"rolling\", \"binned\"]\n</code></pre></p> Source code in <code>behavysis/processes/preprocess.py</code> <pre><code>@classmethod\ndef refine_ids(cls, src_fp: str, dst_fp: str, configs_fp: str, overwrite: bool) -&gt; str:\n    \"\"\"\n    Ensures that the identity is correctly tracked for maDLC.\n    Assumes interpolate_points has already been run.\n\n    Notes\n    -----\n    The config file must contain the following parameters:\n    ```\n    - user\n        - preprocess\n            - refine_ids\n                - marked: str\n                - unmarked: str\n                - marking: str\n                - window_sec: float\n                - metric: [\"current\", \"rolling\", \"binned\"]\n    ```\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    if not overwrite and os.path.exists(dst_fp):\n        logger.warning(file_exists_msg(dst_fp))\n        return get_io_obj_content(io_obj)\n    # Reading file\n    keypoints_df = KeypointsDf.read(src_fp)\n    # Getting necessary config parameters\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs_filt = configs.user.preprocess.refine_ids\n    marked = configs.get_ref(configs_filt.marked)\n    unmarked = configs.get_ref(configs_filt.unmarked)\n    marking = configs.get_ref(configs_filt.marking)\n    window_sec = configs.get_ref(configs_filt.window_sec)\n    bpts = configs.get_ref(configs_filt.bodyparts)\n    metric = configs.get_ref(configs_filt.metric)\n    fps = configs.auto.formatted_vid.fps\n    # Calculating more parameters\n    window_frames = int(np.round(fps * window_sec, 0))\n    # Error checking for invalid/non-existent column names marked, unmarked, and marking\n    for column, level in [\n        (marked, \"individuals\"),\n        (unmarked, \"individuals\"),\n        (marking, \"bodyparts\"),\n    ]:\n        if column not in keypoints_df.columns.unique(level):\n            raise ValueError(\n                f'The marking value in the config file, \"{column}\", is not a column name in the DLC file.'\n            )\n    # Checking that bodyparts are all valid\n    KeypointsDf.check_bpts_exist(keypoints_df, bpts)\n    # Calculating the distances between the averaged bodycentres and the marking\n    mark_dists_df = get_mark_dists_df(keypoints_df, marked, unmarked, [marking], bpts, logger)\n    # Getting \"to_switch\" decision series for each frame\n    switch_df = get_id_switch_df(mark_dists_df, window_frames, marked, unmarked, logger)\n    # Updating df with the switched values\n    switched_keypoints_df = switch_identities(keypoints_df, switch_df[metric], marked, unmarked, logger)\n    KeypointsDf.write(switched_keypoints_df, dst_fp)\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.preprocess.Preprocess.start_stop_trim","title":"<code>start_stop_trim(src_fp, dst_fp, configs_fp, overwrite)</code>  <code>classmethod</code>","text":"<p>Filters the rows of a DLC formatted dataframe to include only rows within the start and end time of the experiment, given a corresponding configs dict.</p> <p>Parameters:</p> Name Type Description Default <code>dlc_fp</code> <code>str</code> <p>The file path of the input DLC formatted dataframe.</p> required <code>dst_fp</code> <code>str</code> <p>The file path of the output trimmed dataframe.</p> required <code>configs_fp</code> <code>str</code> <p>The file path of the configs dict.</p> required <code>overwrite</code> <code>bool</code> <p>If True, overwrite the output file if it already exists. If False, skip processing if the output file already exists.</p> required <p>Returns:</p> Type Description <code>str</code> <p>An outcome message indicating the result of the trimming process.</p> Notes <p>The config file must contain the following parameters: <pre><code>- user\n    - preprocess\n        - start_stop_trim\n            - start_frame: int\n            - stop_frame: int\n</code></pre></p> Source code in <code>behavysis/processes/preprocess.py</code> <pre><code>@classmethod\ndef start_stop_trim(cls, src_fp: str, dst_fp: str, configs_fp: str, overwrite: bool) -&gt; str:\n    \"\"\"\n    Filters the rows of a DLC formatted dataframe to include only rows within the start\n    and end time of the experiment, given a corresponding configs dict.\n\n    Parameters\n    ----------\n    dlc_fp : str\n        The file path of the input DLC formatted dataframe.\n    dst_fp : str\n        The file path of the output trimmed dataframe.\n    configs_fp : str\n        The file path of the configs dict.\n    overwrite : bool\n        If True, overwrite the output file if it already exists. If False, skip processing\n        if the output file already exists.\n\n    Returns\n    -------\n    str\n        An outcome message indicating the result of the trimming process.\n\n    Notes\n    -----\n    The config file must contain the following parameters:\n    ```\n    - user\n        - preprocess\n            - start_stop_trim\n                - start_frame: int\n                - stop_frame: int\n    ```\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    if not overwrite and os.path.exists(dst_fp):\n        logger.warning(file_exists_msg(dst_fp))\n        return get_io_obj_content(io_obj)\n    # Getting necessary config parameters\n    configs = ExperimentConfigs.read_json(configs_fp)\n    start_frame = configs.auto.start_frame\n    stop_frame = configs.auto.stop_frame\n    # Reading file\n    keypoints_df = KeypointsDf.read(src_fp)\n    # Trimming dataframe between start and stop frames\n    keypoints_df = keypoints_df.loc[start_frame:stop_frame, :]\n    KeypointsDf.write(keypoints_df, dst_fp)\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.extract_features.ExtractFeatures","title":"<code>behavysis.processes.extract_features.ExtractFeatures</code>","text":"Source code in <code>behavysis/processes/extract_features.py</code> <pre><code>class ExtractFeatures:\n    @staticmethod\n    def extract_features(\n        keypoints_fp: str,\n        features_fp: str,\n        configs_fp: str,\n        overwrite: bool,\n    ) -&gt; str:\n        \"\"\"\n        Extracting features from preprocessed keypoints dataframe using SimBA\n        processes.\n\n        Parameters\n        ----------\n        keypoints_fp : str\n            Preprocessed keypoints filepath.\n        dst_fp : str\n            Filepath to save extracted_features dataframe.\n        configs_fp : str\n            Configs JSON filepath.\n        overwrite : bool\n            Whether to overwrite the dst_fp file (if it exists).\n\n        Returns\n        -------\n        str\n            The outcome of the process.\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        if not overwrite and os.path.exists(features_fp):\n            logger.warning(file_exists_msg(features_fp))\n            return get_io_obj_content(io_obj)\n        # Getting directory and file paths\n        name = get_name(keypoints_fp)\n        cpid = get_cpid()\n        configs_dir = os.path.dirname(configs_fp)\n        simba_in_dir = os.path.join(CACHE_DIR, f\"input_{cpid}\")\n        simba_dir = os.path.join(CACHE_DIR, f\"simba_proj_{cpid}\")\n        simba_features_dir = os.path.join(simba_dir, \"project_folder\", \"csv\", \"features_extracted\")\n        simba_features_fp = os.path.join(simba_features_dir, f\"{name}.csv\")\n        # Removing temp folders (preemptively)\n        silent_remove(simba_in_dir)\n        silent_remove(simba_dir)\n        # Preparing keypoints dataframes for input to SimBA project\n        os.makedirs(simba_in_dir, exist_ok=True)\n        simba_in_fp = os.path.join(simba_in_dir, f\"{name}.csv\")\n        # Selecting bodyparts for SimBA (8 bpts, 2 indivs)\n        keypoints_df = KeypointsDf.read(keypoints_fp)\n        keypoints_df = select_cols(keypoints_df, configs_fp, logger)\n        # Saving keypoints index to use in the SimBA features extraction df\n        index = keypoints_df.index\n        # Need to remove index name for SimBA to import correctly\n        keypoints_df.index.name = None\n        # Saving as csv\n        keypoints_df.to_csv(simba_in_fp)\n        # Running SimBA env and script to run SimBA feature extraction\n        run_simba_subproc(simba_dir, simba_in_dir, configs_dir, CACHE_DIR, cpid, logger)\n        # Exporting SimBA feature extraction csv to disk\n        export2df(simba_features_fp, features_fp, index, logger)\n        # Removing temp folders\n        silent_remove(simba_in_dir)\n        silent_remove(simba_dir)\n        return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.extract_features.ExtractFeatures.extract_features","title":"<code>extract_features(keypoints_fp, features_fp, configs_fp, overwrite)</code>  <code>staticmethod</code>","text":"<p>Extracting features from preprocessed keypoints dataframe using SimBA processes.</p> <p>Parameters:</p> Name Type Description Default <code>keypoints_fp</code> <code>str</code> <p>Preprocessed keypoints filepath.</p> required <code>dst_fp</code> <code>str</code> <p>Filepath to save extracted_features dataframe.</p> required <code>configs_fp</code> <code>str</code> <p>Configs JSON filepath.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the dst_fp file (if it exists).</p> required <p>Returns:</p> Type Description <code>str</code> <p>The outcome of the process.</p> Source code in <code>behavysis/processes/extract_features.py</code> <pre><code>@staticmethod\ndef extract_features(\n    keypoints_fp: str,\n    features_fp: str,\n    configs_fp: str,\n    overwrite: bool,\n) -&gt; str:\n    \"\"\"\n    Extracting features from preprocessed keypoints dataframe using SimBA\n    processes.\n\n    Parameters\n    ----------\n    keypoints_fp : str\n        Preprocessed keypoints filepath.\n    dst_fp : str\n        Filepath to save extracted_features dataframe.\n    configs_fp : str\n        Configs JSON filepath.\n    overwrite : bool\n        Whether to overwrite the dst_fp file (if it exists).\n\n    Returns\n    -------\n    str\n        The outcome of the process.\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    if not overwrite and os.path.exists(features_fp):\n        logger.warning(file_exists_msg(features_fp))\n        return get_io_obj_content(io_obj)\n    # Getting directory and file paths\n    name = get_name(keypoints_fp)\n    cpid = get_cpid()\n    configs_dir = os.path.dirname(configs_fp)\n    simba_in_dir = os.path.join(CACHE_DIR, f\"input_{cpid}\")\n    simba_dir = os.path.join(CACHE_DIR, f\"simba_proj_{cpid}\")\n    simba_features_dir = os.path.join(simba_dir, \"project_folder\", \"csv\", \"features_extracted\")\n    simba_features_fp = os.path.join(simba_features_dir, f\"{name}.csv\")\n    # Removing temp folders (preemptively)\n    silent_remove(simba_in_dir)\n    silent_remove(simba_dir)\n    # Preparing keypoints dataframes for input to SimBA project\n    os.makedirs(simba_in_dir, exist_ok=True)\n    simba_in_fp = os.path.join(simba_in_dir, f\"{name}.csv\")\n    # Selecting bodyparts for SimBA (8 bpts, 2 indivs)\n    keypoints_df = KeypointsDf.read(keypoints_fp)\n    keypoints_df = select_cols(keypoints_df, configs_fp, logger)\n    # Saving keypoints index to use in the SimBA features extraction df\n    index = keypoints_df.index\n    # Need to remove index name for SimBA to import correctly\n    keypoints_df.index.name = None\n    # Saving as csv\n    keypoints_df.to_csv(simba_in_fp)\n    # Running SimBA env and script to run SimBA feature extraction\n    run_simba_subproc(simba_dir, simba_in_dir, configs_dir, CACHE_DIR, cpid, logger)\n    # Exporting SimBA feature extraction csv to disk\n    export2df(simba_features_fp, features_fp, index, logger)\n    # Removing temp folders\n    silent_remove(simba_in_dir)\n    silent_remove(simba_dir)\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.classify_behavs.ClassifyBehavs","title":"<code>behavysis.processes.classify_behavs.ClassifyBehavs</code>","text":"Source code in <code>behavysis/processes/classify_behavs.py</code> <pre><code>class ClassifyBehavs:\n    @classmethod\n    def classify_behavs(\n        cls,\n        features_fp: str,\n        behavs_fp: str,\n        configs_fp: str,\n        overwrite: bool,\n    ) -&gt; str:\n        \"\"\"\n        Given model config files in the BehavClassifier format, generates beahviour predidctions\n        on the given extracted features dataframe.\n\n        Parameters\n        ----------\n        features_fp : str\n            _description_\n        dst_fp : str\n            _description_\n        configs_fp : str\n            _description_\n        overwrite : bool\n            Whether to overwrite the output file (if it exists).\n\n        Returns\n        -------\n        str\n            Description of the function's outcome.\n\n        Notes\n        -----\n        The config file must contain the following parameters:\n        ```\n        - user\n            - classify_behaviours\n                - models: list[str]\n        ```\n        Where the `models` list is a list of `model_config.json` filepaths.\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        if not overwrite and os.path.exists(behavs_fp):\n            logger.warning(file_exists_msg(behavs_fp))\n            return get_io_obj_content(io_obj)\n        # Getting necessary config parameters\n        configs = ExperimentConfigs.read_json(configs_fp)\n        fps = configs.auto.formatted_vid.fps\n        model_configs_ls = configs.user.classify_behavs\n        # Getting features data\n        features_df = FeaturesDf.read(features_fp)\n        # Initialising y_preds df\n        # Getting predictions for each classifier model and saving\n        # in a list of pd.DataFrames\n        behavs_df_ls = []\n        for model_config in model_configs_ls:\n            proj_dir = configs.get_ref(model_config.proj_dir)\n            behav_name = configs.get_ref(model_config.behav_name)\n            behav_model = BehavClassifier.load(proj_dir, behav_name)\n            pcutoff = get_pcutoff(configs.get_ref(model_config.pcutoff), behav_model.configs.pcutoff, logger)\n            min_window_secs = configs.get_ref(model_config.min_empty_window_secs)\n            min_window_frames = int(np.round(min_window_secs * fps))\n            # Running the clf pipeline\n            behav_df_i = behav_model.pipeline_inference(features_df)\n            # Getting prob and pred column names\n            prob_col = (behav_name, OutcomesPredictedCols.PROB.value)\n            pred_col = (behav_name, OutcomesPredictedCols.PRED.value)\n            # Using pcutoff to get binary predictions\n            behav_df_i[pred_col] = (behav_df_i[prob_col] &gt; pcutoff).astype(int)\n            # Filling in small non-behav bouts\n            behav_df_i[pred_col] = merge_bouts(behav_df_i[pred_col], min_window_frames, logger)\n            # Adding model predictions df to list\n            behavs_df_ls.append(behav_df_i)\n            # Logging outcome\n            logger.info(f\"Completed {behav_name} classification.\")\n        # If no models were run, then return outcome\n        if len(behavs_df_ls) == 0:\n            return get_io_obj_content(io_obj)\n        # Concatenating predictions to a single dataframe\n        behavs_df = pd.concat(behavs_df_ls, axis=1)\n        # Saving behav_preds df\n        BehavPredictedDf.write(behavs_df, behavs_fp)\n        return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.classify_behavs.ClassifyBehavs.classify_behavs","title":"<code>classify_behavs(features_fp, behavs_fp, configs_fp, overwrite)</code>  <code>classmethod</code>","text":"<p>Given model config files in the BehavClassifier format, generates beahviour predidctions on the given extracted features dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>features_fp</code> <code>str</code> <p>description</p> required <code>dst_fp</code> <code>str</code> <p>description</p> required <code>configs_fp</code> <code>str</code> <p>description</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file (if it exists).</p> required <p>Returns:</p> Type Description <code>str</code> <p>Description of the function's outcome.</p> Notes <p>The config file must contain the following parameters: <pre><code>- user\n    - classify_behaviours\n        - models: list[str]\n</code></pre> Where the <code>models</code> list is a list of <code>model_config.json</code> filepaths.</p> Source code in <code>behavysis/processes/classify_behavs.py</code> <pre><code>@classmethod\ndef classify_behavs(\n    cls,\n    features_fp: str,\n    behavs_fp: str,\n    configs_fp: str,\n    overwrite: bool,\n) -&gt; str:\n    \"\"\"\n    Given model config files in the BehavClassifier format, generates beahviour predidctions\n    on the given extracted features dataframe.\n\n    Parameters\n    ----------\n    features_fp : str\n        _description_\n    dst_fp : str\n        _description_\n    configs_fp : str\n        _description_\n    overwrite : bool\n        Whether to overwrite the output file (if it exists).\n\n    Returns\n    -------\n    str\n        Description of the function's outcome.\n\n    Notes\n    -----\n    The config file must contain the following parameters:\n    ```\n    - user\n        - classify_behaviours\n            - models: list[str]\n    ```\n    Where the `models` list is a list of `model_config.json` filepaths.\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    if not overwrite and os.path.exists(behavs_fp):\n        logger.warning(file_exists_msg(behavs_fp))\n        return get_io_obj_content(io_obj)\n    # Getting necessary config parameters\n    configs = ExperimentConfigs.read_json(configs_fp)\n    fps = configs.auto.formatted_vid.fps\n    model_configs_ls = configs.user.classify_behavs\n    # Getting features data\n    features_df = FeaturesDf.read(features_fp)\n    # Initialising y_preds df\n    # Getting predictions for each classifier model and saving\n    # in a list of pd.DataFrames\n    behavs_df_ls = []\n    for model_config in model_configs_ls:\n        proj_dir = configs.get_ref(model_config.proj_dir)\n        behav_name = configs.get_ref(model_config.behav_name)\n        behav_model = BehavClassifier.load(proj_dir, behav_name)\n        pcutoff = get_pcutoff(configs.get_ref(model_config.pcutoff), behav_model.configs.pcutoff, logger)\n        min_window_secs = configs.get_ref(model_config.min_empty_window_secs)\n        min_window_frames = int(np.round(min_window_secs * fps))\n        # Running the clf pipeline\n        behav_df_i = behav_model.pipeline_inference(features_df)\n        # Getting prob and pred column names\n        prob_col = (behav_name, OutcomesPredictedCols.PROB.value)\n        pred_col = (behav_name, OutcomesPredictedCols.PRED.value)\n        # Using pcutoff to get binary predictions\n        behav_df_i[pred_col] = (behav_df_i[prob_col] &gt; pcutoff).astype(int)\n        # Filling in small non-behav bouts\n        behav_df_i[pred_col] = merge_bouts(behav_df_i[pred_col], min_window_frames, logger)\n        # Adding model predictions df to list\n        behavs_df_ls.append(behav_df_i)\n        # Logging outcome\n        logger.info(f\"Completed {behav_name} classification.\")\n    # If no models were run, then return outcome\n    if len(behavs_df_ls) == 0:\n        return get_io_obj_content(io_obj)\n    # Concatenating predictions to a single dataframe\n    behavs_df = pd.concat(behavs_df_ls, axis=1)\n    # Saving behav_preds df\n    BehavPredictedDf.write(behavs_df, behavs_fp)\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.analyse_behavs.AnalyseBehavs","title":"<code>behavysis.processes.analyse_behavs.AnalyseBehavs</code>","text":"Source code in <code>behavysis/processes/analyse_behavs.py</code> <pre><code>class AnalyseBehavs:\n    @staticmethod\n    def analyse_behavs(\n        behavs_fp: str,\n        dst_dir: str,\n        configs_fp: str,\n    ) -&gt; str:\n        \"\"\"\n        Takes a behavs dataframe and generates a summary and binned version of the data.\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        f_name = get_func_name_in_stack()\n        name = get_name(behavs_fp)\n        dst_subdir = os.path.join(dst_dir, f_name)\n        # Calculating the deltas (changes in body position) between each frame for the subject\n        configs = ExperimentConfigs.read_json(configs_fp)\n        fps, _, _, _, bins_ls, cbins_ls = configs.get_analysis_configs()\n        # Loading in dataframe\n        behavs_df = BehavScoredDf.read(behavs_fp)\n        # Setting all na and undetermined behav to non-behav\n        behavs_df = behavs_df.fillna(0).replace(BehavValues.UNDETERMINED.value, BehavValues.NON_BEHAV.value)\n        # Getting the behaviour names and each user_defined for the behaviour\n        # Not incl. the `pred` or `prob` (`prob` shouldn't be here anyway) columns\n        columns = np.isin(\n            behavs_df.columns.get_level_values(BehavScoredDf.CN.OUTCOMES.value),\n            [BehavScoredDf.OutcomesCols.PRED.value],\n            invert=True,\n        )\n        behavs_df = behavs_df.loc[:, columns]\n        behavs_df = AnalysisDf.basic_clean(behavs_df)\n        # Writing the behavs_df to the fbf file\n        fbf_fp = os.path.join(dst_subdir, FBF, f\"{name}.{AnalysisDf.IO}\")\n        AnalysisDf.write(behavs_df, fbf_fp)\n        # Making the summary and binned dataframes\n        AnalysisBinnedDf.summary_binned_behavs(\n            behavs_df,\n            dst_subdir,\n            name,\n            fps,\n            bins_ls,\n            cbins_ls,\n        )\n        return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.analyse_behavs.AnalyseBehavs.analyse_behavs","title":"<code>analyse_behavs(behavs_fp, dst_dir, configs_fp)</code>  <code>staticmethod</code>","text":"<p>Takes a behavs dataframe and generates a summary and binned version of the data.</p> Source code in <code>behavysis/processes/analyse_behavs.py</code> <pre><code>@staticmethod\ndef analyse_behavs(\n    behavs_fp: str,\n    dst_dir: str,\n    configs_fp: str,\n) -&gt; str:\n    \"\"\"\n    Takes a behavs dataframe and generates a summary and binned version of the data.\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    f_name = get_func_name_in_stack()\n    name = get_name(behavs_fp)\n    dst_subdir = os.path.join(dst_dir, f_name)\n    # Calculating the deltas (changes in body position) between each frame for the subject\n    configs = ExperimentConfigs.read_json(configs_fp)\n    fps, _, _, _, bins_ls, cbins_ls = configs.get_analysis_configs()\n    # Loading in dataframe\n    behavs_df = BehavScoredDf.read(behavs_fp)\n    # Setting all na and undetermined behav to non-behav\n    behavs_df = behavs_df.fillna(0).replace(BehavValues.UNDETERMINED.value, BehavValues.NON_BEHAV.value)\n    # Getting the behaviour names and each user_defined for the behaviour\n    # Not incl. the `pred` or `prob` (`prob` shouldn't be here anyway) columns\n    columns = np.isin(\n        behavs_df.columns.get_level_values(BehavScoredDf.CN.OUTCOMES.value),\n        [BehavScoredDf.OutcomesCols.PRED.value],\n        invert=True,\n    )\n    behavs_df = behavs_df.loc[:, columns]\n    behavs_df = AnalysisDf.basic_clean(behavs_df)\n    # Writing the behavs_df to the fbf file\n    fbf_fp = os.path.join(dst_subdir, FBF, f\"{name}.{AnalysisDf.IO}\")\n    AnalysisDf.write(behavs_df, fbf_fp)\n    # Making the summary and binned dataframes\n    AnalysisBinnedDf.summary_binned_behavs(\n        behavs_df,\n        dst_subdir,\n        name,\n        fps,\n        bins_ls,\n        cbins_ls,\n    )\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.analyse.Analyse","title":"<code>behavysis.processes.analyse.Analyse</code>","text":"Source code in <code>behavysis/processes/analyse.py</code> <pre><code>class Analyse:\n    @staticmethod\n    def in_roi(\n        keypoints_fp: str,\n        dst_dir: str,\n        configs_fp: str,\n    ) -&gt; str:\n        \"\"\"\n        Determines the frames in which the subject is inside the cage (from average\n        of given bodypoints).\n\n        Points are `padding_px` padded (away) from center.\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        f_name = get_func_name_in_stack()\n        name = get_name(keypoints_fp)\n        dst_subdir = os.path.join(dst_dir, f_name)\n        # Calculating the deltas (changes in body position) between each frame for the subject\n        configs = ExperimentConfigs.read_json(configs_fp)\n        fps, _, _, px_per_mm, bins_ls, cbins_ls = configs.get_analysis_configs()\n        configs_filt_ls = configs.user.analyse.in_roi\n        # Loading in dataframe\n        keypoints_df = KeypointsDf.clean_headings(KeypointsDf.read(keypoints_fp))\n        assert keypoints_df.shape[0] &gt; 0, \"No frames in keypoints_df. Please check keypoints file.\"\n        # Getting indivs list\n        indivs, _ = KeypointsDf.get_indivs_bpts(keypoints_df)\n        # Making analysis_df\n        analysis_df_ls = []\n        scatter_df_ls = []\n        corners_df_ls = []\n        roi_names_ls = []\n        # For each roi, calculate the in-roi status of the subject\n        x = CoordsCols.X.value\n        y = CoordsCols.Y.value\n        idx = pd.IndexSlice\n        for configs_filt in configs_filt_ls:\n            # Getting necessary config parameters\n            roi_name = configs.get_ref(configs_filt.roi_name)\n            is_in = configs.get_ref(configs_filt.is_in)\n            bpts = configs.get_ref(configs_filt.bodyparts)\n            padding_mm = configs.get_ref(configs_filt.padding_mm)\n            roi_corners = configs.get_ref(configs_filt.roi_corners)\n            # Calculating more parameters\n            padding_px = padding_mm / px_per_mm\n            # Checking bodyparts and roi_corners exist\n            KeypointsDf.check_bpts_exist(keypoints_df, bpts)\n            KeypointsDf.check_bpts_exist(keypoints_df, roi_corners)\n            # Getting average corner coordinates. Assumes arena does not move.\n            corners_i_df = pd.DataFrame([keypoints_df[(IndivCols.SINGLE.value, pt)].mean() for pt in roi_corners]).drop(\n                columns=[\"likelihood\"]\n            )\n            # Adjusting x-y to have `padding_px` dilation/erosion from the points themselves\n            roi_center = corners_i_df.mean()\n            for i in corners_i_df.index:\n                # Calculating angle from centre to point (going out from centre)\n                theta = np.arctan2(\n                    corners_i_df.loc[i, y] - roi_center[y],\n                    corners_i_df.loc[i, x] - roi_center[x],\n                )\n                # Getting x, y distances so point is `padding_px` padded (away) from center\n                corners_i_df.loc[i, x] = corners_i_df.loc[i, x] + (padding_px * np.cos(theta))\n                corners_i_df.loc[i, y] = corners_i_df.loc[i, y] + (padding_px * np.sin(theta))\n            # Making the res_df\n            analysis_i_df = AnalysisDf.init_df(keypoints_df.index)\n            # For each individual, getting the in-roi status\n            for indiv in indivs:\n                # Getting average body center (x, y) for each individual\n                analysis_i_df[(indiv, x)] = keypoints_df.loc[:, idx[indiv, bpts, x]].mean(axis=1).values  # type: ignore\n                analysis_i_df[(indiv, y)] = keypoints_df.loc[:, idx[indiv, bpts, y]].mean(axis=1).values  # type: ignore\n                # Determining if the indiv body center is in the ROI\n                analysis_i_df[(indiv, roi_name)] = analysis_i_df[indiv].apply(\n                    lambda pt: pt_in_roi(pt, corners_i_df, logger), axis=1\n                )\n            # Inverting in_roi status if is_in is False\n            if not is_in:\n                analysis_i_df.loc[:, idx[:, roi_name]] = ~analysis_i_df.loc[:, idx[:, roi_name]]  # type: ignore\n            analysis_df_ls.append(analysis_i_df.loc[:, idx[:, roi_name]].astype(np.int8))  # type: ignore\n            scatter_df_ls.append(analysis_i_df)\n            corners_df_ls.append(corners_i_df)\n            roi_names_ls.append(roi_name)\n        # Concatenating all analysis_df_ls and roi_corners_df_ls\n        analysis_df = pd.concat(analysis_df_ls, axis=1)\n        scatter_df = pd.concat(scatter_df_ls, axis=1)\n        corners_df = pd.concat(corners_df_ls, keys=roi_names_ls, names=[\"roi\"]).reset_index(level=\"roi\")\n        # Saving analysis_df\n        fbf_fp = os.path.join(dst_subdir, FBF, f\"{name}.{AnalysisDf.IO}\")\n        AnalysisDf.write(analysis_df, fbf_fp)\n        plot_fp = os.path.join(dst_subdir, \"scatter_plot\", f\"{name}.png\")\n        AnalysisDf.make_location_scatterplot(scatter_df, corners_df, plot_fp)\n        # Summarising and binning analysis_df\n        AnalysisBinnedDf.summary_binned_behavs(\n            analysis_df,\n            dst_subdir,\n            name,\n            fps,\n            bins_ls,\n            cbins_ls,\n        )\n        return get_io_obj_content(io_obj)\n\n    @staticmethod\n    def speed(\n        keypoints_fp: str,\n        dst_dir: str,\n        configs_fp: str,\n    ) -&gt; str:\n        \"\"\"\n        Determines the speed of the subject in each frame.\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        f_name = get_func_name_in_stack()\n        name = get_name(keypoints_fp)\n        dst_subdir = os.path.join(dst_dir, f_name)\n        # Calculating the deltas (changes in body position) between each frame for the subject\n        configs = ExperimentConfigs.read_json(configs_fp)\n        fps, _, _, px_per_mm, bins_ls, cbins_ls = configs.get_analysis_configs()\n        configs_filt = configs.user.analyse.speed\n        bpts = configs.get_ref(configs_filt.bodyparts)\n        smoothing_sec = configs.get_ref(configs_filt.smoothing_sec)\n        # Calculating more parameters\n        smoothing_frames = int(smoothing_sec * fps)\n\n        # Loading in dataframe\n        keypoints_df = KeypointsDf.clean_headings(KeypointsDf.read(keypoints_fp))\n        assert keypoints_df.shape[0] &gt; 0, \"No frames in keypoints_df. Please check keypoints file.\"\n        # Checking body-centre bodypart exists\n        KeypointsDf.check_bpts_exist(keypoints_df, bpts)\n        # Getting indivs and bpts list\n        indivs, _ = KeypointsDf.get_indivs_bpts(keypoints_df)\n\n        # Calculating speed of subject for each frame\n        analysis_df = AnalysisDf.init_df(keypoints_df.index)\n        # keypoints_df.index = analysis_df.index\n        idx = pd.IndexSlice\n        for indiv in indivs:\n            # Making a rolling window of 3 frames for average body-centre\n            # Otherwise jitter contributes to movement\n            jitter_frames = 3\n            smoothed_xy_df = keypoints_df.rolling(window=jitter_frames, min_periods=1, center=True).agg(np.nanmean)\n            # Getting changes in x-y values between frames (deltas)\n            delta_x = smoothed_xy_df.loc[:, idx[indiv, bpts, \"x\"]].mean(axis=1).diff()  # type: ignore\n            delta_y = smoothed_xy_df.loc[:, idx[indiv, bpts, \"y\"]].mean(axis=1).diff()  # type: ignore\n            delta = np.array(np.sqrt(np.power(delta_x, 2) + np.power(delta_y, 2)))\n            # Storing speed (raw and smoothed)\n            analysis_df[(indiv, \"SpeedMMperSec\")] = (delta / px_per_mm) * fps\n            analysis_df[(indiv, \"SpeedMMperSecSmoothed\")] = (\n                analysis_df[(indiv, \"SpeedMMperSec\")]\n                .rolling(window=smoothing_frames, min_periods=1, center=True)\n                .agg(np.nanmean)\n            )\n        # Backfilling the analysis_df so no nan's\n        analysis_df = analysis_df.bfill()\n        # Saving analysis_df\n        fbf_fp = os.path.join(dst_subdir, FBF, f\"{name}.{AnalysisDf.IO}\")\n        AnalysisDf.write(analysis_df, fbf_fp)\n\n        # Summarising and binning analysis_df\n        AnalysisBinnedDf.summary_binned_quantitative(\n            analysis_df,\n            dst_subdir,\n            name,\n            fps,\n            bins_ls,\n            cbins_ls,\n        )\n        return get_io_obj_content(io_obj)\n\n    @staticmethod\n    def social_distance(\n        keypoints_fp: str,\n        dst_dir: str,\n        configs_fp: str,\n    ) -&gt; str:\n        \"\"\"\n        Determines the speed of the subject in each frame.\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        f_name = get_func_name_in_stack()\n        name = get_name(keypoints_fp)\n        dst_subdir = os.path.join(dst_dir, f_name)\n        # Calculating the deltas (changes in body position) between each frame for the subject\n        configs = ExperimentConfigs.read_json(configs_fp)\n        fps, _, _, px_per_mm, bins_ls, cbins_ls = configs.get_analysis_configs()\n        configs_filt = configs.user.analyse.social_distance\n        bpts = configs.get_ref(configs_filt.bodyparts)\n        smoothing_sec = configs.get_ref(configs_filt.smoothing_sec)\n        # Calculating more parameters\n        smoothing_frames = int(smoothing_sec * fps)\n\n        # Loading in dataframe\n        keypoints_df = KeypointsDf.clean_headings(KeypointsDf.read(keypoints_fp))\n        assert keypoints_df.shape[0] &gt; 0, \"No frames in keypoints_df. Please check keypoints file.\"\n        # Checking body-centre bodypart exists\n        KeypointsDf.check_bpts_exist(keypoints_df, bpts)\n        # Getting indivs and bpts list\n        indivs, _ = KeypointsDf.get_indivs_bpts(keypoints_df)\n\n        # Calculating speed of subject for each frame\n        analysis_df = AnalysisDf.init_df(keypoints_df.index)\n        idx = pd.IndexSlice\n        # Assumes there are only two individuals\n        indiv_a = indivs[0]\n        indiv_b = indivs[1]\n        # Getting distances between each individual\n        idx_a = idx[indiv_b, bpts, \"x\"]\n        dist_x = (keypoints_df.loc[:, idx_a] - keypoints_df.loc[:, idx_a]).mean(axis=1)  # type: ignore\n        idx_b = idx[indiv_a, bpts, \"y\"]\n        dist_y = (keypoints_df.loc[:, idx_b] - keypoints_df.loc[:, idx_b]).mean(axis=1)  # type: ignore\n        dist = np.array(np.sqrt(np.power(dist_x, 2) + np.power(dist_y, 2)))\n        # Adding mm distance to saved analysis_df table\n        analysis_df[(f\"{indiv_a}_{indiv_b}\", \"DistMM\")] = dist / px_per_mm\n        analysis_df[(f\"{indiv_a}_{indiv_b}\", \"DistMMSmoothed\")] = (\n            analysis_df[(f\"{indiv_a}_{indiv_b}\", \"DistMM\")]\n            .rolling(window=smoothing_frames, min_periods=1, center=True)\n            .agg(np.nanmean)\n        )\n        # Saving analysis_df\n        fbf_fp = os.path.join(dst_subdir, FBF, f\"{name}.{AnalysisDf.IO}\")\n        AnalysisDf.write(analysis_df, fbf_fp)\n\n        # Summarising and binning analysis_df\n        AnalysisBinnedDf.summary_binned_quantitative(\n            analysis_df,\n            dst_subdir,\n            name,\n            fps,\n            bins_ls,\n            cbins_ls,\n        )\n        return get_io_obj_content(io_obj)\n\n    @staticmethod\n    def freezing(\n        keypoints_fp: str,\n        dst_dir: str,\n        configs_fp: str,\n    ) -&gt; str:\n        \"\"\"\n        Determines the frames in which the subject is frozen.\n\n        \"Frozen\" is defined as not moving outside of a radius of `threshold_mm`, and only\n        includes bouts that last longer than `window_sec` spent seconds.\n\n        NOTE: method is \"greedy\" because it looks at a freezing bout from earliest possible frame.\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        f_name = get_func_name_in_stack()\n        name = get_name(keypoints_fp)\n        dst_subdir = os.path.join(dst_dir, f_name)\n        # Calculating the deltas (changes in body position) between each frame for the subject\n        configs = ExperimentConfigs.read_json(configs_fp)\n        fps, _, _, px_per_mm, bins_ls, cbins_ls = configs.get_analysis_configs()\n        configs_filt = configs.user.analyse.freezing\n        bpts = configs.get_ref(configs_filt.bodyparts)\n        thresh_mm = configs.get_ref(configs_filt.thresh_mm)\n        smoothing_sec = configs.get_ref(configs_filt.smoothing_sec)\n        window_sec = configs.get_ref(configs_filt.window_sec)\n        # Calculating more parameters\n        thresh_px = thresh_mm / px_per_mm\n        smoothing_frames = int(smoothing_sec * fps)\n        window_frames = int(np.round(fps * window_sec, 0))\n\n        # Loading in dataframe\n        keypoints_df = KeypointsDf.clean_headings(KeypointsDf.read(keypoints_fp))\n        assert keypoints_df.shape[0] &gt; 0, \"No frames in keypoints_df. Please check keypoints file.\"\n        # Checking body-centre bodypart exists\n        KeypointsDf.check_bpts_exist(keypoints_df, bpts)\n        # Getting indivs and bpts list\n        indivs, _ = KeypointsDf.get_indivs_bpts(keypoints_df)\n\n        # Calculating speed of subject for each frame\n        analysis_df = AnalysisDf.init_df(keypoints_df.index)\n        keypoints_df.index = analysis_df.index\n        for indiv in indivs:\n            temp_df = pd.DataFrame(index=analysis_df.index)\n            # Calculating frame-by-frame delta distances for current bpt\n            for bpt in bpts:\n                # Getting x and y changes\n                delta_x = keypoints_df[(indiv, bpt, \"x\")].diff()\n                delta_y = keypoints_df[(indiv, bpt, \"y\")].diff()\n                # Getting Euclidean distance between frames for bpt\n                delta = np.sqrt(np.power(delta_x, 2) + np.power(delta_y, 2))\n                # Converting from px to mm\n                temp_df[f\"{bpt}_dist\"] = delta\n                # Smoothing\n                temp_df[f\"{bpt}_dist\"] = (\n                    temp_df[f\"{bpt}_dist\"].rolling(window=smoothing_frames, min_periods=1, center=True).agg(np.nanmean)\n                )\n            # If ALL bodypoints do not leave `thresh_px`\n            analysis_df[(indiv, f_name)] = temp_df.apply(lambda x: pd.Series(np.all(x &lt; thresh_px)), axis=1).astype(\n                np.int8\n            )\n\n            # Getting start, stop, and duration of each freezing behav bout\n            freezingbouts_df = BehavScoredDf.vect2bouts_df(analysis_df[(indiv, f_name)] == 1)\n            # For each freezing bout, if there is less than window_frames, tehn\n            # it is not actually freezing\n            for _, row in freezingbouts_df.iterrows():\n                if row[\"dur\"] &lt; window_frames:\n                    analysis_df.loc[row[\"start\"] : row[\"stop\"], (indiv, f_name)] = 0\n        # Saving analysis_df\n        fbf_fp = os.path.join(dst_subdir, FBF, f\"{name}.{AnalysisDf.IO}\")\n        AnalysisDf.write(analysis_df, fbf_fp)\n\n        # Summarising and binning analysis_df\n        AnalysisBinnedDf.summary_binned_behavs(\n            analysis_df,\n            dst_subdir,\n            name,\n            fps,\n            bins_ls,\n            cbins_ls,\n        )\n        return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.analyse.Analyse.freezing","title":"<code>freezing(keypoints_fp, dst_dir, configs_fp)</code>  <code>staticmethod</code>","text":"<p>Determines the frames in which the subject is frozen.</p> <p>\"Frozen\" is defined as not moving outside of a radius of <code>threshold_mm</code>, and only includes bouts that last longer than <code>window_sec</code> spent seconds.</p> <p>NOTE: method is \"greedy\" because it looks at a freezing bout from earliest possible frame.</p> Source code in <code>behavysis/processes/analyse.py</code> <pre><code>@staticmethod\ndef freezing(\n    keypoints_fp: str,\n    dst_dir: str,\n    configs_fp: str,\n) -&gt; str:\n    \"\"\"\n    Determines the frames in which the subject is frozen.\n\n    \"Frozen\" is defined as not moving outside of a radius of `threshold_mm`, and only\n    includes bouts that last longer than `window_sec` spent seconds.\n\n    NOTE: method is \"greedy\" because it looks at a freezing bout from earliest possible frame.\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    f_name = get_func_name_in_stack()\n    name = get_name(keypoints_fp)\n    dst_subdir = os.path.join(dst_dir, f_name)\n    # Calculating the deltas (changes in body position) between each frame for the subject\n    configs = ExperimentConfigs.read_json(configs_fp)\n    fps, _, _, px_per_mm, bins_ls, cbins_ls = configs.get_analysis_configs()\n    configs_filt = configs.user.analyse.freezing\n    bpts = configs.get_ref(configs_filt.bodyparts)\n    thresh_mm = configs.get_ref(configs_filt.thresh_mm)\n    smoothing_sec = configs.get_ref(configs_filt.smoothing_sec)\n    window_sec = configs.get_ref(configs_filt.window_sec)\n    # Calculating more parameters\n    thresh_px = thresh_mm / px_per_mm\n    smoothing_frames = int(smoothing_sec * fps)\n    window_frames = int(np.round(fps * window_sec, 0))\n\n    # Loading in dataframe\n    keypoints_df = KeypointsDf.clean_headings(KeypointsDf.read(keypoints_fp))\n    assert keypoints_df.shape[0] &gt; 0, \"No frames in keypoints_df. Please check keypoints file.\"\n    # Checking body-centre bodypart exists\n    KeypointsDf.check_bpts_exist(keypoints_df, bpts)\n    # Getting indivs and bpts list\n    indivs, _ = KeypointsDf.get_indivs_bpts(keypoints_df)\n\n    # Calculating speed of subject for each frame\n    analysis_df = AnalysisDf.init_df(keypoints_df.index)\n    keypoints_df.index = analysis_df.index\n    for indiv in indivs:\n        temp_df = pd.DataFrame(index=analysis_df.index)\n        # Calculating frame-by-frame delta distances for current bpt\n        for bpt in bpts:\n            # Getting x and y changes\n            delta_x = keypoints_df[(indiv, bpt, \"x\")].diff()\n            delta_y = keypoints_df[(indiv, bpt, \"y\")].diff()\n            # Getting Euclidean distance between frames for bpt\n            delta = np.sqrt(np.power(delta_x, 2) + np.power(delta_y, 2))\n            # Converting from px to mm\n            temp_df[f\"{bpt}_dist\"] = delta\n            # Smoothing\n            temp_df[f\"{bpt}_dist\"] = (\n                temp_df[f\"{bpt}_dist\"].rolling(window=smoothing_frames, min_periods=1, center=True).agg(np.nanmean)\n            )\n        # If ALL bodypoints do not leave `thresh_px`\n        analysis_df[(indiv, f_name)] = temp_df.apply(lambda x: pd.Series(np.all(x &lt; thresh_px)), axis=1).astype(\n            np.int8\n        )\n\n        # Getting start, stop, and duration of each freezing behav bout\n        freezingbouts_df = BehavScoredDf.vect2bouts_df(analysis_df[(indiv, f_name)] == 1)\n        # For each freezing bout, if there is less than window_frames, tehn\n        # it is not actually freezing\n        for _, row in freezingbouts_df.iterrows():\n            if row[\"dur\"] &lt; window_frames:\n                analysis_df.loc[row[\"start\"] : row[\"stop\"], (indiv, f_name)] = 0\n    # Saving analysis_df\n    fbf_fp = os.path.join(dst_subdir, FBF, f\"{name}.{AnalysisDf.IO}\")\n    AnalysisDf.write(analysis_df, fbf_fp)\n\n    # Summarising and binning analysis_df\n    AnalysisBinnedDf.summary_binned_behavs(\n        analysis_df,\n        dst_subdir,\n        name,\n        fps,\n        bins_ls,\n        cbins_ls,\n    )\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.analyse.Analyse.in_roi","title":"<code>in_roi(keypoints_fp, dst_dir, configs_fp)</code>  <code>staticmethod</code>","text":"<p>Determines the frames in which the subject is inside the cage (from average of given bodypoints).</p> <p>Points are <code>padding_px</code> padded (away) from center.</p> Source code in <code>behavysis/processes/analyse.py</code> <pre><code>@staticmethod\ndef in_roi(\n    keypoints_fp: str,\n    dst_dir: str,\n    configs_fp: str,\n) -&gt; str:\n    \"\"\"\n    Determines the frames in which the subject is inside the cage (from average\n    of given bodypoints).\n\n    Points are `padding_px` padded (away) from center.\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    f_name = get_func_name_in_stack()\n    name = get_name(keypoints_fp)\n    dst_subdir = os.path.join(dst_dir, f_name)\n    # Calculating the deltas (changes in body position) between each frame for the subject\n    configs = ExperimentConfigs.read_json(configs_fp)\n    fps, _, _, px_per_mm, bins_ls, cbins_ls = configs.get_analysis_configs()\n    configs_filt_ls = configs.user.analyse.in_roi\n    # Loading in dataframe\n    keypoints_df = KeypointsDf.clean_headings(KeypointsDf.read(keypoints_fp))\n    assert keypoints_df.shape[0] &gt; 0, \"No frames in keypoints_df. Please check keypoints file.\"\n    # Getting indivs list\n    indivs, _ = KeypointsDf.get_indivs_bpts(keypoints_df)\n    # Making analysis_df\n    analysis_df_ls = []\n    scatter_df_ls = []\n    corners_df_ls = []\n    roi_names_ls = []\n    # For each roi, calculate the in-roi status of the subject\n    x = CoordsCols.X.value\n    y = CoordsCols.Y.value\n    idx = pd.IndexSlice\n    for configs_filt in configs_filt_ls:\n        # Getting necessary config parameters\n        roi_name = configs.get_ref(configs_filt.roi_name)\n        is_in = configs.get_ref(configs_filt.is_in)\n        bpts = configs.get_ref(configs_filt.bodyparts)\n        padding_mm = configs.get_ref(configs_filt.padding_mm)\n        roi_corners = configs.get_ref(configs_filt.roi_corners)\n        # Calculating more parameters\n        padding_px = padding_mm / px_per_mm\n        # Checking bodyparts and roi_corners exist\n        KeypointsDf.check_bpts_exist(keypoints_df, bpts)\n        KeypointsDf.check_bpts_exist(keypoints_df, roi_corners)\n        # Getting average corner coordinates. Assumes arena does not move.\n        corners_i_df = pd.DataFrame([keypoints_df[(IndivCols.SINGLE.value, pt)].mean() for pt in roi_corners]).drop(\n            columns=[\"likelihood\"]\n        )\n        # Adjusting x-y to have `padding_px` dilation/erosion from the points themselves\n        roi_center = corners_i_df.mean()\n        for i in corners_i_df.index:\n            # Calculating angle from centre to point (going out from centre)\n            theta = np.arctan2(\n                corners_i_df.loc[i, y] - roi_center[y],\n                corners_i_df.loc[i, x] - roi_center[x],\n            )\n            # Getting x, y distances so point is `padding_px` padded (away) from center\n            corners_i_df.loc[i, x] = corners_i_df.loc[i, x] + (padding_px * np.cos(theta))\n            corners_i_df.loc[i, y] = corners_i_df.loc[i, y] + (padding_px * np.sin(theta))\n        # Making the res_df\n        analysis_i_df = AnalysisDf.init_df(keypoints_df.index)\n        # For each individual, getting the in-roi status\n        for indiv in indivs:\n            # Getting average body center (x, y) for each individual\n            analysis_i_df[(indiv, x)] = keypoints_df.loc[:, idx[indiv, bpts, x]].mean(axis=1).values  # type: ignore\n            analysis_i_df[(indiv, y)] = keypoints_df.loc[:, idx[indiv, bpts, y]].mean(axis=1).values  # type: ignore\n            # Determining if the indiv body center is in the ROI\n            analysis_i_df[(indiv, roi_name)] = analysis_i_df[indiv].apply(\n                lambda pt: pt_in_roi(pt, corners_i_df, logger), axis=1\n            )\n        # Inverting in_roi status if is_in is False\n        if not is_in:\n            analysis_i_df.loc[:, idx[:, roi_name]] = ~analysis_i_df.loc[:, idx[:, roi_name]]  # type: ignore\n        analysis_df_ls.append(analysis_i_df.loc[:, idx[:, roi_name]].astype(np.int8))  # type: ignore\n        scatter_df_ls.append(analysis_i_df)\n        corners_df_ls.append(corners_i_df)\n        roi_names_ls.append(roi_name)\n    # Concatenating all analysis_df_ls and roi_corners_df_ls\n    analysis_df = pd.concat(analysis_df_ls, axis=1)\n    scatter_df = pd.concat(scatter_df_ls, axis=1)\n    corners_df = pd.concat(corners_df_ls, keys=roi_names_ls, names=[\"roi\"]).reset_index(level=\"roi\")\n    # Saving analysis_df\n    fbf_fp = os.path.join(dst_subdir, FBF, f\"{name}.{AnalysisDf.IO}\")\n    AnalysisDf.write(analysis_df, fbf_fp)\n    plot_fp = os.path.join(dst_subdir, \"scatter_plot\", f\"{name}.png\")\n    AnalysisDf.make_location_scatterplot(scatter_df, corners_df, plot_fp)\n    # Summarising and binning analysis_df\n    AnalysisBinnedDf.summary_binned_behavs(\n        analysis_df,\n        dst_subdir,\n        name,\n        fps,\n        bins_ls,\n        cbins_ls,\n    )\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.analyse.Analyse.social_distance","title":"<code>social_distance(keypoints_fp, dst_dir, configs_fp)</code>  <code>staticmethod</code>","text":"<p>Determines the speed of the subject in each frame.</p> Source code in <code>behavysis/processes/analyse.py</code> <pre><code>@staticmethod\ndef social_distance(\n    keypoints_fp: str,\n    dst_dir: str,\n    configs_fp: str,\n) -&gt; str:\n    \"\"\"\n    Determines the speed of the subject in each frame.\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    f_name = get_func_name_in_stack()\n    name = get_name(keypoints_fp)\n    dst_subdir = os.path.join(dst_dir, f_name)\n    # Calculating the deltas (changes in body position) between each frame for the subject\n    configs = ExperimentConfigs.read_json(configs_fp)\n    fps, _, _, px_per_mm, bins_ls, cbins_ls = configs.get_analysis_configs()\n    configs_filt = configs.user.analyse.social_distance\n    bpts = configs.get_ref(configs_filt.bodyparts)\n    smoothing_sec = configs.get_ref(configs_filt.smoothing_sec)\n    # Calculating more parameters\n    smoothing_frames = int(smoothing_sec * fps)\n\n    # Loading in dataframe\n    keypoints_df = KeypointsDf.clean_headings(KeypointsDf.read(keypoints_fp))\n    assert keypoints_df.shape[0] &gt; 0, \"No frames in keypoints_df. Please check keypoints file.\"\n    # Checking body-centre bodypart exists\n    KeypointsDf.check_bpts_exist(keypoints_df, bpts)\n    # Getting indivs and bpts list\n    indivs, _ = KeypointsDf.get_indivs_bpts(keypoints_df)\n\n    # Calculating speed of subject for each frame\n    analysis_df = AnalysisDf.init_df(keypoints_df.index)\n    idx = pd.IndexSlice\n    # Assumes there are only two individuals\n    indiv_a = indivs[0]\n    indiv_b = indivs[1]\n    # Getting distances between each individual\n    idx_a = idx[indiv_b, bpts, \"x\"]\n    dist_x = (keypoints_df.loc[:, idx_a] - keypoints_df.loc[:, idx_a]).mean(axis=1)  # type: ignore\n    idx_b = idx[indiv_a, bpts, \"y\"]\n    dist_y = (keypoints_df.loc[:, idx_b] - keypoints_df.loc[:, idx_b]).mean(axis=1)  # type: ignore\n    dist = np.array(np.sqrt(np.power(dist_x, 2) + np.power(dist_y, 2)))\n    # Adding mm distance to saved analysis_df table\n    analysis_df[(f\"{indiv_a}_{indiv_b}\", \"DistMM\")] = dist / px_per_mm\n    analysis_df[(f\"{indiv_a}_{indiv_b}\", \"DistMMSmoothed\")] = (\n        analysis_df[(f\"{indiv_a}_{indiv_b}\", \"DistMM\")]\n        .rolling(window=smoothing_frames, min_periods=1, center=True)\n        .agg(np.nanmean)\n    )\n    # Saving analysis_df\n    fbf_fp = os.path.join(dst_subdir, FBF, f\"{name}.{AnalysisDf.IO}\")\n    AnalysisDf.write(analysis_df, fbf_fp)\n\n    # Summarising and binning analysis_df\n    AnalysisBinnedDf.summary_binned_quantitative(\n        analysis_df,\n        dst_subdir,\n        name,\n        fps,\n        bins_ls,\n        cbins_ls,\n    )\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.analyse.Analyse.speed","title":"<code>speed(keypoints_fp, dst_dir, configs_fp)</code>  <code>staticmethod</code>","text":"<p>Determines the speed of the subject in each frame.</p> Source code in <code>behavysis/processes/analyse.py</code> <pre><code>@staticmethod\ndef speed(\n    keypoints_fp: str,\n    dst_dir: str,\n    configs_fp: str,\n) -&gt; str:\n    \"\"\"\n    Determines the speed of the subject in each frame.\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    f_name = get_func_name_in_stack()\n    name = get_name(keypoints_fp)\n    dst_subdir = os.path.join(dst_dir, f_name)\n    # Calculating the deltas (changes in body position) between each frame for the subject\n    configs = ExperimentConfigs.read_json(configs_fp)\n    fps, _, _, px_per_mm, bins_ls, cbins_ls = configs.get_analysis_configs()\n    configs_filt = configs.user.analyse.speed\n    bpts = configs.get_ref(configs_filt.bodyparts)\n    smoothing_sec = configs.get_ref(configs_filt.smoothing_sec)\n    # Calculating more parameters\n    smoothing_frames = int(smoothing_sec * fps)\n\n    # Loading in dataframe\n    keypoints_df = KeypointsDf.clean_headings(KeypointsDf.read(keypoints_fp))\n    assert keypoints_df.shape[0] &gt; 0, \"No frames in keypoints_df. Please check keypoints file.\"\n    # Checking body-centre bodypart exists\n    KeypointsDf.check_bpts_exist(keypoints_df, bpts)\n    # Getting indivs and bpts list\n    indivs, _ = KeypointsDf.get_indivs_bpts(keypoints_df)\n\n    # Calculating speed of subject for each frame\n    analysis_df = AnalysisDf.init_df(keypoints_df.index)\n    # keypoints_df.index = analysis_df.index\n    idx = pd.IndexSlice\n    for indiv in indivs:\n        # Making a rolling window of 3 frames for average body-centre\n        # Otherwise jitter contributes to movement\n        jitter_frames = 3\n        smoothed_xy_df = keypoints_df.rolling(window=jitter_frames, min_periods=1, center=True).agg(np.nanmean)\n        # Getting changes in x-y values between frames (deltas)\n        delta_x = smoothed_xy_df.loc[:, idx[indiv, bpts, \"x\"]].mean(axis=1).diff()  # type: ignore\n        delta_y = smoothed_xy_df.loc[:, idx[indiv, bpts, \"y\"]].mean(axis=1).diff()  # type: ignore\n        delta = np.array(np.sqrt(np.power(delta_x, 2) + np.power(delta_y, 2)))\n        # Storing speed (raw and smoothed)\n        analysis_df[(indiv, \"SpeedMMperSec\")] = (delta / px_per_mm) * fps\n        analysis_df[(indiv, \"SpeedMMperSecSmoothed\")] = (\n            analysis_df[(indiv, \"SpeedMMperSec\")]\n            .rolling(window=smoothing_frames, min_periods=1, center=True)\n            .agg(np.nanmean)\n        )\n    # Backfilling the analysis_df so no nan's\n    analysis_df = analysis_df.bfill()\n    # Saving analysis_df\n    fbf_fp = os.path.join(dst_subdir, FBF, f\"{name}.{AnalysisDf.IO}\")\n    AnalysisDf.write(analysis_df, fbf_fp)\n\n    # Summarising and binning analysis_df\n    AnalysisBinnedDf.summary_binned_quantitative(\n        analysis_df,\n        dst_subdir,\n        name,\n        fps,\n        bins_ls,\n        cbins_ls,\n    )\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.combine_analysis","title":"<code>behavysis.processes.combine_analysis</code>","text":""},{"location":"reference/processes.html#behavysis.processes.combine_analysis.CombineAnalysis","title":"<code>CombineAnalysis</code>","text":"Source code in <code>behavysis/processes/combine_analysis.py</code> <pre><code>class CombineAnalysis:\n    @classmethod\n    def combine_analysis(\n        cls,\n        analysis_dir: str,\n        analysis_combined_fp: str,\n        configs_fp: str,\n        overwrite: bool,\n    ) -&gt; str:\n        \"\"\"\n        Concatenates across columns the frame-by-frame dataframes for all analysis subdirectories\n        and saves this in a single dataframe.\n        \"\"\"\n        logger, io_obj = init_logger_io_obj()\n        if not overwrite and os.path.exists(analysis_combined_fp):\n            logger.warning(file_exists_msg(analysis_combined_fp))\n            return get_io_obj_content(io_obj)\n        name = get_name(configs_fp)\n        # For each analysis subdir, combining fbf files\n        analysis_subdir_ls = [i for i in os.listdir(analysis_dir) if os.path.isdir(os.path.join(analysis_dir, i))]\n        # If no analysis files, then return warning and don't make df\n        if len(analysis_subdir_ls) == 0:\n            logger.warning(\"no analysis fbf files made. Run `exp.analyse` first\")\n            return get_io_obj_content(io_obj)\n        # Reading in each fbf analysis df\n        comb_df_ls = [\n            AnalysisDf.read(os.path.join(analysis_dir, analysis_subdir, FBF, f\"{name}.{AnalysisDf.IO}\"))\n            for analysis_subdir in analysis_subdir_ls\n        ]\n        # Making combined df from list of dfs\n        comb_df = pd.concat(\n            comb_df_ls,\n            axis=1,\n            keys=analysis_subdir_ls,\n            names=[AnalysisCombinedDf.CN.ANALYSIS.value],\n        )\n        # Writing to file\n        AnalysisCombinedDf.write(comb_df, analysis_combined_fp)\n        return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.combine_analysis.CombineAnalysis.combine_analysis","title":"<code>combine_analysis(analysis_dir, analysis_combined_fp, configs_fp, overwrite)</code>  <code>classmethod</code>","text":"<p>Concatenates across columns the frame-by-frame dataframes for all analysis subdirectories and saves this in a single dataframe.</p> Source code in <code>behavysis/processes/combine_analysis.py</code> <pre><code>@classmethod\ndef combine_analysis(\n    cls,\n    analysis_dir: str,\n    analysis_combined_fp: str,\n    configs_fp: str,\n    overwrite: bool,\n) -&gt; str:\n    \"\"\"\n    Concatenates across columns the frame-by-frame dataframes for all analysis subdirectories\n    and saves this in a single dataframe.\n    \"\"\"\n    logger, io_obj = init_logger_io_obj()\n    if not overwrite and os.path.exists(analysis_combined_fp):\n        logger.warning(file_exists_msg(analysis_combined_fp))\n        return get_io_obj_content(io_obj)\n    name = get_name(configs_fp)\n    # For each analysis subdir, combining fbf files\n    analysis_subdir_ls = [i for i in os.listdir(analysis_dir) if os.path.isdir(os.path.join(analysis_dir, i))]\n    # If no analysis files, then return warning and don't make df\n    if len(analysis_subdir_ls) == 0:\n        logger.warning(\"no analysis fbf files made. Run `exp.analyse` first\")\n        return get_io_obj_content(io_obj)\n    # Reading in each fbf analysis df\n    comb_df_ls = [\n        AnalysisDf.read(os.path.join(analysis_dir, analysis_subdir, FBF, f\"{name}.{AnalysisDf.IO}\"))\n        for analysis_subdir in analysis_subdir_ls\n    ]\n    # Making combined df from list of dfs\n    comb_df = pd.concat(\n        comb_df_ls,\n        axis=1,\n        keys=analysis_subdir_ls,\n        names=[AnalysisCombinedDf.CN.ANALYSIS.value],\n    )\n    # Writing to file\n    AnalysisCombinedDf.write(comb_df, analysis_combined_fp)\n    return get_io_obj_content(io_obj)\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.evaluate.Evaluate","title":"<code>behavysis.processes.evaluate.Evaluate</code>","text":"<p>summary</p> Source code in <code>behavysis/processes/evaluate/__init__.py</code> <pre><code>class Evaluate:\n    \"\"\"__summary__\"\"\"\n\n    ###############################################################################################\n    #               MAKE KEYPOINTS PLOTS\n    ###############################################################################################\n\n    @staticmethod\n    def keypoints_plot(\n        vid_fp: str,\n        dlc_fp: str,\n        behavs_fp: str,\n        dst_dir: str,\n        configs_fp: str,\n        overwrite: bool,\n    ) -&gt; str:\n        \"\"\"\n        Make keypoints evaluation plot of likelihood of each bodypart through time.\n        \"\"\"\n        name = get_name(dlc_fp)\n        dst_dir = os.path.join(dst_dir, Evaluate.keypoints_plot.__name__)\n        dst_fp = os.path.join(dst_dir, f\"{name}.png\")\n        os.makedirs(dst_dir, exist_ok=True)\n\n        # Getting necessary config parameters\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs_filt = configs.user.evaluate.keypoints_plot\n        bpts = configs.get_ref(configs_filt.bodyparts)\n        fps = configs.auto.formatted_vid.fps\n\n        # Read the file\n        df = KeypointsDf.clean_headings(KeypointsDf.read(dlc_fp))\n        # Checking the bodyparts specified in the configs exist in the dataframe\n        KeypointsDf.check_bpts_exist(df, bpts)\n        # Making data-long ways\n        idx = pd.IndexSlice\n        df = (\n            df.loc[:, idx[:, bpts]]\n            .stack([KeypointsDf.CN.INDIVIDUALS.value, KeypointsDf.CN.BODYPARTS.value])\n            .reset_index()\n        )\n        # Adding the timestamp column\n        df[\"timestamp\"] = df[KeypointsDf.IN.FRAME.value] / fps\n        # Making plot\n        g = sns.FacetGrid(\n            df,\n            row=KeypointsDf.CN.INDIVIDUALS.value,\n            height=5,\n            aspect=10,\n        )\n        g.map_dataframe(\n            sns.lineplot,\n            x=\"timestamp\",\n            y=CoordsCols.LIKELIHOOD.value,\n            hue=KeypointsDf.CN.BODYPARTS.value,\n            alpha=0.4,\n        )\n        g.add_legend()\n        # Saving plot\n        g.savefig(dst_fp)\n        g.figure.clf()\n        return \"\"\n\n    ###############################################################################################\n    # MAKE BEHAVIOUR PLOTS\n    ###############################################################################################\n\n    @staticmethod\n    def behav_plot(\n        vid_fp: str,\n        dlc_fp: str,\n        behavs_fp: str,\n        dst_dir: str,\n        configs_fp: str,\n        overwrite: bool,\n    ) -&gt; str:\n        \"\"\"\n        Make behaviour evaluation plot of the predicted and actual behaviours through time.\n        \"\"\"\n        name = get_name(behavs_fp)\n        dst_dir = os.path.join(dst_dir, Evaluate.behav_plot.__name__)\n        dst_fp = os.path.join(dst_dir, f\"{name}.png\")\n        os.makedirs(dst_dir, exist_ok=True)\n        # If overwrite is False, checking if we should skip processing\n        if not overwrite and os.path.exists(dst_fp):\n            return file_exists_msg()\n\n        # Getting necessary config parameters\n        configs = ExperimentConfigs.read_json(configs_fp)\n        # configs_filt = configs.user.evaluate.behav_plot\n        fps = float(configs.auto.formatted_vid.fps)\n\n        # Read the file\n        df = BehavScoredDf.read(behavs_fp)\n        # Making data-long ways\n        df = (\n            df.stack([BehavScoredDf.CN.BEHAVS.value, BehavScoredDf.CN.OUTCOMES.value])\n            .reset_index()\n            .rename(columns={0: \"value\"})\n        )\n        # Adding the timestamp column\n        df[\"timestamp\"] = df[BehavScoredDf.IN.FRAME.value] / fps\n        # Making plot\n        g = sns.FacetGrid(\n            df,\n            row=BehavScoredDf.CN.BEHAVS.value,\n            height=5,\n            aspect=10,\n        )\n        g.map_dataframe(\n            sns.lineplot,\n            x=\"timestamp\",\n            y=\"value\",\n            hue=BehavScoredDf.CN.OUTCOMES.value,\n            alpha=0.4,\n        )\n        g.add_legend()\n        # Saving plot\n        g.savefig(dst_fp)\n        g.figure.clf()\n        return \"\"\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.evaluate.Evaluate.behav_plot","title":"<code>behav_plot(vid_fp, dlc_fp, behavs_fp, dst_dir, configs_fp, overwrite)</code>  <code>staticmethod</code>","text":"<p>Make behaviour evaluation plot of the predicted and actual behaviours through time.</p> Source code in <code>behavysis/processes/evaluate/__init__.py</code> <pre><code>@staticmethod\ndef behav_plot(\n    vid_fp: str,\n    dlc_fp: str,\n    behavs_fp: str,\n    dst_dir: str,\n    configs_fp: str,\n    overwrite: bool,\n) -&gt; str:\n    \"\"\"\n    Make behaviour evaluation plot of the predicted and actual behaviours through time.\n    \"\"\"\n    name = get_name(behavs_fp)\n    dst_dir = os.path.join(dst_dir, Evaluate.behav_plot.__name__)\n    dst_fp = os.path.join(dst_dir, f\"{name}.png\")\n    os.makedirs(dst_dir, exist_ok=True)\n    # If overwrite is False, checking if we should skip processing\n    if not overwrite and os.path.exists(dst_fp):\n        return file_exists_msg()\n\n    # Getting necessary config parameters\n    configs = ExperimentConfigs.read_json(configs_fp)\n    # configs_filt = configs.user.evaluate.behav_plot\n    fps = float(configs.auto.formatted_vid.fps)\n\n    # Read the file\n    df = BehavScoredDf.read(behavs_fp)\n    # Making data-long ways\n    df = (\n        df.stack([BehavScoredDf.CN.BEHAVS.value, BehavScoredDf.CN.OUTCOMES.value])\n        .reset_index()\n        .rename(columns={0: \"value\"})\n    )\n    # Adding the timestamp column\n    df[\"timestamp\"] = df[BehavScoredDf.IN.FRAME.value] / fps\n    # Making plot\n    g = sns.FacetGrid(\n        df,\n        row=BehavScoredDf.CN.BEHAVS.value,\n        height=5,\n        aspect=10,\n    )\n    g.map_dataframe(\n        sns.lineplot,\n        x=\"timestamp\",\n        y=\"value\",\n        hue=BehavScoredDf.CN.OUTCOMES.value,\n        alpha=0.4,\n    )\n    g.add_legend()\n    # Saving plot\n    g.savefig(dst_fp)\n    g.figure.clf()\n    return \"\"\n</code></pre>"},{"location":"reference/processes.html#behavysis.processes.evaluate.Evaluate.keypoints_plot","title":"<code>keypoints_plot(vid_fp, dlc_fp, behavs_fp, dst_dir, configs_fp, overwrite)</code>  <code>staticmethod</code>","text":"<p>Make keypoints evaluation plot of likelihood of each bodypart through time.</p> Source code in <code>behavysis/processes/evaluate/__init__.py</code> <pre><code>@staticmethod\ndef keypoints_plot(\n    vid_fp: str,\n    dlc_fp: str,\n    behavs_fp: str,\n    dst_dir: str,\n    configs_fp: str,\n    overwrite: bool,\n) -&gt; str:\n    \"\"\"\n    Make keypoints evaluation plot of likelihood of each bodypart through time.\n    \"\"\"\n    name = get_name(dlc_fp)\n    dst_dir = os.path.join(dst_dir, Evaluate.keypoints_plot.__name__)\n    dst_fp = os.path.join(dst_dir, f\"{name}.png\")\n    os.makedirs(dst_dir, exist_ok=True)\n\n    # Getting necessary config parameters\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs_filt = configs.user.evaluate.keypoints_plot\n    bpts = configs.get_ref(configs_filt.bodyparts)\n    fps = configs.auto.formatted_vid.fps\n\n    # Read the file\n    df = KeypointsDf.clean_headings(KeypointsDf.read(dlc_fp))\n    # Checking the bodyparts specified in the configs exist in the dataframe\n    KeypointsDf.check_bpts_exist(df, bpts)\n    # Making data-long ways\n    idx = pd.IndexSlice\n    df = (\n        df.loc[:, idx[:, bpts]]\n        .stack([KeypointsDf.CN.INDIVIDUALS.value, KeypointsDf.CN.BODYPARTS.value])\n        .reset_index()\n    )\n    # Adding the timestamp column\n    df[\"timestamp\"] = df[KeypointsDf.IN.FRAME.value] / fps\n    # Making plot\n    g = sns.FacetGrid(\n        df,\n        row=KeypointsDf.CN.INDIVIDUALS.value,\n        height=5,\n        aspect=10,\n    )\n    g.map_dataframe(\n        sns.lineplot,\n        x=\"timestamp\",\n        y=CoordsCols.LIKELIHOOD.value,\n        hue=KeypointsDf.CN.BODYPARTS.value,\n        alpha=0.4,\n    )\n    g.add_legend()\n    # Saving plot\n    g.savefig(dst_fp)\n    g.figure.clf()\n    return \"\"\n</code></pre>"},{"location":"reference/project.html","title":"Project","text":""},{"location":"reference/project.html#behavysis.pipeline.project.Project","title":"<code>behavysis.pipeline.project.Project</code>","text":"<p>A project is used to process and analyse many experiments at the same time.</p> <p>Attributes:</p> Name Type Description <code>root_dir</code> <code>str</code> <pre><code>The filepath of the project directory. Can be relative to\ncurrent dir or absolute dir.\n</code></pre> <p>experiments : dict[str, Experiment]     The experiments that have been loaded into the project. nprocs : int     The number of processes to use for multiprocessing.</p> Source code in <code>behavysis/pipeline/project.py</code> <pre><code>class Project:\n    \"\"\"\n    A project is used to process and analyse many experiments at the same time.\n\n    Attributes\n    ----------\n        root_dir : str\n            The filepath of the project directory. Can be relative to\n            current dir or absolute dir.\n        experiments : dict[str, Experiment]\n            The experiments that have been loaded into the project.\n        nprocs : int\n            The number of processes to use for multiprocessing.\n    \"\"\"\n\n    logger = init_logger_file()\n\n    root_dir: str\n    _experiments: dict[str, Experiment]\n    nprocs: int\n\n    def __init__(self, root_dir: str) -&gt; None:\n        \"\"\"\n        Make a Project instance.\n\n        Parameters\n        ----------\n        root_dir : str\n            The filepath of the project directory. Can be relative to\n            current dir or absolute dir.\n        \"\"\"\n        # Assertion: project directory must exist\n        if not os.path.isdir(root_dir):\n            raise ValueError(\n                f'Error: The folder, \"{root_dir}\" does not exist.\\n'\n                \"Please specify a folder that exists. Ensure you have the correct\"\n                \"forward-slashes or back-slashes for the path name.\"\n            )\n        self.root_dir = os.path.abspath(root_dir)\n        self._experiments = {}\n        self.nprocs = 4\n\n    #####################################################################\n    # GETTER METHODS\n    #####################################################################\n\n    @property\n    def experiments(self) -&gt; list[Experiment]:\n        \"\"\"\n        Gets the ordered list of Experiment instances in the Project.\n\n        Returns\n        -------\n        list[Experiment]\n            The list of all Experiment instances stored in the Project instance.\n        \"\"\"\n        return [self._experiments[i] for i in natsorted(self._experiments)]\n\n    def get_experiment(self, name: str) -&gt; Experiment:\n        \"\"\"\n        Gets the experiment with the given name\n\n        Parameters\n        ----------\n        name : str\n            The experiment name.\n\n        Returns\n        -------\n        Experiment\n            The experiment.\n\n        Raises\n        ------\n        ValueError\n            Experiment with the given name does not exist.\n        \"\"\"\n        if name in self._experiments:\n            return self._experiments[name]\n        raise ValueError(f'Experiment with the name \"{name}\" does not exist in the project.')\n\n    #####################################################################\n    #               PROJECT PROCESSING SCAFFOLD METHODS\n    #####################################################################\n\n    def _proc_scaff_mp(self, method: Callable, *args: Any, **kwargs: Any) -&gt; list[dict]:\n        \"\"\"\n        Processes an experiment with the given `Experiment` method and records\n        the diagnostics of the process in a MULTI-PROCESSING way.\n\n        Parameters\n        ----------\n        method : Callable\n            The `Experiment` class method to run.\n\n        Notes\n        -----\n        Can call any `Experiment` methods instance.\n        Effectively, `method` gets called with:\n        ```\n        exp is a Experiment instance\n        method(exp, *args, **kwargs)\n        ```\n        \"\"\"\n        # Starting a dask cluster\n        with cluster_process(LocalCluster(n_workers=self.nprocs, threads_per_worker=1)):\n            # Preparing all experiments for execution\n            f_d_ls = [dask.delayed(method)(exp, *args, **kwargs) for exp in self.experiments]  # type: ignore\n            # Executing in parallel\n            dd_ls = list(dask.compute(*f_d_ls))  # type: ignore\n        return dd_ls\n\n    def _proc_scaff_sp(self, method: Callable, *args: Any, **kwargs: Any) -&gt; list[dict]:\n        \"\"\"\n        Processes an experiment with the given `Experiment` method and records\n        the diagnostics of the process in a SINGLE-PROCESSING way.\n\n        Parameters\n        ----------\n        method : Callable\n            The experiment `Experiment` class method to run.\n\n        Notes\n        -----\n        Can call any `Experiment` instance method.\n        Effectively, `method` gets called with:\n        ```\n        exp is a Experiment instance\n        method(exp, *args, **kwargs)\n        ```\n        \"\"\"\n        # Processing all experiments and storing process outcomes as list of dicts\n        return [method(exp, *args, **kwargs) for exp in self.experiments]\n\n    def _proc_scaff(self, method: Callable, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Runs the given method on all experiments in the project.\n        \"\"\"\n        # Choosing whether to run the scaffold function in single or multi-processing mode\n        if self.nprocs == 1:\n            scaffold_func = self._proc_scaff_sp\n        else:\n            scaffold_func = self._proc_scaff_mp\n        # Running the scaffold function\n        # Starting\n        self.logger.info(f\"Running {method.__name__} for all experiments.\")\n        # Running\n        dd_ls = scaffold_func(method, *args, **kwargs)\n        if len(dd_ls) &gt; 0:\n            # Processing all experiments\n            df = DiagnosticsDf.init_from_dd_ls(dd_ls)\n            # Updating the diagnostics file at each step\n            DiagnosticsDf.write(df, os.path.join(self.root_dir, DIAGNOSTICS_DIR, f\"{method.__name__}.csv\"))\n            # Finishing\n            self.logger.info(f\"Finished running {method.__name__} for all experiments\")\n\n    #####################################################################\n    #               IMPORT EXPERIMENTS METHODS\n    #####################################################################\n\n    def import_experiment(self, name: str) -&gt; bool:\n        \"\"\"\n        Adds an experiment with the given name to the .experiments dict.\n        The key of this experiment in the `self.experiments` dict is \"dir/name\".\n        If the experiment already exists in the project, it is not added.\n\n        Parameters\n        ----------\n        name : str\n            The experiment name.\n\n        Returns\n        -------\n        bool\n            Whether the experiment was imported or not.\n            True if imported, False if not.\n        \"\"\"\n        if name not in self._experiments:\n            self._experiments[name] = Experiment(name, self.root_dir)\n            return True\n        return False\n\n    def import_experiments(self) -&gt; None:\n        \"\"\"\n        Add all experiments in the project folder to the experiments dict.\n        The key of each experiment in the .experiments dict is \"name\".\n        Refer to Project.addExperiment() for details about how each experiment is added.\n        \"\"\"\n        self.logger.info(f\"Searching project folder: {self.root_dir}\")\n        # Storing file existences in {folder1: [file1, file2, ...], ...} format\n        dd_dict = {}\n        # Adding all experiments within given project dir\n        for f in Folders:\n            folder = os.path.join(self.root_dir, f.value)\n            dd_dict[f.value] = []\n            # If folder does not exist, skip\n            if not os.path.isdir(folder):\n                continue\n            # For each file in the folder\n            for fp_name in natsorted(os.listdir(folder)):\n                if re.search(r\"^\\.\", fp_name):  # do not add hidden files\n                    continue\n                name = get_name(fp_name)\n                try:\n                    self.import_experiment(name)\n                    dd_dict[f.value].append(name)\n                except ValueError as e:  # do not add invalid files\n                    self.logger.info(f\"failed: {f.value}    --    {fp_name}: {e}\")\n        # Logging outcome of imported and failed experiments\n        exp_ls_msg = \"\".join([f\"\\n    - {exp.name}\" for exp in self.experiments])\n        self.logger.info(f\"Experiments imported:{exp_ls_msg}\")\n        # Constructing dd_df from dd_dict\n        dd_df = DiagnosticsDf.init_df(pd.Series(np.unique(np.concatenate(list(dd_dict.values())))))\n        # Setting each (experiment, folder) pair to True if the file exists\n        for folder in dd_dict:\n            dd_df[folder] = False\n            for exp_name in dd_dict[folder]:\n                dd_df.loc[exp_name, folder] = True\n        # Saving the diagnostics DataFrame\n        DiagnosticsDf.write(dd_df, os.path.join(self.root_dir, DIAGNOSTICS_DIR, \"import_experiments.csv\"))\n\n    #####################################################################\n    #         BATCH PROCESSING WRAPPING EXPERIMENT METHODS\n    #####################################################################\n\n    def update_configs(self, default_configs_fp: str, overwrite: str) -&gt; None:\n        self._proc_scaff(Experiment.update_configs, default_configs_fp, overwrite)\n\n    def format_vid(self, overwrite: bool) -&gt; None:\n        self._proc_scaff(Experiment.format_vid, overwrite)\n\n    def get_vid_metadata(self) -&gt; None:\n        self._proc_scaff(Experiment.get_vid_metadata)\n\n    def run_dlc(self, gputouse: int | None = None, overwrite: bool = False) -&gt; None:\n        \"\"\"\n        Batch processing corresponding to\n        [behavysis.pipeline.experiment.Experiment.run_dlc][]\n\n        Uses a multiprocessing pool to run DLC on each batch of experiments with each GPU\n        natively as batch in the same spawned subprocess (a DLC subprocess is spawned).\n        This is a slight tweak from the regular method of running\n        each experiment separately with multiprocessing.\n        \"\"\"\n        # TODO: implement diagnostics\n        # TODO: implement error handling\n        # If gputouse is not specified, using all GPUs\n        gputouse_ls = get_gpu_ids() if gputouse is None else [gputouse]\n        nprocs = len(gputouse_ls)\n        # Getting the experiments to run DLC on\n        exp_ls = self.experiments\n        # If overwrite is False, filtering for only experiments that need processing\n        if not overwrite:\n            exp_ls = [exp for exp in exp_ls if not os.path.isfile(exp.get_fp(Folders.KEYPOINTS.value))]\n        # Running DLC on each batch of experiments with each GPU (given allocated GPU ID)\n        exp_batches_ls = np.array_split(np.array(exp_ls), nprocs)\n        # Starting a dask cluster\n        with cluster_process(LocalCluster(n_workers=nprocs, threads_per_worker=1)):\n            # Preparing all experiments for execution\n            f_d_ls = [\n                dask.delayed(RunDLC.ma_dlc_run_batch)(  # type: ignore\n                    vid_fp_ls=[exp.get_fp(Folders.FORMATTED_VID.value) for exp in exp_batch],\n                    keypoints_dir=os.path.join(self.root_dir, Folders.KEYPOINTS.value),\n                    configs_dir=os.path.join(self.root_dir, Folders.CONFIGS.value),\n                    gputouse=gputouse,\n                    overwrite=overwrite,\n                )\n                for gputouse, exp_batch in zip(gputouse_ls, exp_batches_ls)\n            ]\n            # Executing in parallel\n            list(dask.compute(*f_d_ls))  # type: ignore\n\n    def calculate_parameters(self, funcs: tuple[Callable, ...]) -&gt; None:\n        self._proc_scaff(Experiment.calculate_parameters, funcs)\n\n    def collate_auto_configs(self) -&gt; None:\n        # Saving the auto fields of the configs of all experiments in the diagnostics folder\n        self._proc_scaff(Experiment.collate_auto_configs)\n        f_name = Experiment.collate_auto_configs.__name__\n        auto_configs_df = DiagnosticsDf.read(os.path.join(self.root_dir, DIAGNOSTICS_DIR, f\"{f_name}.csv\"))\n        # Making and saving histogram plots of the numerical auto fields\n        # NOTE: NOT including string frequencies, only numerical\n        auto_configs_df = auto_configs_df.loc[:, auto_configs_df.apply(pd.api.types.is_numeric_dtype)]\n        g = sns.FacetGrid(\n            data=auto_configs_df.fillna(-1).melt(var_name=\"measure\", value_name=\"value\"),\n            col=\"measure\",\n            sharex=False,\n            col_wrap=4,\n        )\n        g.map(sns.histplot, \"value\", bins=10)\n        g.set_titles(\"{col_name}\")\n        g.savefig(os.path.join(self.root_dir, DIAGNOSTICS_DIR, \"collate_auto_configs.png\"))\n        g.figure.clf()\n\n    def preprocess(self, funcs: tuple[Callable, ...], overwrite: bool) -&gt; None:\n        self._proc_scaff(Experiment.preprocess, funcs, overwrite)\n\n    def extract_features(self, overwrite: bool) -&gt; None:\n        self._proc_scaff(Experiment.extract_features, overwrite)\n\n    def classify_behavs(self, overwrite: bool) -&gt; None:\n        # TODO: IO error with multiprocessing. Using single processing for now.\n        nprocs = self.nprocs\n        self.nprocs = 1\n        self._proc_scaff(Experiment.classify_behavs, overwrite)\n        self.nprocs = nprocs\n\n    def export_behavs(self, overwrite: bool) -&gt; None:\n        self._proc_scaff(Experiment.export_behavs, overwrite)\n\n    def analyse(self, funcs: tuple[Callable, ...]) -&gt; None:\n        self._proc_scaff(Experiment.analyse, funcs)\n\n    def analyse_behavs(self) -&gt; None:\n        self._proc_scaff(Experiment.analyse_behavs)\n\n    def combine_analysis(self) -&gt; None:\n        self._proc_scaff(Experiment.combine_analysis)\n\n    def evaluate_vid(self, overwrite: bool) -&gt; None:\n        # TODO: IO error with multiprocessing. Using single processing for now.\n        # nprocs = self.nprocs\n        # self.nprocs = 1\n        self._proc_scaff(Experiment.evaluate_vid, overwrite)\n        # self.nprocs = nprocs\n\n    @functools.wraps(Experiment.export2csv)\n    def export2csv(self, src_dir: str, dst_dir: str, overwrite: bool) -&gt; None:\n        self._proc_scaff(Experiment.export2csv, src_dir, dst_dir, overwrite)\n\n    #####################################################################\n    #            COMBINING ANALYSIS DATA ACROSS EXPS METHODS\n    #####################################################################\n\n    def collate_analysis(self) -&gt; None:\n        \"\"\"\n        Combines an analysis of all the experiments together to generate combined files for:\n        - Each binned data. The index is (bin) and columns are (expName, indiv, measure).\n        - The summary data. The index is (expName, indiv, measure) and columns are\n        (statistics -e.g., mean).\n        \"\"\"\n        # TODO: fix up\n        self._analyse_collate_binned()\n        self._analyse_collate_summary()\n\n    def _analyse_collate_binned(self) -&gt; None:\n        \"\"\"\n        Combines an analysis of all the experiments together to generate combined h5 files for:\n        - Each binned data. The index is (bin) and columns are (expName, indiv, measure).\n        \"\"\"\n        # Initialising the process and logging description\n        description = \"Combining binned analysis\"\n        self.logger.info(\"%s...\", description)\n        # AGGREGATING BINNED DATA\n        # NOTE: need a more robust way of getting the list of bin sizes\n        proj_analyse_dir = os.path.join(self.root_dir, ANALYSIS_DIR)\n        configs = ExperimentConfigs.read_json(self.experiments[0].get_fp(Folders.CONFIGS.value))\n        bin_sizes_sec = configs.get_ref(configs.user.analyse.bins_sec)\n        bin_sizes_sec = np.append(bin_sizes_sec, \"custom\")\n        # Searching through all the analysis subdir\n        for analyse_subdir in os.listdir(proj_analyse_dir):\n            for bin_i in bin_sizes_sec:\n                df_ls = []\n                names_ls = []\n                for exp in self.experiments:\n                    in_fp = os.path.join(\n                        proj_analyse_dir, analyse_subdir, f\"binned_{bin_i}\", f\"{exp.name}.{AnalysisBinnedDf.IO}\"\n                    )\n                    if os.path.isfile(in_fp):\n                        df_ls.append(AnalysisBinnedDf.read(in_fp))\n                        names_ls.append(exp.name)\n                # Concatenating total_df with df across columns, with experiment name to column MultiIndex\n                if len(df_ls) &gt; 0:\n                    df = pd.concat(df_ls, keys=names_ls, names=[\"experiment\"], axis=1)\n                    df = df.fillna(0)\n                    AnalysisBinnedCollatedDf.write(\n                        df,\n                        os.path.join(\n                            proj_analyse_dir, analyse_subdir, f\"__ALL_binned_{bin_i}.{AnalysisBinnedCollatedDf.IO}\"\n                        ),\n                    )\n                    AnalysisBinnedCollatedDf.write_csv(\n                        df, os.path.join(proj_analyse_dir, analyse_subdir, f\"__ALL_binned_{bin_i}.csv\")\n                    )\n\n    def _analyse_collate_summary(self) -&gt; None:\n        \"\"\"\n        Combines an analysis of all the experiments together to generate combined h5 files for:\n        - The summary data. The index is (expName, indiv, measure) and columns are\n        (statistics -e.g., mean).\n        \"\"\"\n        # Initialising the process and logging description\n        description = \"Combining summary analysis\"\n        self.logger.info(\"%s...\", description)\n        # AGGREGATING SUMMARY DATA\n        proj_analyse_dir = os.path.join(self.root_dir, ANALYSIS_DIR)\n        # Searching through all the analysis subdir\n        for analyse_subdir in os.listdir(proj_analyse_dir):\n            df_ls = []\n            names_ls = []\n            for exp in self.experiments:\n                in_fp = os.path.join(proj_analyse_dir, analyse_subdir, \"summary\", f\"{exp.name}.{AnalysisSummaryDf.IO}\")\n                if os.path.isfile(in_fp):\n                    # Reading exp summary df\n                    df_ls.append(AnalysisSummaryDf.read(in_fp))\n                    names_ls.append(exp.name)\n            # Concatenating total_df with df across columns, with experiment name to column MultiIndex\n            if len(df_ls) &gt; 0:\n                df = pd.concat(df_ls, keys=names_ls, names=[\"experiment\"], axis=0)\n                df = df.fillna(0)\n                AnalysisSummaryCollatedDf.write(\n                    df, os.path.join(proj_analyse_dir, analyse_subdir, f\"__ALL_summary.{AnalysisSummaryCollatedDf.IO}\")\n                )\n                AnalysisSummaryCollatedDf.write_csv(\n                    df, os.path.join(proj_analyse_dir, analyse_subdir, \"__ALL_summary.csv\")\n                )\n</code></pre>"},{"location":"reference/project.html#behavysis.pipeline.project.Project.experiments","title":"<code>experiments</code>  <code>property</code>","text":"<p>Gets the ordered list of Experiment instances in the Project.</p> <p>Returns:</p> Type Description <code>list[Experiment]</code> <p>The list of all Experiment instances stored in the Project instance.</p>"},{"location":"reference/project.html#behavysis.pipeline.project.Project.__init__","title":"<code>__init__(root_dir)</code>","text":"<p>Make a Project instance.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>The filepath of the project directory. Can be relative to current dir or absolute dir.</p> required Source code in <code>behavysis/pipeline/project.py</code> <pre><code>def __init__(self, root_dir: str) -&gt; None:\n    \"\"\"\n    Make a Project instance.\n\n    Parameters\n    ----------\n    root_dir : str\n        The filepath of the project directory. Can be relative to\n        current dir or absolute dir.\n    \"\"\"\n    # Assertion: project directory must exist\n    if not os.path.isdir(root_dir):\n        raise ValueError(\n            f'Error: The folder, \"{root_dir}\" does not exist.\\n'\n            \"Please specify a folder that exists. Ensure you have the correct\"\n            \"forward-slashes or back-slashes for the path name.\"\n        )\n    self.root_dir = os.path.abspath(root_dir)\n    self._experiments = {}\n    self.nprocs = 4\n</code></pre>"},{"location":"reference/project.html#behavysis.pipeline.project.Project._analyse_collate_binned","title":"<code>_analyse_collate_binned()</code>","text":"<p>Combines an analysis of all the experiments together to generate combined h5 files for: - Each binned data. The index is (bin) and columns are (expName, indiv, measure).</p> Source code in <code>behavysis/pipeline/project.py</code> <pre><code>def _analyse_collate_binned(self) -&gt; None:\n    \"\"\"\n    Combines an analysis of all the experiments together to generate combined h5 files for:\n    - Each binned data. The index is (bin) and columns are (expName, indiv, measure).\n    \"\"\"\n    # Initialising the process and logging description\n    description = \"Combining binned analysis\"\n    self.logger.info(\"%s...\", description)\n    # AGGREGATING BINNED DATA\n    # NOTE: need a more robust way of getting the list of bin sizes\n    proj_analyse_dir = os.path.join(self.root_dir, ANALYSIS_DIR)\n    configs = ExperimentConfigs.read_json(self.experiments[0].get_fp(Folders.CONFIGS.value))\n    bin_sizes_sec = configs.get_ref(configs.user.analyse.bins_sec)\n    bin_sizes_sec = np.append(bin_sizes_sec, \"custom\")\n    # Searching through all the analysis subdir\n    for analyse_subdir in os.listdir(proj_analyse_dir):\n        for bin_i in bin_sizes_sec:\n            df_ls = []\n            names_ls = []\n            for exp in self.experiments:\n                in_fp = os.path.join(\n                    proj_analyse_dir, analyse_subdir, f\"binned_{bin_i}\", f\"{exp.name}.{AnalysisBinnedDf.IO}\"\n                )\n                if os.path.isfile(in_fp):\n                    df_ls.append(AnalysisBinnedDf.read(in_fp))\n                    names_ls.append(exp.name)\n            # Concatenating total_df with df across columns, with experiment name to column MultiIndex\n            if len(df_ls) &gt; 0:\n                df = pd.concat(df_ls, keys=names_ls, names=[\"experiment\"], axis=1)\n                df = df.fillna(0)\n                AnalysisBinnedCollatedDf.write(\n                    df,\n                    os.path.join(\n                        proj_analyse_dir, analyse_subdir, f\"__ALL_binned_{bin_i}.{AnalysisBinnedCollatedDf.IO}\"\n                    ),\n                )\n                AnalysisBinnedCollatedDf.write_csv(\n                    df, os.path.join(proj_analyse_dir, analyse_subdir, f\"__ALL_binned_{bin_i}.csv\")\n                )\n</code></pre>"},{"location":"reference/project.html#behavysis.pipeline.project.Project._analyse_collate_summary","title":"<code>_analyse_collate_summary()</code>","text":"<p>Combines an analysis of all the experiments together to generate combined h5 files for: - The summary data. The index is (expName, indiv, measure) and columns are (statistics -e.g., mean).</p> Source code in <code>behavysis/pipeline/project.py</code> <pre><code>def _analyse_collate_summary(self) -&gt; None:\n    \"\"\"\n    Combines an analysis of all the experiments together to generate combined h5 files for:\n    - The summary data. The index is (expName, indiv, measure) and columns are\n    (statistics -e.g., mean).\n    \"\"\"\n    # Initialising the process and logging description\n    description = \"Combining summary analysis\"\n    self.logger.info(\"%s...\", description)\n    # AGGREGATING SUMMARY DATA\n    proj_analyse_dir = os.path.join(self.root_dir, ANALYSIS_DIR)\n    # Searching through all the analysis subdir\n    for analyse_subdir in os.listdir(proj_analyse_dir):\n        df_ls = []\n        names_ls = []\n        for exp in self.experiments:\n            in_fp = os.path.join(proj_analyse_dir, analyse_subdir, \"summary\", f\"{exp.name}.{AnalysisSummaryDf.IO}\")\n            if os.path.isfile(in_fp):\n                # Reading exp summary df\n                df_ls.append(AnalysisSummaryDf.read(in_fp))\n                names_ls.append(exp.name)\n        # Concatenating total_df with df across columns, with experiment name to column MultiIndex\n        if len(df_ls) &gt; 0:\n            df = pd.concat(df_ls, keys=names_ls, names=[\"experiment\"], axis=0)\n            df = df.fillna(0)\n            AnalysisSummaryCollatedDf.write(\n                df, os.path.join(proj_analyse_dir, analyse_subdir, f\"__ALL_summary.{AnalysisSummaryCollatedDf.IO}\")\n            )\n            AnalysisSummaryCollatedDf.write_csv(\n                df, os.path.join(proj_analyse_dir, analyse_subdir, \"__ALL_summary.csv\")\n            )\n</code></pre>"},{"location":"reference/project.html#behavysis.pipeline.project.Project._proc_scaff","title":"<code>_proc_scaff(method, *args, **kwargs)</code>","text":"<p>Runs the given method on all experiments in the project.</p> Source code in <code>behavysis/pipeline/project.py</code> <pre><code>def _proc_scaff(self, method: Callable, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Runs the given method on all experiments in the project.\n    \"\"\"\n    # Choosing whether to run the scaffold function in single or multi-processing mode\n    if self.nprocs == 1:\n        scaffold_func = self._proc_scaff_sp\n    else:\n        scaffold_func = self._proc_scaff_mp\n    # Running the scaffold function\n    # Starting\n    self.logger.info(f\"Running {method.__name__} for all experiments.\")\n    # Running\n    dd_ls = scaffold_func(method, *args, **kwargs)\n    if len(dd_ls) &gt; 0:\n        # Processing all experiments\n        df = DiagnosticsDf.init_from_dd_ls(dd_ls)\n        # Updating the diagnostics file at each step\n        DiagnosticsDf.write(df, os.path.join(self.root_dir, DIAGNOSTICS_DIR, f\"{method.__name__}.csv\"))\n        # Finishing\n        self.logger.info(f\"Finished running {method.__name__} for all experiments\")\n</code></pre>"},{"location":"reference/project.html#behavysis.pipeline.project.Project._proc_scaff_mp","title":"<code>_proc_scaff_mp(method, *args, **kwargs)</code>","text":"<p>Processes an experiment with the given <code>Experiment</code> method and records the diagnostics of the process in a MULTI-PROCESSING way.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Callable</code> <p>The <code>Experiment</code> class method to run.</p> required Notes <p>Can call any <code>Experiment</code> methods instance. Effectively, <code>method</code> gets called with: <pre><code>exp is a Experiment instance\nmethod(exp, *args, **kwargs)\n</code></pre></p> Source code in <code>behavysis/pipeline/project.py</code> <pre><code>def _proc_scaff_mp(self, method: Callable, *args: Any, **kwargs: Any) -&gt; list[dict]:\n    \"\"\"\n    Processes an experiment with the given `Experiment` method and records\n    the diagnostics of the process in a MULTI-PROCESSING way.\n\n    Parameters\n    ----------\n    method : Callable\n        The `Experiment` class method to run.\n\n    Notes\n    -----\n    Can call any `Experiment` methods instance.\n    Effectively, `method` gets called with:\n    ```\n    exp is a Experiment instance\n    method(exp, *args, **kwargs)\n    ```\n    \"\"\"\n    # Starting a dask cluster\n    with cluster_process(LocalCluster(n_workers=self.nprocs, threads_per_worker=1)):\n        # Preparing all experiments for execution\n        f_d_ls = [dask.delayed(method)(exp, *args, **kwargs) for exp in self.experiments]  # type: ignore\n        # Executing in parallel\n        dd_ls = list(dask.compute(*f_d_ls))  # type: ignore\n    return dd_ls\n</code></pre>"},{"location":"reference/project.html#behavysis.pipeline.project.Project._proc_scaff_sp","title":"<code>_proc_scaff_sp(method, *args, **kwargs)</code>","text":"<p>Processes an experiment with the given <code>Experiment</code> method and records the diagnostics of the process in a SINGLE-PROCESSING way.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Callable</code> <p>The experiment <code>Experiment</code> class method to run.</p> required Notes <p>Can call any <code>Experiment</code> instance method. Effectively, <code>method</code> gets called with: <pre><code>exp is a Experiment instance\nmethod(exp, *args, **kwargs)\n</code></pre></p> Source code in <code>behavysis/pipeline/project.py</code> <pre><code>def _proc_scaff_sp(self, method: Callable, *args: Any, **kwargs: Any) -&gt; list[dict]:\n    \"\"\"\n    Processes an experiment with the given `Experiment` method and records\n    the diagnostics of the process in a SINGLE-PROCESSING way.\n\n    Parameters\n    ----------\n    method : Callable\n        The experiment `Experiment` class method to run.\n\n    Notes\n    -----\n    Can call any `Experiment` instance method.\n    Effectively, `method` gets called with:\n    ```\n    exp is a Experiment instance\n    method(exp, *args, **kwargs)\n    ```\n    \"\"\"\n    # Processing all experiments and storing process outcomes as list of dicts\n    return [method(exp, *args, **kwargs) for exp in self.experiments]\n</code></pre>"},{"location":"reference/project.html#behavysis.pipeline.project.Project.collate_analysis","title":"<code>collate_analysis()</code>","text":"<p>Combines an analysis of all the experiments together to generate combined files for: - Each binned data. The index is (bin) and columns are (expName, indiv, measure). - The summary data. The index is (expName, indiv, measure) and columns are (statistics -e.g., mean).</p> Source code in <code>behavysis/pipeline/project.py</code> <pre><code>def collate_analysis(self) -&gt; None:\n    \"\"\"\n    Combines an analysis of all the experiments together to generate combined files for:\n    - Each binned data. The index is (bin) and columns are (expName, indiv, measure).\n    - The summary data. The index is (expName, indiv, measure) and columns are\n    (statistics -e.g., mean).\n    \"\"\"\n    # TODO: fix up\n    self._analyse_collate_binned()\n    self._analyse_collate_summary()\n</code></pre>"},{"location":"reference/project.html#behavysis.pipeline.project.Project.get_experiment","title":"<code>get_experiment(name)</code>","text":"<p>Gets the experiment with the given name</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The experiment name.</p> required <p>Returns:</p> Type Description <code>Experiment</code> <p>The experiment.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Experiment with the given name does not exist.</p> Source code in <code>behavysis/pipeline/project.py</code> <pre><code>def get_experiment(self, name: str) -&gt; Experiment:\n    \"\"\"\n    Gets the experiment with the given name\n\n    Parameters\n    ----------\n    name : str\n        The experiment name.\n\n    Returns\n    -------\n    Experiment\n        The experiment.\n\n    Raises\n    ------\n    ValueError\n        Experiment with the given name does not exist.\n    \"\"\"\n    if name in self._experiments:\n        return self._experiments[name]\n    raise ValueError(f'Experiment with the name \"{name}\" does not exist in the project.')\n</code></pre>"},{"location":"reference/project.html#behavysis.pipeline.project.Project.import_experiment","title":"<code>import_experiment(name)</code>","text":"<p>Adds an experiment with the given name to the .experiments dict. The key of this experiment in the <code>self.experiments</code> dict is \"dir/name\". If the experiment already exists in the project, it is not added.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The experiment name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the experiment was imported or not. True if imported, False if not.</p> Source code in <code>behavysis/pipeline/project.py</code> <pre><code>def import_experiment(self, name: str) -&gt; bool:\n    \"\"\"\n    Adds an experiment with the given name to the .experiments dict.\n    The key of this experiment in the `self.experiments` dict is \"dir/name\".\n    If the experiment already exists in the project, it is not added.\n\n    Parameters\n    ----------\n    name : str\n        The experiment name.\n\n    Returns\n    -------\n    bool\n        Whether the experiment was imported or not.\n        True if imported, False if not.\n    \"\"\"\n    if name not in self._experiments:\n        self._experiments[name] = Experiment(name, self.root_dir)\n        return True\n    return False\n</code></pre>"},{"location":"reference/project.html#behavysis.pipeline.project.Project.import_experiments","title":"<code>import_experiments()</code>","text":"<p>Add all experiments in the project folder to the experiments dict. The key of each experiment in the .experiments dict is \"name\". Refer to Project.addExperiment() for details about how each experiment is added.</p> Source code in <code>behavysis/pipeline/project.py</code> <pre><code>def import_experiments(self) -&gt; None:\n    \"\"\"\n    Add all experiments in the project folder to the experiments dict.\n    The key of each experiment in the .experiments dict is \"name\".\n    Refer to Project.addExperiment() for details about how each experiment is added.\n    \"\"\"\n    self.logger.info(f\"Searching project folder: {self.root_dir}\")\n    # Storing file existences in {folder1: [file1, file2, ...], ...} format\n    dd_dict = {}\n    # Adding all experiments within given project dir\n    for f in Folders:\n        folder = os.path.join(self.root_dir, f.value)\n        dd_dict[f.value] = []\n        # If folder does not exist, skip\n        if not os.path.isdir(folder):\n            continue\n        # For each file in the folder\n        for fp_name in natsorted(os.listdir(folder)):\n            if re.search(r\"^\\.\", fp_name):  # do not add hidden files\n                continue\n            name = get_name(fp_name)\n            try:\n                self.import_experiment(name)\n                dd_dict[f.value].append(name)\n            except ValueError as e:  # do not add invalid files\n                self.logger.info(f\"failed: {f.value}    --    {fp_name}: {e}\")\n    # Logging outcome of imported and failed experiments\n    exp_ls_msg = \"\".join([f\"\\n    - {exp.name}\" for exp in self.experiments])\n    self.logger.info(f\"Experiments imported:{exp_ls_msg}\")\n    # Constructing dd_df from dd_dict\n    dd_df = DiagnosticsDf.init_df(pd.Series(np.unique(np.concatenate(list(dd_dict.values())))))\n    # Setting each (experiment, folder) pair to True if the file exists\n    for folder in dd_dict:\n        dd_df[folder] = False\n        for exp_name in dd_dict[folder]:\n            dd_df.loc[exp_name, folder] = True\n    # Saving the diagnostics DataFrame\n    DiagnosticsDf.write(dd_df, os.path.join(self.root_dir, DIAGNOSTICS_DIR, \"import_experiments.csv\"))\n</code></pre>"},{"location":"reference/project.html#behavysis.pipeline.project.Project.run_dlc","title":"<code>run_dlc(gputouse=None, overwrite=False)</code>","text":"<p>Batch processing corresponding to behavysis.pipeline.experiment.Experiment.run_dlc</p> <p>Uses a multiprocessing pool to run DLC on each batch of experiments with each GPU natively as batch in the same spawned subprocess (a DLC subprocess is spawned). This is a slight tweak from the regular method of running each experiment separately with multiprocessing.</p> Source code in <code>behavysis/pipeline/project.py</code> <pre><code>def run_dlc(self, gputouse: int | None = None, overwrite: bool = False) -&gt; None:\n    \"\"\"\n    Batch processing corresponding to\n    [behavysis.pipeline.experiment.Experiment.run_dlc][]\n\n    Uses a multiprocessing pool to run DLC on each batch of experiments with each GPU\n    natively as batch in the same spawned subprocess (a DLC subprocess is spawned).\n    This is a slight tweak from the regular method of running\n    each experiment separately with multiprocessing.\n    \"\"\"\n    # TODO: implement diagnostics\n    # TODO: implement error handling\n    # If gputouse is not specified, using all GPUs\n    gputouse_ls = get_gpu_ids() if gputouse is None else [gputouse]\n    nprocs = len(gputouse_ls)\n    # Getting the experiments to run DLC on\n    exp_ls = self.experiments\n    # If overwrite is False, filtering for only experiments that need processing\n    if not overwrite:\n        exp_ls = [exp for exp in exp_ls if not os.path.isfile(exp.get_fp(Folders.KEYPOINTS.value))]\n    # Running DLC on each batch of experiments with each GPU (given allocated GPU ID)\n    exp_batches_ls = np.array_split(np.array(exp_ls), nprocs)\n    # Starting a dask cluster\n    with cluster_process(LocalCluster(n_workers=nprocs, threads_per_worker=1)):\n        # Preparing all experiments for execution\n        f_d_ls = [\n            dask.delayed(RunDLC.ma_dlc_run_batch)(  # type: ignore\n                vid_fp_ls=[exp.get_fp(Folders.FORMATTED_VID.value) for exp in exp_batch],\n                keypoints_dir=os.path.join(self.root_dir, Folders.KEYPOINTS.value),\n                configs_dir=os.path.join(self.root_dir, Folders.CONFIGS.value),\n                gputouse=gputouse,\n                overwrite=overwrite,\n            )\n            for gputouse, exp_batch in zip(gputouse_ls, exp_batches_ls)\n        ]\n        # Executing in parallel\n        list(dask.compute(*f_d_ls))  # type: ignore\n</code></pre>"},{"location":"tutorials/configs_json.html","title":"Configs JSON File","text":"<p>A configs JSON file is attached to each experiment. This file defines a) how the experiment should be processed (e.g. hyperparameters like the <code>dlc_config_fp</code> to use), and b) the inherent parameters of the experiment (e.g. like the <code>px/mm</code> and <code>start_frame</code> calculations).</p> <p>An example configs file is shown below:</p> <pre><code>{\n  \"user\": {\n    \"format_vid\": {\n      \"height_px\": 540,\n      \"width_px\": 960,\n      \"fps\": 15,\n      \"start_sec\": null,\n      \"stop_sec\": null\n    },\n    \"run_dlc\": {\n      \"model_fp\": \"/path/to/dlc_config.yaml\"\n    },\n    \"calculate_params\": {\n      \"start_frame\": {\n        \"window_sec\": 1,\n        \"pcutoff\": 0.9,\n        \"bodyparts\": \"--bodyparts-simba\"\n      },\n      \"exp_dur\": {\n        \"window_sec\": 1,\n        \"pcutoff\": 0.9,\n        \"bodyparts\": \"--bodyparts-simba\"\n      },\n      \"stop_frame\": {\n        \"dur_sec\": 6000\n      },\n      \"px_per_mm\": {\n        \"pt_a\": \"--tl\",\n        \"pt_b\": \"--tr\",\n        \"dist_mm\": 400\n      }\n    },\n    \"preprocess\": {\n      \"interpolate\": {\n        \"pcutoff\": 0.5\n      },\n      \"bodycentre\": {\n        \"bodyparts\": \"--bodyparts-centre\"\n      },\n      \"refine_ids\": {\n        \"marked\": \"mouse1marked\",\n        \"unmarked\": \"mouse2unmarked\",\n        \"marking\": \"AnimalColourMark\",\n        \"window_sec\": 0.5,\n        \"metric\": \"rolling\",\n        \"bodyparts\": \"--bodyparts-centre\"\n      }\n    },\n    \"evaluate\": {\n      \"keypoints_plot\": {\n        \"bodyparts\": [\"Nose\", \"BodyCentre\", \"TailBase1\"]\n      },\n      \"eval_vid\": {\n        \"funcs\": [\"keypoints\", \"behavs\"],\n        \"pcutoff\": 0.5,\n        \"colour_level\": \"individuals\",\n        \"radius\": 4,\n        \"cmap\": \"rainbow\"\n      }\n    },\n    \"extract_features\": {\n      \"individuals\": [\"mouse1marked\", \"mouse2unmarked\"],\n      \"bodyparts\": \"--bodyparts-simba\"\n    },\n    \"classify_behaviours\": [\n      {\n        \"model_fp\": \"/path/to/behav_model_1.json\",\n        \"pcutoff\": null,\n        \"min_window_frames\": \"--min_window_frames\",\n        \"user_behavs\": \"--user_behavs\"\n      },\n      {\n        \"model_fp\": \"/path/to/behav_model_2.json\",\n        \"pcutoff\": null,\n        \"min_window_frames\": \"--min_window_frames\",\n        \"user_behavs\": \"--user_behavs\"\n      }\n    ],\n    \"analyse\": {\n      \"thigmotaxis\": {\n        \"thresh_mm\": 50,\n        \"roi_top_left\": \"--tl\",\n        \"roi_top_right\": \"--tr\",\n        \"roi_bottom_left\": \"--bl\",\n        \"roi_bottom_right\": \"--br\",\n        \"bodyparts\": \"--bodyparts-centre\"\n      },\n      \"center_crossing\": {\n        \"thresh_mm\": 125,\n        \"roi_top_left\": \"--tl\",\n        \"roi_top_right\": \"--tr\",\n        \"roi_bottom_left\": \"--bl\",\n        \"roi_bottom_right\": \"--br\",\n        \"bodyparts\": \"--bodyparts-centre\"\n      },\n      \"in_roi\": {\n        \"thresh_mm\": 5,\n        \"roi_top_left\": \"--tl\",\n        \"roi_top_right\": \"--tr\",\n        \"roi_bottom_left\": \"--bl\",\n        \"roi_bottom_right\": \"--br\",\n        \"bodyparts\": [\"Nose\"]\n      },\n      \"speed\": {\n        \"smoothing_sec\": 1,\n        \"bodyparts\": \"--bodyparts-centre\"\n      },\n      \"social_distance\": {\n        \"smoothing_sec\": 1,\n        \"bodyparts\": \"--bodyparts-centre\"\n      },\n      \"freezing\": {\n        \"window_sec\": 2,\n        \"thresh_mm\": 5,\n        \"smoothing_sec\": 0.2,\n        \"bodyparts\": \"--bodyparts-simba\"\n      },\n      \"bins_sec\": [30, 60, 120, 300],\n      \"custom_bins_sec\": [60, 120, 300, 600]\n    }\n  },\n  \"ref\": {\n    \"bodyparts-centre\": [\n      \"LeftFlankMid\",\n      \"BodyCentre\",\n      \"RightFlankMid\",\n      \"LeftFlankRear\",\n      \"RightFlankRear\",\n      \"TailBase1\"\n    ],\n    \"bodyparts-simba\": [\n      \"LeftEar\",\n      \"RightEar\",\n      \"Nose\",\n      \"BodyCentre\",\n      \"LeftFlankMid\",\n      \"RightFlankMid\",\n      \"TailBase1\",\n      \"TailTip4\"\n    ],\n    \"tl\": \"TopLeft\",\n    \"tr\": \"TopRight\",\n    \"bl\": \"BottomLeft\",\n    \"br\": \"BottomRight\",\n    \"min_window_frames\": 2,\n    \"user_behavs\": [\"fight\", \"aggression\"]\n  }\n}\n</code></pre>"},{"location":"tutorials/configs_json.html#the-structure","title":"The Structure","text":"<p>The configs file has three main sections - <code>user</code>: User defined parameters to process the experiment. - <code>auto</code>: Automatically calculated parameters which are used     in later processes for the experiment.     Also gives useful insights into how the experiment \"went\" (e.g. over/under time, arena is smaller than other videos). - <code>ref</code>: User defined parameters can be referenced from keys defined here.     Useful when the same parameter values are used for many processes     (e.g. bodyparts).</p>"},{"location":"tutorials/configs_json.html#understanding-specific-parameters","title":"Understanding Specific Parameters","text":"<p>Notes</p> <p>To understand specific parameters in the <code>configs.yaml</code>, see each processing function's API documentation.</p> <p>For example, <code>user.calculate_params.px_per_mm</code> requires <code>pt_a</code>, <code>pt_b</code>, and <code>dist_mm</code>, which are described in the API docs.</p>"},{"location":"tutorials/configs_json.html#the-ref-section","title":"The Ref section","text":"<p>The <code>ref</code> section defines values that can be referenced in the <code>user</code> section.</p> <p>To reference a value from the ref section, first define it:</p> <pre><code>{\n    ...\n    \"ref\": {\n        \"example\": [\"values\", \"of\", \"any\", \"type\"]\n    }\n}\n</code></pre> <p>You can now reference <code>example</code> by prepending a double hyphen (<code>--</code>) when referencing it:</p> <pre><code>{\n    \"user\": {\n        ...\n        \"parameter\": \"--example\",\n        ...\n    },\n    ...\n}\n</code></pre>"},{"location":"tutorials/configs_json.html#setting-the-configs-for-an-experiment-or-all-experiments-in-a-project","title":"Setting the Configs for an Experiment or all Experiments in a Project","text":"<p>Each experiment requires a corresponding configs file.</p> <p>To generate or modify an experiment's configs file, first make a <code>default.json</code> file with the configs configured as you'd like.</p> <p>Tip</p> <p>You can copy the example configs file from here.</p> <p>Just make sure to change the multiple <code>model_fp</code> filepaths.</p> <pre><code>from behavysis import Experiment\n\n# Getting the experiment\nexperiment = Experiment(\"exp_name\", \"root_dir\")\n# Making/overwriting the configs file\nexperiment.update_configs(\"/path/to/default.json\", overwrite=\"all\")\n</code></pre> <p>Note</p> <p>The <code>overwrite</code> keyword can be <code>\"all\"</code>, or <code>\"user\"</code>.</p>"},{"location":"tutorials/diagnostics_messages.html","title":"Diagnostics Messages","text":""},{"location":"tutorials/diagnostics_messages.html#the-diagnostics-outputs","title":"The Diagnostics Outputs","text":"<p>After any process is run, a diagnostics file with the name of the process is generated in the diagnostics folder.</p> <p>An example of what this can look like is shown below.</p> <p></p>"},{"location":"tutorials/diagnostics_messages.html#common-errors-and-warning","title":"Common Errors and Warning","text":"<p>In the diagnostics file, an error is something that has caused the processing of the experiment to fail completely. A warning is something that the program detected as unusual - it won't cause the program to fail but it is worth noting because it may affect future processes.</p> <p>Common warnings and errors that may arise are shown below, grouped by the process they usually occur in:</p> <ul> <li>updateConfigFiles</li> <li>The user-given value for <code>config_fp</code> may be an incorrect filepath or that cconfig file itself may not be in a valid JSON format (e.g., a bracket or a comma might be missing). You can use one of the configs templates.</li> <li>formatVideos</li> <li>The raw mp4 file may be missing or corrupted. Please try to open this file to see if this is the case.</li> <li>Sometimes, the resolution is an uncommon value and the downsampling fails. When this happens, the video is instead copied and the diagnostics file notes that the video failed to downsample and was copied instead.</li> <li>runDLC</li> <li>The specified dlc_config_path may be missing or incorrect. Please check that the correct filepath is used.</li> <li>calculateParams</li> <li>Throughout all the processes run within calculateParams, if any necessary parameter in the configs file are missing, then an error is added to the diagnostics file and the specific process stops. Future processes are usually affected by this error.</li> <li>getVideoMetadata<ul> <li>If the video file does not exist or is corrupted, then a warning is added the diagnostics file.</li> </ul> </li> <li>calcStartFrame<ul> <li>If the bodypart listed in the config file is not in the dlc_csv file, then a warning is thrown and the invalid bodypart is ignored.</li> <li>If the subject is not detected in any frames, then a warning is added to the diagnostics file. Please open and check the video to see if this is the case.</li> </ul> </li> <li>calcEndFrame<ul> <li>If the user-specified dur_sec (duration of the experiment in seconds) is larger than the length of the video, then a warning is added to the diagnostics file. Please open and check the video to see if this is the case.</li> </ul> </li> <li>calcPxPerMM<ul> <li>If the labels, \"TopLeft\", \"TopRight\", \"BottomLeft\", or \"BottomRight\", are missing from the dlc_csv file (i.e., if any of the box corners were not tracked), then an error message is added to the diagnostics file. Please ensure that you are using a DLC model that tracks the arena edges.</li> </ul> </li> <li>preprocessing</li> <li>Please note that some processes run within preprocessing depend on calculateParams. If there were any failed processes in calculateParams, please troubleshoot them before moving onto here.</li> <li>Throughout all the processes run within preprocessing, if any necessary parameter in the configs file are missing, then an error is added to the diagnostics file and the specific process stops. Future processes are usually affected by this error.</li> <li>InterpolatePoints:<ul> <li>If the bodypart listed in the config file is not in the dlc_csv file, then a warning is thrown and the invalid bodypart is ignored.</li> </ul> </li> <li>analysis</li> <li>Please note that most processes run within analysis depend on preprocessing. If there were any failed processes in calculateParams, please troubleshoot them before moving onto here.</li> <li>aggregateAnalysis</li> <li>The analysis process must be successfully run before the files generated from it can be aggregated.</li> </ul>"},{"location":"tutorials/explanation.html","title":"Explanation","text":"<p>The behavysis is used to analyse raw mp4 footage of lab mice. Analysis can include:</p> <ul> <li>Open Field</li> <li>Subject thigmotaxis</li> <li>Subject speed</li> <li>Choice (need to implement)</li> <li>etc.</li> </ul> <p>Converting raw mp4 footage to interpretable data and analysis involves the following steps:</p> <ol> <li>Setting up a BA project. This project will perform all the calculations to render analysises.</li> <li>Importing all the raw videos to into the project.</li> <li>Formatting all the raw videos so they can be interpreted by the DeepLabCut (DLC) pose estimation model.</li> <li>Run the formatted videos through the DLC pose estimation model to generate a video with the pose markers and a csv file of x-y coordinates of each marker (e.g., nose, left ear, right ear, body, front right foot, etc.).</li> <li>Preprocess the csv file of x-y marker coordinates so it is ready for analysis.</li> <li>Generate analysis from the preprocessed x-y marker coordinates file.</li> </ol> <p></p>"},{"location":"tutorials/explanation.html#making-a-project-to-analyse","title":"Making a Project to Analyse","text":"<p>The experiment files must be stored in the computer in a certain way so the program can analyse them. Files pertaining to each experiment must be stored in folders with specific names. The overall way that the files are structured in a project are shown below.</p> <p></p> <p>For more information about how to set up experiment files, see setup</p>"},{"location":"tutorials/setup.html","title":"Setup","text":"<p>Before running the behavysis analysises, the files that we want to analyse must be set up a certain way for the behavysis program to recognise them.</p> <p>There are three important guidelines to set up the project:</p> <ul> <li>Structure of files in folders .</li> <li>Experiment files.</li> <li>Config files for each experiment.</li> </ul>"},{"location":"tutorials/setup.html#folder-structure","title":"Folder Structure","text":"<p>They need to be set up inside specially named folders, as shown below.</p> <p>An example of how this would look on a computer (in this case, a Mac) is shown below.</p>"},{"location":"tutorials/setup.html#experiment-files","title":"Experiment Files","text":"<p>Each experiment must have files that have same name (not including the suffix like <code>.csv</code> or <code>.mp4</code>). An example is \"day1_experiment1\" must have all files named \"day1_experiment1.mp4\", \"day1_experiment1.csv\", \"day1_experiment1.json\" etc. stored in the corresponding folder.</p>"},{"location":"tutorials/setup.html#config-files","title":"Config Files","text":"<p>The config file for an experiment stores all the parameters for how the experiment was recorded (e.g., the frames per second of the raw video, the experiment duration, etc.), and the parameters for how we want to process the data (e.g., the intended frames per second to format the video to, the DLC model to use to analyse, the likeliness pcutoff to interpolate points, etc.)</p> <p>An example of a config file is shown here.</p>"},{"location":"tutorials/setup.html#running-behavysis","title":"Running behavysis","text":"<p>To install <code>behavysis</code>, follow these instructions.</p> <p>To run <code>behavysis</code>, follow these these instructions.</p>"}]}