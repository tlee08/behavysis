var __index = {
  config: {
    lang: ["en"],
    separator: "[\\s\\-]+",
    pipeline: ["stopWordFilter"],
  },
  docs: [
    {
      location: "index.html",
      title: "Home",
      text: "<p>This site contains the documentation for the <code>behavysis_pipeline</code> program.</p>",
    },
    {
      location: "examples/analysis.html",
      title: "Analysing a Folder of Experiments",
      text: "<p>All outcomes for experiment processing is stored in csv files in the <code>proj_dir/diagnostics</code> folder. These files store the outcome and process description (i.e. error explanations) of all experiments.</p>",
    },
    {
      location: "examples/analysis.html#loading-in-all-relevant-packages",
      title: "Loading in all relevant packages",
      text: "<pre><code>from behavysis_pipeline import Project\nfrom behavysis_pipeline.processes import *\n</code></pre>",
    },
    {
      location:
        "examples/analysis.html#making-the-project-and-importing-all-experiments",
      title: "Making the project and importing all experiments",
      text: '<p>The directory path of the project must be specified and must contain the experiment files you wish to analyse in a particular folder structure.</p> <p>For more information on how to structure a project directory, please see setup.</p> <p>For more information on how a <code>Experiment</code> works, please see behavysis_pipeline.pipeline.project.Project.</p> <pre><code># Defining the project\'s folder\nproj_dir = "./project"\n# Initialising the project\nproj = Project(proj_dir)\n# Importing all the experiments (from the project folder)\nproj.importExperiments()\n</code></pre>',
    },
    {
      location: "examples/analysis.html#checking-all-imported-experiments",
      title: "Checking all imported experiments",
      text: "<p>To see all imported experiments, see the <code>proj_dir/diagnostics/importExperiments.csv</code> file that has been generated.</p>",
    },
    {
      location:
        "examples/analysis.html#updating-the-configurations-for-all-experiments",
      title: "Updating the configurations for all experiments",
      text: '<p>If you would like the configurations (which are stored in config files) to be updated new parameters, define the JSON style of configuration parameters you would like to add and run the following lines.</p> <p>For more information about how a configurations file works, please see here.</p> <pre><code># Defining the default configs json path\nconfigs_fp = "path/to/default_configs.json"\n# Overwriting the configs\nproj.update_configs(\n    configs_fp,\n    overwrite="user",\n)\n</code></pre>',
    },
    {
      location: "examples/analysis.html#get-animal-keypoints-in-videos",
      title: "Get Animal Keypoints in Videos",
      text: "<p>The following code processes and analyses all experiments that have been imported into a project. This is similar to analysing a single experiment.</p>",
    },
    {
      location: "examples/analysis.html#downsample-videos",
      title: "Downsample videos",
      text: "<p>Formatting the raw mp4 videos so it can be fed through the DLC pose estimation algorithm.</p> <pre><code>proj.format_vid(\n    (\n        FormatVid.format_vid,\n        FormatVid.get_vid_metadata,\n    ),\n    overwrite=True,\n)\n</code></pre>",
    },
    {
      location: "examples/analysis.html#run-keypoints-detection-deeplabcut",
      title: "Run Keypoints detection (DeepLabCut)",
      text: "<p>Running the DLC pose estimation algorithm on the formatted mp4 files.</p> <p>Note</p> <p>Make sure to change the <code>user.run_dlc.model_fp</code> to the DeepLabCut model's config file you'd like to use.</p> <pre><code>proj.run_dlc(\n    gputouse=None,\n    overwrite=True,\n)\n</code></pre>",
    },
    {
      location:
        "examples/analysis.html#calculating-inherent-parameters-from-keypoints",
      title: "Calculating Inherent Parameters from Keypoints",
      text: "<p>Calculating relevant parameters to store in the <code>auto</code> section of the config file. The calculations performed are:</p> <pre><code>proj.calculate_params(\n    (\n        CalculateParams.start_frame,\n        CalculateParams.stop_frame,\n        CalculateParams.exp_dur,\n        CalculateParams.px_per_mm,\n    )\n)\n</code></pre> <p>And see a collation of all experiments' inherent parameters to spot any anomolies before continuing</p> <pre><code>proj.collate_configs_auto()\n</code></pre>",
    },
    {
      location: "examples/analysis.html#postprocessing",
      title: "Postprocessing",
      text: "<p>Preprocessing the DLC csv data and output the preprocessed data to a <code>preprocessed_csv.&lt;exp_name&gt;.csv</code> file. The preprocessings performed are:</p> <pre><code>proj.preprocess(\n    (\n        Preprocess.start_stop_trim,\n        Preprocess.interpolate,\n        Preprocess.refine_ids,\n    ),\n    overwrite=overwrite,\n)\n</code></pre>",
    },
    {
      location: "examples/analysis.html#make-simple-analysis",
      title: "Make Simple Analysis",
      text: "<p>Analysing the preprocessed csv data to extract useful analysis and results. The analyses performed are:</p> <pre><code>proj.analyse(\n    (\n        Analyse.thigmotaxis,\n        Analyse.center_crossing,\n        Analyse.in_roi,\n        Analyse.speed,\n        Analyse.social_distance,\n        Analyse.freezing,\n    )\n)\n</code></pre>",
    },
    {
      location: "examples/analysis.html#automated-behaviour-detection",
      title: "Automated Behaviour Detection",
      text: "",
    },
    {
      location: "examples/analysis.html#extracting-features",
      title: "Extracting Features",
      text: "<p>Extracting derivative features from keypoints. For example - speed, bounding ellipse size, distance between points, etc.</p> <pre><code>proj.extract_features(overwrite)\n</code></pre>",
    },
    {
      location: "examples/analysis.html#running-behaviour-classifiers",
      title: "Running Behaviour Classifiers",
      text: "<p>Note</p> <p>Make sure to change the <code>user.classify_behaviours</code> list to the behaviours classifiers you'd like to use.</p> <pre><code>proj.classify_behaviours(overwrite)\n</code></pre>",
    },
    {
      location:
        "examples/analysis.html#exporting-the-behaviour-detection-results",
      title: "Exporting the Behaviour Detection Results",
      text: "<p>Exports to such a format, where a) <code>behavysis_viewer</code> can load it and perform semi-automated analysis, and b) after semi-automated verification, can be used to make a new/improve a current behaviour classifier (with behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier)</p> <pre><code>proj.export_behaviours(overwrite)\n</code></pre>",
    },
    {
      location: "examples/analysis.html#analyse-behaviours",
      title: "Analyse Behaviours",
      text: "<p>Similar to simple analysis, which calculates each experiment's a) the overall summary, and b) binned summary.</p> <pre><code>proj.behav_analyse()\n</code></pre>",
    },
    {
      location: "examples/analysis.html#export-any-tables",
      title: "Export any Tables",
      text: '<p>Tables are stored as <code>.feather</code> files.</p> <p>To export these to csv files, run the following:</p> <pre><code>proj.export_feather("7_scored_behavs", "/path/to/csv_out")\n</code></pre>',
    },
    {
      location: "examples/analysis.html#evaluate",
      title: "Evaluate",
      text: "<p>Evaluates keypoints and behaviours accuracy by making annotated experiment videos.</p> <pre><code>proj.evaluate(\n    (\n        Evaluate.eval_vid,\n        Evaluate.keypoints_plot,\n    ),\n    overwrite=overwrite,\n)\n</code></pre>",
    },
    {
      location: "examples/train.html",
      title: "Training a Behaviour Classifier",
      text: "",
    },
    {
      location: "examples/train.html#loading-in-all-relevant-packages",
      title: "Loading in all relevant packages",
      text: '<pre><code>import os\n\nfrom behavysis_core.mixins.behav_mixin import BehavMixin\nfrom behavysis_pipeline.behav_classifier import BehavClassifier\nfrom behavysis_pipeline.behav_classifier.clf_templates import ClfTemplates\nfrom behavysis_pipeline.pipeline import Project\n\nif __name__ == "__main__":\n    root_dir = "."\n    overwrite = True\n\n    # Option 1: From BORIS\n    # Define behaviours in BORIS\n    behavs_ls = ["potential huddling", "huddling"]\n    # Paths\n    configs_dir = os.path.join(root_dir, "0_configs")\n    boris_dir = os.path.join(root_dir, "boris")\n    out_dir = os.path.join(root_dir, "7_scored_behavs")\n    # Getting names of all files\n    names = [os.path.splitext(i)[0] for i in os.listdir(boris_dir)]\n    for name in names:\n        # Paths\n        boris_fp = os.path.join(boris_dir, f"{name}.tsv")\n        configs_fp = os.path.join(configs_dir, f"{name}.json")\n        out_fp = os.path.join(out_dir, f"{name}.feather")\n        # Making df from BORIS\n        df = BehavMixin.import_boris_tsv(boris_fp, configs_fp, behavs_ls)\n        # Saving df\n        df.to_feather(out_fp)\n    # Making BehavClassifier objects\n    for behav in behavs_ls:\n        BehavClassifier.create_new_model(os.path.join(root_dir, "behav_models"), behav)\n\n    # Option 2: From previous behavysis project\n    proj = Project(root_dir)\n    proj.import_experiments()\n    # Making BehavClassifier objects\n    BehavClassifier.create_from_project(proj)\n\n    # Loading a BehavModel\n    behav = "fight"\n    model = BehavClassifier.load(\n        os.path.join(root_dir, "behav_models", f"{behav}.json")\n    )\n    # Testing all different classifiers\n    model.clf_eval_compare_all()\n    # MANUALLY LOOK AT THE BEST CLASSIFIER AND SELECT\n    # Example\n    model.pipeline_build(ClfTemplates.dnn_1)\n</code></pre>',
    },
    {
      location: "installation/installing.html",
      title: "Installing",
      text: "<p>Step 1:</p> <p>Install conda by visiting the Miniconda downloads page and following the prompts to install on your system.</p> <p>Open the downloaded miniconda file and follow the installation prompts.</p> <p>Step 2:</p> <p>Open a terminal (Mac or Linux) or Anaconda PowerShell Prompt (Windows) and verify that conda has been installed with the following command.</p> <pre><code>conda --version\n</code></pre> <p>A response like <code>conda xx.xx.xx</code> indicates that it has been correctly installed.</p> <p>Step 3:</p> <p>Update conda and use the libmamba solver (makes downloading conda programs MUCH faster):</p> <pre><code>conda update -n base conda\nconda install -n base conda-libmamba-solver\nconda config --set solver libmamba\n</code></pre> <p>Step 4:</p> <p>Install packages that help Jupyter notebooks read conda environments:</p> <pre><code>conda install -n base nb_conda nb_conda_kernels\n</code></pre> <p>Step 5:</p> <p>Install the <code>behavysis_pipeline</code> conda environment (download here).</p> <pre><code>conda env create -f path/to/conda_env.yaml\n</code></pre> <p>Step 6:</p> <p>Install the <code>DEEPLABCUT</code> conda environment (download here).</p> <pre><code>conda env create -f path/to/DEEPLABCUT.yaml\n</code></pre> <p>Step 7:</p> <p>Install the <code>simba</code> conda environment (download here).</p> <pre><code>conda env create -f path/to/simba_env.yaml\n</code></pre>",
    },
    {
      location: "installation/running.html",
      title: "Running",
      text: "<p>Step 1:</p> <p>Open a terminal (Mac or Linux) or Anaconda PowerShell Prompt (Windows)</p> <p>Step 2:</p> <p>Activate the program environment with the following command:</p> <pre><code>conda activate behavysis_pipeline_env\n</code></pre> <p>Step 3:</p> <p>You can now use the <code>behavysis</code> package in a Jupyter kernel or regular Python script.</p> <p>See here for examples of Jupyter notebooks to run behaviour analysis.</p> <p>See here for examples of Jupyter notebooks to train behaviour classifiers.</p> <p>See here for a tutorial of <code>behavysis</code>'s workflow.</p> <p>See here for API documentation.</p> <p>Note</p> <p>To run jupyter, run the following command in the terminal</p> <pre><code>jupyter-lab\n</code></pre> <p>This will open a browser to <code>http://127.0.0.1:8888/lab</code>, where you can run jupyter notebooks.</p> <p>You can also run jupyter notebooks in VS Code.</p>",
    },
    {
      location: "installation/uninstalling.html",
      title: "Uninstalling",
      text: "<p>For more information about how to uninstall conda, see here.</p> <p>Step 1:</p> <p>Open a terminal (Mac or Linux) or Anaconda PowerShell Prompt (Windows)</p> <p>Step 2:</p> <p>To uninstall the <code>behavysis_pipeline_env</code>, <code>DEEPLABCUT</code>, and <code>simba</code> conda envs, run the following commands:</p> <pre><code>conda env remove -n behavysis_pipeline_env\nconda env remove -n DEEPLABCUT\nconda env remove -n simba\n</code></pre> <p>Step 3:</p> <p>To remove conda, enter the following commands in the terminal.</p> <pre><code>conda install anaconda-clean\nanaconda-clean --yes\n\nrm -rf ~/anaconda3\nrm -rf ~/opt/anaconda3\nrm -rf ~/.anaconda_backup\n</code></pre> <p>Step 5: Edit your bash or zsh profile so conda it does not look for conda anymore. Open each of these files (note that not all of them may exist on your computer), <code>~/.zshrc</code>, <code>~/.zprofile</code>, or <code>~/.bash_profile</code>, with the following command.</p> <pre><code>open ~/.zshrc\nopen ~/.zprofile\nopen ~/.bash_profile\n</code></pre>",
    },
    {
      location: "installation/updating.html",
      title: "Updating",
      text: "<p>Step 1: Download the <code>conda_env.yaml</code> file from here</p> <p>Step 2: Run the following command to update <code>behavysis_pipeline</code>:</p> <pre><code>conda env update -f conda_env.yaml --prune\n</code></pre>",
    },
    {
      location: "reference/behav_classifier.html",
      title: "Behav classifier",
      text: "",
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier",
      title:
        "<code>behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier</code>",
      text: '<p>BehavClassifier abstract class peforms behav classifier model preparation, training, saving, evaluation, and inference.</p> <p>Attributes:</p> Name Type Description <code>configs_fp</code> <code>str</code> <p>description</p> <code>clf</code> <code>BaseTorchModel</code> <p>description</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>class BehavClassifier:\n    """\n    BehavClassifier abstract class peforms behav classifier model preparation, training, saving,\n    evaluation, and inference.\n\n    Attributes\n    ----------\n    configs_fp\n        _description_\n    clf\n        _description_\n    """\n\n    configs_fp: str\n    clf: BaseTorchModel\n\n    def __init__(self, configs_fp: str) -&gt; None:\n        """\n        Make a BehavClassifier instance.\n\n        Parameters\n        ----------\n        configs_fp :\n            _description_\n        """\n        # Storing configs json fp\n        self.configs_fp = configs_fp\n        self.clf = None\n        # Trying to read in configs json. Making a new one if it doesn\'t exist\n        try:\n            configs = BehavClassifierConfigs.read_json(self.configs_fp)\n            logging.info("Reading existing model configs")\n        except FileNotFoundError:\n            configs = BehavClassifierConfigs()\n            logging.info("Making new model configs")\n        # Saving configs\n        configs.write_json(self.configs_fp)\n\n    #################################################\n    # CREATE MODEL METHODS\n    #################################################\n\n    @classmethod\n    def create_from_project(cls, proj: Project) -&gt; list[BehavClassifier]:\n        """\n        Loading classifier from given Project instance.\n\n        Parameters\n        ----------\n        proj :\n            The Project instance.\n\n        Returns\n        -------\n        :\n            The loaded BehavClassifier instance.\n        """\n        # Getting the list of behaviours\n        y_df = cls.wrangle_columns_y(\n            cls.combine(os.path.join(proj.root_dir, Folders.SCORED_BEHAVS.value))\n        )\n        # For each behaviour, making a new BehavClassifier instance\n        behavs_ls = y_df.columns.to_list()\n        models_dir = os.path.join(proj.root_dir, BEHAV_MODELS_SUBDIR)\n        models_ls = [cls.create_new_model(models_dir, behav) for behav in behavs_ls]\n        # Importing data from project to "beham_models" folder (only need one model for this)\n        if len(models_ls) &gt; 0:\n            models_ls[0].import_data(\n                os.path.join(proj.root_dir, Folders.FEATURES_EXTRACTED.value),\n                os.path.join(proj.root_dir, Folders.SCORED_BEHAVS.value),\n                False,\n            )\n        return models_ls\n\n    @classmethod\n    def create_new_model(cls, root_dir: str, behaviour_name: str) -&gt; BehavClassifier:\n        """\n        Creating a new BehavClassifier model in the given directory\n        """\n        configs_fp = os.path.join(root_dir, f"{behaviour_name}.json")\n        # Making new BehavClassifier instance\n        inst = cls(configs_fp)\n        # Updating configs with project data\n        configs = inst.configs\n        configs.behaviour_name = behaviour_name\n        configs.write_json(inst.configs_fp)\n        # Returning model\n        return inst\n\n    def create_from_model(self, root_dir: str, behaviour_name: str) -&gt; BehavClassifier:\n        """\n        Creating a new BehavClassifier model in the given directory\n        """\n        configs_fp = os.path.join(root_dir, f"{behaviour_name}.json")\n        # Making new BehavClassifier instance\n        inst = self.create_new_model(configs_fp, behaviour_name)\n        # Using current instance\'s configs (but using given behaviour_name)\n        configs = self.configs\n        configs.behaviour_name = behaviour_name\n        configs.write_json(inst.configs_fp)\n        # Returning model\n        return inst\n\n    #################################################\n    #            READING MODEL\n    #################################################\n\n    @classmethod\n    def load(cls, configs_fp: str) -&gt; BehavClassifier:\n        """\n        Reads the model from the expected model file.\n        """\n        if not os.path.isfile(configs_fp):\n            raise FileNotFoundError(f"The model file does not exist: {configs_fp}")\n        return cls(configs_fp)\n\n    #################################################\n    #            GETTER AND SETTERS\n    #################################################\n\n    @property\n    def configs(self) -&gt; BehavClassifierConfigs:\n        """Returns the config model from the expected config file."""\n        return BehavClassifierConfigs.read_json(self.configs_fp)\n\n    @property\n    def root_dir(self) -&gt; str:\n        """Returns the model\'s root directory"""\n        return os.path.dirname(self.configs_fp)\n\n    @property\n    def clf_fp(self) -&gt; str:\n        """Returns the model\'s filepath"""\n        path = os.path.join(self.root_dir, self.configs.behaviour_name, "model.sav")\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        return path\n\n    @property\n    def preproc_fp(self) -&gt; str:\n        """Returns the model\'s preprocessor filepath"""\n        path = os.path.join(self.root_dir, self.configs.behaviour_name, "preproc.sav")\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        return path\n\n    @property\n    def eval_dir(self) -&gt; str:\n        """Returns the model\'s evaluation directory"""\n        path = os.path.join(self.root_dir, self.configs.behaviour_name, "eval")\n        os.makedirs(path, exist_ok=True)\n        return path\n\n    #################################################\n    #            IMPORTING DATA TO MODEL\n    #################################################\n\n    def import_data(self, x_dir: str, y_dir: str, overwrite=False) -&gt; None:\n        """\n        Importing data from extracted features and labelled behaviours dataframes.\n\n        Parameters\n        ----------\n        x_dir :\n            _description_\n        y_dir :\n            _description_\n        """\n        # For each x and y directory\n        for in_dir, id in ((x_dir, X_ID), (y_dir, Y_ID)):\n            out_dir = os.path.join(self.root_dir, id)\n            os.makedirs(out_dir, exist_ok=True)\n            # Copying each file to model root directory\n            for fp in os.listdir(in_dir):\n                in_fp = os.path.join(in_dir, fp)\n                out_fp = os.path.join(out_dir, fp)\n                # If not overwriting and out file already exists, then skip\n                if not overwrite and os.path.exists(out_fp):\n                    continue\n                # Copying file\n                shutil.copyfile(in_fp, out_fp)\n\n    #################################################\n    #            COMBINING DFS TO SINGLE DF\n    #################################################\n\n    @staticmethod\n    def combine(src_dir):\n        data_dict = {\n            os.path.splitext(i)[0]: pd.read_feather(os.path.join(src_dir, i))\n            for i in os.listdir(os.path.join(src_dir))\n        }\n        return pd.concat(data_dict.values(), axis=0, keys=data_dict.keys())\n\n    def combine_dfs(self) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n        """\n        Combines the data into a single `X` df, `y` df, and index.\n        The indexes of `x` and `y` will be the same (with an inner join)\n\n        Returns\n        -------\n        x :\n            Features dataframe of all experiments in the `x` directory\n        y :\n            Outcomes dataframe of all experiments in the `y` directory\n        """\n        # Getting the x and y dfs\n        x = BehavClassifier.combine(os.path.join(self.root_dir, X_ID))\n        y = BehavClassifier.combine(os.path.join(self.root_dir, Y_ID))\n        # Getting the intersection pf the x and y row indexes\n        index = x.index.intersection(y.index)\n        x = x.loc[index]\n        y = y.loc[index]\n        # Assert that x and y are the same length\n        assert x.shape[0] == y.shape[0]\n        # Returning the x and y dfs\n        return x, y\n\n    #################################################\n    #            PREPROCESSING DFS\n    #################################################\n\n    @staticmethod\n    def preproc_x_fit(x: np.ndarray, preproc_fp: str) -&gt; None:\n        """\n        __summary__\n        """\n        # Making pipeline\n        preproc_pipe = Pipeline(steps=[("MinMaxScaler", MinMaxScaler())])\n        # Fitting pipeline\n        preproc_pipe.fit(x)\n        # Saving pipeline\n        joblib.dump(preproc_pipe, preproc_fp)\n\n    @staticmethod\n    def preproc_x(x: np.ndarray, preproc_fp: str) -&gt; np.ndarray:\n        """\n        The preprocessing steps are:\n        - MinMax scaling (using previously fitted MinMaxScaler)\n        """\n        # Loading in pipeline\n        preproc_pipe = joblib.load(preproc_fp)\n        # Uses trained fit for preprocessing new data\n        x = preproc_pipe.transform(x)\n        # Returning df\n        return x\n\n    @staticmethod\n    def wrangle_columns_y(y: pd.DataFrame) -&gt; pd.DataFrame:\n        """\n        _summary_\n\n        Parameters\n        ----------\n        y :\n            _description_\n\n        Returns\n        -------\n        :\n            _description_\n        """\n        # Filtering out the prob and pred columns (in the `outcomes` level)\n        cols_filter = np.isin(\n            y.columns.get_level_values(BehavCN.OUTCOMES.value),\n            [BehavColumns.PROB.value, BehavColumns.PRED.value],\n            invert=True,\n        )\n        y = y.loc[:, cols_filter]\n        # Converting MultiIndex columns to single columns by\n        # setting the column names from `(behav, outcome)` to `{behav}__{outcome}`\n        y.columns = [\n            f"{i[0]}" if i[1] == BehavColumns.ACTUAL.value else f"{i[0]}__{i[1]}"\n            for i in y.columns\n        ]\n        return y\n\n    @staticmethod\n    def preproc_y(y: np.ndarray) -&gt; np.ndarray:\n        """\n        The preprocessing steps are:\n        - Imputing NaN values with 0\n        - Setting -1 to 0\n        - Converting the MultiIndex columns from `(behav, outcome)` to `{behav}__{outcome}`,\n        by expanding the `actual` and all specific outcome columns of each behav.\n        """\n        # Imputing NaN values with 0\n        y = np.nan_to_num(y, nan=0)\n        # Setting -1 to 0 (i.e. "undecided" to "no behaviour")\n        y = np.maximum(y, 0)\n        # Returning arr\n        return y\n\n    @staticmethod\n    def undersample(index: np.ndarray, y: np.ndarray, ratio: float) -&gt; np.ndarray:\n        # Assert that index and y are the same length\n        assert index.shape[0] == y.shape[0]\n        # Getting array of True indices\n        t = index[y == 1]\n        # Getting array of False indices\n        f = index[y == 0]\n        # Undersampling the False indices\n        f = np.random.choice(f, size=int(t.shape[0] / ratio), replace=False)\n        # Combining the True and False indices\n        uindex = np.union1d(t, f)\n        # Returning the undersampled index\n        return uindex\n\n    #################################################\n    #            PIPELINE FOR DATA PREP\n    #################################################\n\n    def prepare_data_training(self) -&gt; tuple[np.ndarray, np.ndarray]:\n        """\n        Prepares the data (`x` and `y`) in the model for training.\n        Data is taken from the model\'s `x` and `y` dirs.\n\n        Performs the following:\n        - Combining dfs from x and y directories (individual experiment data)\n        - Ensures the x and y dfs have the same index, and are in the same row order\n        - Preprocesses x df. Refer to `preprocess_x` for details.\n        - Selects the y class (given in the configs file) from the y df.\n        - Preprocesses y df. Refer to `preprocess_y` for details.\n\n        Returns\n        -------\n        x : np.ndarray\n            Features array in the format: `(samples, window, features)`\n        y : np.ndarray\n            Outcomes array in the format: `(samples, class)`\n        """\n        # Combining dfs from x and y directories (individual experiment data)\n        x, y = self.combine_dfs()\n        # Fitting the preprocessor pipeline\n        self.preproc_x_fit(x, self.preproc_fp)\n        # Preprocessing x df\n        x = self.preproc_x(x, self.preproc_fp)\n        # Preprocessing y df\n        y = self.wrangle_columns_y(y)[self.configs.behaviour_name].values\n        y = self.preproc_y(y)\n        # Returning x, y, and index to use\n        return x, y\n\n    def prepare_data_training_pipeline(\n        self,\n    ) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        """\n        Prepares the data for the training pipeline.\n\n        Performs the following:\n        - Preprocesses `x` and `y` data. Refer to `prepare_data_training` for details.\n        - Splits into training and test indexes.\n            - The training indexes are undersampled to the ratio given in the configs.\n\n        Returns:\n            A tuple containing four numpy arrays:\n            - x: The input data.\n            - y: The target labels.\n            - ind_train: The indexes for the training data.\n            - ind_test: The indexes for the testing data.\n        """\n        # Preparing data\n        x, y = self.prepare_data_training()\n        # Getting entire index\n        index = np.arange(x.shape[0])\n        # Splitting into train and test indexes\n        ind_train, ind_test = train_test_split(\n            index,\n            test_size=self.configs.test_split,\n            stratify=y[index],\n        )\n        # Undersampling training index\n        ind_train = self.undersample(\n            ind_train, y[ind_train], self.configs.undersample_ratio\n        )\n        # Return\n        return x, y, ind_train, ind_test\n\n    def prepare_data(self, x: pd.DataFrame) -&gt; np.ndarray:\n        """\n        Prepares novel (`x` only) data, given the `x` pd.DataFrame.\n\n        Performs the following:\n        - Preprocesses x df. Refer to `preprocess_x` for details.\n        - Makes the X windowed array, for each index.\n\n        Returns\n        -------\n        x : np.ndarray\n            Features array in the format: `(samples, window, features)`\n        """\n        # Preprocessing x df\n        x = self.preproc_x(x, self.preproc_fp)\n        # Returning x\n        return x\n\n    #################################################\n    # PIPELINE FOR CLASSIFIER TRAINING AND INFERENCE\n    #################################################\n\n    def pipeline_build(self, clf_init_f: Callable) -&gt; None:\n        """\n        Makes a classifier and saves it to the model\'s root directory.\n\n        Callable is a method from `ClfTemplates`.\n        """\n        # Preparing data\n        x, y, ind_train, ind_test = self.prepare_data_training_pipeline()\n        # Initialising the model\n        self.clf = clf_init_f()\n        # Training the model\n        history = self.clf.fit(\n            x=x,\n            y=y,\n            index=ind_train,\n            batch_size=self.configs.batch_size,\n            epochs=self.configs.epochs,\n            val_split=self.configs.val_split,\n        )\n        # Saving history\n        self.clf_eval_save_history(history)\n        # Evaluating the model\n        self.clf_eval(x, y, ind_test)\n        # Updating the model configs\n        configs = self.configs\n        configs.clf_structure = clf_init_f.__name__\n        configs.write_json(self.configs_fp)\n        # Saving the model to disk\n        self.clf_save()\n\n    def pipeline_run(self, x: pd.DataFrame) -&gt; pd.DataFrame:\n        """\n        Given the unprocessed features dataframe, runs the model pipeline to make predictions.\n\n        Pipeline is:\n        - Preprocess `x` df. Refer to\n        [behavysis_pipeline.behav_classifier.BehavClassifier.preproc_x][] for details.\n        - Makes predictions and returns the predicted behaviours.\n        """\n        # Saving index for later\n        index = x.index\n        # Preprocessing features\n        x = self.prepare_data(x)\n        # Loading the model\n        self.clf_load()\n        # Making predictions\n        y_eval = self.clf_predict(x, self.configs.batch_size)\n        # Settings the index\n        y_eval.index = index\n        # Returning predictions\n        return y_eval\n\n    #################################################\n    # MODEL CLASSIFIER METHODS\n    #################################################\n\n    def clf_load(self):\n        """\n        Loads the model stored in `&lt;root_dir&gt;/&lt;behav_name&gt;.sav` to the model attribute.\n        """\n        self.clf = joblib.load(self.clf_fp)\n\n    def clf_save(self):\n        """\n        Saves the model\'s classifier to `&lt;root_dir&gt;/&lt;behav_name&gt;.sav`.\n        """\n        joblib.dump(self.clf, self.clf_fp)\n\n    def clf_predict(\n        self,\n        x: np.ndarray,\n        batch_size: int,\n        index: Optional[np.ndarray] = None,\n    ) -&gt; pd.DataFrame:\n        """\n        Making predictions using the given model and preprocessed features.\n        Assumes the x array is already preprocessed.\n\n        Parameters\n        ----------\n        x : np.ndarray\n            Preprocessed features.\n\n        Returns\n        -------\n        pd.DataFrame\n            Predicted behaviour classifications. Dataframe columns are in the format:\n            ```\n            behaviours :  behav    behav\n            outcomes   :  "prob"   "pred"\n            ```\n        """\n        # Getting probabilities\n        index = np.arange(x.shape[0]) if index is None else index\n        y_probs = self.clf.predict(\n            x=x,\n            index=index,\n            batch_size=batch_size,\n        )\n        # Making predictions from probabilities (and pcutoff)\n        y_preds = (y_probs &gt; self.configs.pcutoff).astype(int)\n        # Making df\n        pred_df = BehavMixin.init_df(index)\n        pred_df[(self.configs.behaviour_name, BehavColumns.PROB.value)] = y_probs\n        pred_df[(self.configs.behaviour_name, BehavColumns.PRED.value)] = y_preds\n        # Returning predicted behavs\n        return pred_df\n\n    #################################################\n    # COMPREHENSIVE EVALUATION FUNCTIONS\n    #################################################\n\n    def clf_eval_save_history(self, history: pd.DataFrame, name: Optional[str] = ""):\n        # Saving history df\n        DFMixin.write_feather(\n            history, os.path.join(self.eval_dir, f"{name}_history.feather")\n        )\n        # Making and saving history figure\n        fig, ax = plt.subplots(figsize=(10, 7))\n        sns.lineplot(data=history, ax=ax)\n        fig.savefig(os.path.join(self.eval_dir, f"{name}_history.png"))\n\n    def clf_eval(\n        self,\n        x: np.ndarray,\n        y: np.ndarray,\n        index: Optional[np.ndarray] = None,\n        name: Optional[str] = "",\n    ) -&gt; tuple[pd.DataFrame, dict, plt.Figure, plt.Figure, plt.Figure]:\n        """\n        Evaluates the classifier performance on the given x and y data.\n        Saves the `metrics_fig` and `pcutoffs_fig` to the model\'s root directory.\n\n        Returns\n        -------\n        y_eval : pd.DataFrame\n            Predicted behaviour classifications against the true labels.\n        metrics_fig : mpl.Figure\n            Figure showing the confusion matrix.\n        pcutoffs_fig : mpl.Figure\n            Figure showing the precision, recall, f1, and accuracy for different pcutoffs.\n        logc_fig : mpl.Figure\n            Figure showing the logistic curve for different predicted probabilities.\n        """\n        # Making eval df\n        index = np.arange(x.shape[0]) if index is None else index\n        y_eval = self.clf_predict(x=x, index=index, batch_size=self.configs.batch_size)\n        # Including `actual` lables in `y_eval`\n        y_eval[self.configs.behaviour_name, BehavColumns.ACTUAL.value] = y[index]\n        # Getting individual columns\n        y_prob = y_eval[self.configs.behaviour_name, BehavColumns.PROB.value]\n        y_pred = y_eval[self.configs.behaviour_name, BehavColumns.PRED.value]\n        y_true = y_eval[self.configs.behaviour_name, BehavColumns.ACTUAL.value]\n        # Making classification report\n        report_dict = self.eval_report(y_true, y_pred)\n        # Making confusion matrix figure\n        metrics_fig = self.eval_conf_matr(y_true, y_pred)\n        # Making performance for different pcutoffs figure\n        pcutoffs_fig = self.eval_metrics_pcutoffs(y_true, y_prob)\n        # Logistic curve\n        logc_fig = self.eval_logc(y_true, y_prob)\n        # Saving data and figures\n        DFMixin.write_feather(\n            y_eval, os.path.join(self.eval_dir, f"{name}_eval.feather")\n        )\n        with open(os.path.join(self.eval_dir, f"{name}_report.json"), "w") as f:\n            json.dump(report_dict, f)\n        metrics_fig.savefig(os.path.join(self.eval_dir, f"{name}_confm.png"))\n        pcutoffs_fig.savefig(os.path.join(self.eval_dir, f"{name}_pcutoffs.png"))\n        logc_fig.savefig(os.path.join(self.eval_dir, f"{name}_logc.png"))\n        # Print classification report\n        print(json.dumps(report_dict, indent=4))\n        # Return evaluations\n        return y_eval, report_dict, metrics_fig, pcutoffs_fig, logc_fig\n\n    def clf_eval_compare_all(self):\n        """\n        Making classifier for all available templates.\n\n        Notes\n        -----\n        Takes a long time to run.\n        """\n        # Saving existing clf\n        clf = self.clf\n        # Preparing data\n        x, y, ind_train, ind_test = self.prepare_data_training_pipeline()\n        # # Adding noise (TODO: use with augmentation)\n        # noise = 0.05\n        # x_train += np.random.normal(0, noise, x_train.shape)\n        # x_test += np.random.normal(0, noise, x_test.shape)\n        # Getting eval for each classifier in ClfTemplates\n        for clf_init_f in CLF_TEMPLATES:\n            clf_name = clf_init_f.__name__\n            # Making classifier\n            self.clf = clf_init_f()\n            # Training\n            history = self.clf.fit(\n                x=x,\n                y=y,\n                index=ind_train,\n                batch_size=self.configs.batch_size,\n                epochs=self.configs.epochs,\n                val_split=self.configs.val_split,\n            )\n            # Saving history\n            self.clf_eval_save_history(history, name=clf_name)\n            # Evaluating on train and test data\n            self.clf_eval(x, y, index=ind_train, name=f"{clf_name}_train")\n            self.clf_eval(x, y, index=ind_test, name=f"{clf_name}_test")\n        # Restoring clf\n        self.clf = clf\n\n    #################################################\n    # EVALUATION METRICS FUNCTIONS\n    #################################################\n\n    @staticmethod\n    def eval_report(y_true: pd.Series, y_pred: pd.Series) -&gt; dict:\n        """\n        __summary__\n        """\n        return classification_report(\n            y_true,\n            y_pred,\n            target_names=GENERIC_BEHAV_LABELS,\n            output_dict=True,\n        )\n\n    @staticmethod\n    def eval_conf_matr(y_true: pd.Series, y_pred: pd.Series) -&gt; plt.Figure:\n        """\n        __summary__\n        """\n        # Making confusion matrix\n        fig, ax = plt.subplots(figsize=(7, 7))\n        sns.heatmap(\n            confusion_matrix(y_true, y_pred),\n            annot=True,\n            fmt="d",\n            cmap="viridis",\n            cbar=False,\n            xticklabels=GENERIC_BEHAV_LABELS,\n            yticklabels=GENERIC_BEHAV_LABELS,\n            ax=ax,\n        )\n        ax.set_xlabel("Predicted")\n        ax.set_ylabel("True")\n        return fig\n\n    @staticmethod\n    def eval_metrics_pcutoffs(y_true: pd.Series, y_prob: pd.Series) -&gt; plt.Figure:\n        """\n        __summary__\n        """\n        # Getting precision, recall and accuracy for different cutoffs\n        pcutoffs = np.linspace(0, 1, 101)\n        # Measures\n        precisions = np.zeros(pcutoffs.shape[0])\n        recalls = np.zeros(pcutoffs.shape[0])\n        f1 = np.zeros(pcutoffs.shape[0])\n        accuracies = np.zeros(pcutoffs.shape[0])\n        for i, pcutoff in enumerate(pcutoffs):\n            y_pred = y_prob &gt; pcutoff\n            report = classification_report(\n                y_true,\n                y_pred,\n                target_names=GENERIC_BEHAV_LABELS,\n                output_dict=True,\n            )\n            precisions[i] = report[GENERIC_BEHAV_LABELS[1]]["precision"]\n            recalls[i] = report[GENERIC_BEHAV_LABELS[1]]["recall"]\n            f1[i] = report[GENERIC_BEHAV_LABELS[1]]["f1-score"]\n            accuracies[i] = report["accuracy"]\n        # Making figure\n        fig, ax = plt.subplots(figsize=(10, 7))\n        sns.lineplot(x=pcutoffs, y=precisions, label="precision", ax=ax)\n        sns.lineplot(x=pcutoffs, y=recalls, label="recall", ax=ax)\n        sns.lineplot(x=pcutoffs, y=f1, label="f1", ax=ax)\n        sns.lineplot(x=pcutoffs, y=accuracies, label="accuracy", ax=ax)\n        return fig\n\n    @staticmethod\n    def eval_logc(y_true: pd.Series, y_prob: pd.Series) -&gt; plt.Figure:\n        """\n        __summary__\n        """\n        y_eval = pd.DataFrame(\n            {\n                "y_true": y_true,\n                "y_prob": y_prob,\n                "y_pred": y_prob &gt; 0.4,\n                "y_true_jitter": y_true + (0.2 * (np.random.rand(len(y_prob)) - 0.5)),\n            }\n        )\n        fig, ax = plt.subplots(figsize=(10, 7))\n        sns.scatterplot(\n            data=y_eval,\n            x="y_prob",\n            y="y_true_jitter",\n            marker=".",\n            s=10,\n            linewidth=0,\n            alpha=0.2,\n            ax=ax,\n        )\n        # Making line of ratio of y_true outcomes for each y_prob\n        pcutoffs = np.linspace(0, 1, 101)\n        ratios = np.vectorize(lambda i: np.mean(i &gt; y_eval["y_prob"]))(pcutoffs)\n        sns.lineplot(x=pcutoffs, y=ratios, ax=ax)\n        # Returning figure\n        return fig\n\n    @staticmethod\n    def eval_bouts(y_true: pd.Series, y_pred: pd.Series) -&gt; pd.DataFrame:\n        """\n        __summary__\n        """\n        y_eval = pd.DataFrame({"y_true": y_true, "y_pred": y_pred})\n        y_eval["ids"] = np.cumsum(y_eval["y_true"] != y_eval["y_true"].shift())\n        # Getting the proportion of correct predictions for each bout\n        y_eval_grouped = y_eval.groupby("ids")\n        y_eval_summary = pd.DataFrame(\n            y_eval_grouped.apply(lambda x: (x["y_pred"] == x["y_true"]).mean()),\n            columns=["proportion"],\n        )\n        y_eval_summary["actual_bout"] = y_eval_grouped.apply(\n            lambda x: x["y_true"].mean()\n        )\n        y_eval_summary["bout_len"] = y_eval_grouped.apply(lambda x: x.shape[0])\n        y_eval_summary = y_eval_summary.sort_values("proportion")\n        # # Making figure\n        # fig, ax = plt.subplots(figsize=(10, 7))\n        # sns.scatterplot(\n        #     data=y_eval_summary,\n        #     x="proportion",\n        #     y="bout_len",\n        #     hue="actual_bout",\n        #     alpha=0.4,\n        #     marker=".",\n        #     s=50,\n        #     linewidth=0,\n        #     ax=ax,\n        # )\n        return y_eval_summary\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.clf_fp",
      title: "<code>clf_fp: str</code>  <code>property</code>",
      text: "<p>Returns the model's filepath</p>",
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.configs",
      title:
        "<code>configs: BehavClassifierConfigs</code>  <code>property</code>",
      text: "<p>Returns the config model from the expected config file.</p>",
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.eval_dir",
      title: "<code>eval_dir: str</code>  <code>property</code>",
      text: "<p>Returns the model's evaluation directory</p>",
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.preproc_fp",
      title: "<code>preproc_fp: str</code>  <code>property</code>",
      text: "<p>Returns the model's preprocessor filepath</p>",
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.root_dir",
      title: "<code>root_dir: str</code>  <code>property</code>",
      text: "<p>Returns the model's root directory</p>",
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.__init__",
      title: "<code>__init__(configs_fp)</code>",
      text: '<p>Make a BehavClassifier instance.</p> <p>Parameters:</p> Name Type Description Default <code>configs_fp</code> <code>str</code> <p>description</p> required Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>def __init__(self, configs_fp: str) -&gt; None:\n    """\n    Make a BehavClassifier instance.\n\n    Parameters\n    ----------\n    configs_fp :\n        _description_\n    """\n    # Storing configs json fp\n    self.configs_fp = configs_fp\n    self.clf = None\n    # Trying to read in configs json. Making a new one if it doesn\'t exist\n    try:\n        configs = BehavClassifierConfigs.read_json(self.configs_fp)\n        logging.info("Reading existing model configs")\n    except FileNotFoundError:\n        configs = BehavClassifierConfigs()\n        logging.info("Making new model configs")\n    # Saving configs\n    configs.write_json(self.configs_fp)\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.clf_eval",
      title: "<code>clf_eval(x, y, index=None, name='')</code>",
      text: '<p>Evaluates the classifier performance on the given x and y data. Saves the <code>metrics_fig</code> and <code>pcutoffs_fig</code> to the model\'s root directory.</p> <p>Returns:</p> Name Type Description <code>y_eval</code> <code>DataFrame</code> <p>Predicted behaviour classifications against the true labels.</p> <code>metrics_fig</code> <code>Figure</code> <p>Figure showing the confusion matrix.</p> <code>pcutoffs_fig</code> <code>Figure</code> <p>Figure showing the precision, recall, f1, and accuracy for different pcutoffs.</p> <code>logc_fig</code> <code>Figure</code> <p>Figure showing the logistic curve for different predicted probabilities.</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>def clf_eval(\n    self,\n    x: np.ndarray,\n    y: np.ndarray,\n    index: Optional[np.ndarray] = None,\n    name: Optional[str] = "",\n) -&gt; tuple[pd.DataFrame, dict, plt.Figure, plt.Figure, plt.Figure]:\n    """\n    Evaluates the classifier performance on the given x and y data.\n    Saves the `metrics_fig` and `pcutoffs_fig` to the model\'s root directory.\n\n    Returns\n    -------\n    y_eval : pd.DataFrame\n        Predicted behaviour classifications against the true labels.\n    metrics_fig : mpl.Figure\n        Figure showing the confusion matrix.\n    pcutoffs_fig : mpl.Figure\n        Figure showing the precision, recall, f1, and accuracy for different pcutoffs.\n    logc_fig : mpl.Figure\n        Figure showing the logistic curve for different predicted probabilities.\n    """\n    # Making eval df\n    index = np.arange(x.shape[0]) if index is None else index\n    y_eval = self.clf_predict(x=x, index=index, batch_size=self.configs.batch_size)\n    # Including `actual` lables in `y_eval`\n    y_eval[self.configs.behaviour_name, BehavColumns.ACTUAL.value] = y[index]\n    # Getting individual columns\n    y_prob = y_eval[self.configs.behaviour_name, BehavColumns.PROB.value]\n    y_pred = y_eval[self.configs.behaviour_name, BehavColumns.PRED.value]\n    y_true = y_eval[self.configs.behaviour_name, BehavColumns.ACTUAL.value]\n    # Making classification report\n    report_dict = self.eval_report(y_true, y_pred)\n    # Making confusion matrix figure\n    metrics_fig = self.eval_conf_matr(y_true, y_pred)\n    # Making performance for different pcutoffs figure\n    pcutoffs_fig = self.eval_metrics_pcutoffs(y_true, y_prob)\n    # Logistic curve\n    logc_fig = self.eval_logc(y_true, y_prob)\n    # Saving data and figures\n    DFMixin.write_feather(\n        y_eval, os.path.join(self.eval_dir, f"{name}_eval.feather")\n    )\n    with open(os.path.join(self.eval_dir, f"{name}_report.json"), "w") as f:\n        json.dump(report_dict, f)\n    metrics_fig.savefig(os.path.join(self.eval_dir, f"{name}_confm.png"))\n    pcutoffs_fig.savefig(os.path.join(self.eval_dir, f"{name}_pcutoffs.png"))\n    logc_fig.savefig(os.path.join(self.eval_dir, f"{name}_logc.png"))\n    # Print classification report\n    print(json.dumps(report_dict, indent=4))\n    # Return evaluations\n    return y_eval, report_dict, metrics_fig, pcutoffs_fig, logc_fig\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.clf_eval_compare_all",
      title: "<code>clf_eval_compare_all()</code>",
      text: '<p>Making classifier for all available templates.</p> Notes <p>Takes a long time to run.</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>def clf_eval_compare_all(self):\n    """\n    Making classifier for all available templates.\n\n    Notes\n    -----\n    Takes a long time to run.\n    """\n    # Saving existing clf\n    clf = self.clf\n    # Preparing data\n    x, y, ind_train, ind_test = self.prepare_data_training_pipeline()\n    # # Adding noise (TODO: use with augmentation)\n    # noise = 0.05\n    # x_train += np.random.normal(0, noise, x_train.shape)\n    # x_test += np.random.normal(0, noise, x_test.shape)\n    # Getting eval for each classifier in ClfTemplates\n    for clf_init_f in CLF_TEMPLATES:\n        clf_name = clf_init_f.__name__\n        # Making classifier\n        self.clf = clf_init_f()\n        # Training\n        history = self.clf.fit(\n            x=x,\n            y=y,\n            index=ind_train,\n            batch_size=self.configs.batch_size,\n            epochs=self.configs.epochs,\n            val_split=self.configs.val_split,\n        )\n        # Saving history\n        self.clf_eval_save_history(history, name=clf_name)\n        # Evaluating on train and test data\n        self.clf_eval(x, y, index=ind_train, name=f"{clf_name}_train")\n        self.clf_eval(x, y, index=ind_test, name=f"{clf_name}_test")\n    # Restoring clf\n    self.clf = clf\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.clf_load",
      title: "<code>clf_load()</code>",
      text: '<p>Loads the model stored in <code>&lt;root_dir&gt;/&lt;behav_name&gt;.sav</code> to the model attribute.</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>def clf_load(self):\n    """\n    Loads the model stored in `&lt;root_dir&gt;/&lt;behav_name&gt;.sav` to the model attribute.\n    """\n    self.clf = joblib.load(self.clf_fp)\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.clf_predict",
      title: "<code>clf_predict(x, batch_size, index=None)</code>",
      text: '<p>Making predictions using the given model and preprocessed features. Assumes the x array is already preprocessed.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Preprocessed features.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Predicted behaviour classifications. Dataframe columns are in the format: <pre><code>behaviours :  behav    behav\noutcomes   :  "prob"   "pred"\n</code></pre></p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>def clf_predict(\n    self,\n    x: np.ndarray,\n    batch_size: int,\n    index: Optional[np.ndarray] = None,\n) -&gt; pd.DataFrame:\n    """\n    Making predictions using the given model and preprocessed features.\n    Assumes the x array is already preprocessed.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        Preprocessed features.\n\n    Returns\n    -------\n    pd.DataFrame\n        Predicted behaviour classifications. Dataframe columns are in the format:\n        ```\n        behaviours :  behav    behav\n        outcomes   :  "prob"   "pred"\n        ```\n    """\n    # Getting probabilities\n    index = np.arange(x.shape[0]) if index is None else index\n    y_probs = self.clf.predict(\n        x=x,\n        index=index,\n        batch_size=batch_size,\n    )\n    # Making predictions from probabilities (and pcutoff)\n    y_preds = (y_probs &gt; self.configs.pcutoff).astype(int)\n    # Making df\n    pred_df = BehavMixin.init_df(index)\n    pred_df[(self.configs.behaviour_name, BehavColumns.PROB.value)] = y_probs\n    pred_df[(self.configs.behaviour_name, BehavColumns.PRED.value)] = y_preds\n    # Returning predicted behavs\n    return pred_df\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.clf_save",
      title: "<code>clf_save()</code>",
      text: '<p>Saves the model\'s classifier to <code>&lt;root_dir&gt;/&lt;behav_name&gt;.sav</code>.</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>def clf_save(self):\n    """\n    Saves the model\'s classifier to `&lt;root_dir&gt;/&lt;behav_name&gt;.sav`.\n    """\n    joblib.dump(self.clf, self.clf_fp)\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.combine_dfs",
      title: "<code>combine_dfs()</code>",
      text: '<p>Combines the data into a single <code>X</code> df, <code>y</code> df, and index. The indexes of <code>x</code> and <code>y</code> will be the same (with an inner join)</p> <p>Returns:</p> Name Type Description <code>x</code> <code>DataFrame</code> <p>Features dataframe of all experiments in the <code>x</code> directory</p> <code>y</code> <code>DataFrame</code> <p>Outcomes dataframe of all experiments in the <code>y</code> directory</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>def combine_dfs(self) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    """\n    Combines the data into a single `X` df, `y` df, and index.\n    The indexes of `x` and `y` will be the same (with an inner join)\n\n    Returns\n    -------\n    x :\n        Features dataframe of all experiments in the `x` directory\n    y :\n        Outcomes dataframe of all experiments in the `y` directory\n    """\n    # Getting the x and y dfs\n    x = BehavClassifier.combine(os.path.join(self.root_dir, X_ID))\n    y = BehavClassifier.combine(os.path.join(self.root_dir, Y_ID))\n    # Getting the intersection pf the x and y row indexes\n    index = x.index.intersection(y.index)\n    x = x.loc[index]\n    y = y.loc[index]\n    # Assert that x and y are the same length\n    assert x.shape[0] == y.shape[0]\n    # Returning the x and y dfs\n    return x, y\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.create_from_model",
      title: "<code>create_from_model(root_dir, behaviour_name)</code>",
      text: '<p>Creating a new BehavClassifier model in the given directory</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>def create_from_model(self, root_dir: str, behaviour_name: str) -&gt; BehavClassifier:\n    """\n    Creating a new BehavClassifier model in the given directory\n    """\n    configs_fp = os.path.join(root_dir, f"{behaviour_name}.json")\n    # Making new BehavClassifier instance\n    inst = self.create_new_model(configs_fp, behaviour_name)\n    # Using current instance\'s configs (but using given behaviour_name)\n    configs = self.configs\n    configs.behaviour_name = behaviour_name\n    configs.write_json(inst.configs_fp)\n    # Returning model\n    return inst\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.create_from_project",
      title: "<code>create_from_project(proj)</code>  <code>classmethod</code>",
      text: '<p>Loading classifier from given Project instance.</p> <p>Parameters:</p> Name Type Description Default <code>proj</code> <code>Project</code> <p>The Project instance.</p> required <p>Returns:</p> Type Description <code>list[BehavClassifier]</code> <p>The loaded BehavClassifier instance.</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>@classmethod\ndef create_from_project(cls, proj: Project) -&gt; list[BehavClassifier]:\n    """\n    Loading classifier from given Project instance.\n\n    Parameters\n    ----------\n    proj :\n        The Project instance.\n\n    Returns\n    -------\n    :\n        The loaded BehavClassifier instance.\n    """\n    # Getting the list of behaviours\n    y_df = cls.wrangle_columns_y(\n        cls.combine(os.path.join(proj.root_dir, Folders.SCORED_BEHAVS.value))\n    )\n    # For each behaviour, making a new BehavClassifier instance\n    behavs_ls = y_df.columns.to_list()\n    models_dir = os.path.join(proj.root_dir, BEHAV_MODELS_SUBDIR)\n    models_ls = [cls.create_new_model(models_dir, behav) for behav in behavs_ls]\n    # Importing data from project to "beham_models" folder (only need one model for this)\n    if len(models_ls) &gt; 0:\n        models_ls[0].import_data(\n            os.path.join(proj.root_dir, Folders.FEATURES_EXTRACTED.value),\n            os.path.join(proj.root_dir, Folders.SCORED_BEHAVS.value),\n            False,\n        )\n    return models_ls\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.create_new_model",
      title:
        "<code>create_new_model(root_dir, behaviour_name)</code>  <code>classmethod</code>",
      text: '<p>Creating a new BehavClassifier model in the given directory</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>@classmethod\ndef create_new_model(cls, root_dir: str, behaviour_name: str) -&gt; BehavClassifier:\n    """\n    Creating a new BehavClassifier model in the given directory\n    """\n    configs_fp = os.path.join(root_dir, f"{behaviour_name}.json")\n    # Making new BehavClassifier instance\n    inst = cls(configs_fp)\n    # Updating configs with project data\n    configs = inst.configs\n    configs.behaviour_name = behaviour_name\n    configs.write_json(inst.configs_fp)\n    # Returning model\n    return inst\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.eval_bouts",
      title:
        "<code>eval_bouts(y_true, y_pred)</code>  <code>staticmethod</code>",
      text: '<p>summary</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>@staticmethod\ndef eval_bouts(y_true: pd.Series, y_pred: pd.Series) -&gt; pd.DataFrame:\n    """\n    __summary__\n    """\n    y_eval = pd.DataFrame({"y_true": y_true, "y_pred": y_pred})\n    y_eval["ids"] = np.cumsum(y_eval["y_true"] != y_eval["y_true"].shift())\n    # Getting the proportion of correct predictions for each bout\n    y_eval_grouped = y_eval.groupby("ids")\n    y_eval_summary = pd.DataFrame(\n        y_eval_grouped.apply(lambda x: (x["y_pred"] == x["y_true"]).mean()),\n        columns=["proportion"],\n    )\n    y_eval_summary["actual_bout"] = y_eval_grouped.apply(\n        lambda x: x["y_true"].mean()\n    )\n    y_eval_summary["bout_len"] = y_eval_grouped.apply(lambda x: x.shape[0])\n    y_eval_summary = y_eval_summary.sort_values("proportion")\n    # # Making figure\n    # fig, ax = plt.subplots(figsize=(10, 7))\n    # sns.scatterplot(\n    #     data=y_eval_summary,\n    #     x="proportion",\n    #     y="bout_len",\n    #     hue="actual_bout",\n    #     alpha=0.4,\n    #     marker=".",\n    #     s=50,\n    #     linewidth=0,\n    #     ax=ax,\n    # )\n    return y_eval_summary\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.eval_conf_matr",
      title:
        "<code>eval_conf_matr(y_true, y_pred)</code>  <code>staticmethod</code>",
      text: '<p>summary</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>@staticmethod\ndef eval_conf_matr(y_true: pd.Series, y_pred: pd.Series) -&gt; plt.Figure:\n    """\n    __summary__\n    """\n    # Making confusion matrix\n    fig, ax = plt.subplots(figsize=(7, 7))\n    sns.heatmap(\n        confusion_matrix(y_true, y_pred),\n        annot=True,\n        fmt="d",\n        cmap="viridis",\n        cbar=False,\n        xticklabels=GENERIC_BEHAV_LABELS,\n        yticklabels=GENERIC_BEHAV_LABELS,\n        ax=ax,\n    )\n    ax.set_xlabel("Predicted")\n    ax.set_ylabel("True")\n    return fig\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.eval_logc",
      title:
        "<code>eval_logc(y_true, y_prob)</code>  <code>staticmethod</code>",
      text: '<p>summary</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>@staticmethod\ndef eval_logc(y_true: pd.Series, y_prob: pd.Series) -&gt; plt.Figure:\n    """\n    __summary__\n    """\n    y_eval = pd.DataFrame(\n        {\n            "y_true": y_true,\n            "y_prob": y_prob,\n            "y_pred": y_prob &gt; 0.4,\n            "y_true_jitter": y_true + (0.2 * (np.random.rand(len(y_prob)) - 0.5)),\n        }\n    )\n    fig, ax = plt.subplots(figsize=(10, 7))\n    sns.scatterplot(\n        data=y_eval,\n        x="y_prob",\n        y="y_true_jitter",\n        marker=".",\n        s=10,\n        linewidth=0,\n        alpha=0.2,\n        ax=ax,\n    )\n    # Making line of ratio of y_true outcomes for each y_prob\n    pcutoffs = np.linspace(0, 1, 101)\n    ratios = np.vectorize(lambda i: np.mean(i &gt; y_eval["y_prob"]))(pcutoffs)\n    sns.lineplot(x=pcutoffs, y=ratios, ax=ax)\n    # Returning figure\n    return fig\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.eval_metrics_pcutoffs",
      title:
        "<code>eval_metrics_pcutoffs(y_true, y_prob)</code>  <code>staticmethod</code>",
      text: '<p>summary</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>@staticmethod\ndef eval_metrics_pcutoffs(y_true: pd.Series, y_prob: pd.Series) -&gt; plt.Figure:\n    """\n    __summary__\n    """\n    # Getting precision, recall and accuracy for different cutoffs\n    pcutoffs = np.linspace(0, 1, 101)\n    # Measures\n    precisions = np.zeros(pcutoffs.shape[0])\n    recalls = np.zeros(pcutoffs.shape[0])\n    f1 = np.zeros(pcutoffs.shape[0])\n    accuracies = np.zeros(pcutoffs.shape[0])\n    for i, pcutoff in enumerate(pcutoffs):\n        y_pred = y_prob &gt; pcutoff\n        report = classification_report(\n            y_true,\n            y_pred,\n            target_names=GENERIC_BEHAV_LABELS,\n            output_dict=True,\n        )\n        precisions[i] = report[GENERIC_BEHAV_LABELS[1]]["precision"]\n        recalls[i] = report[GENERIC_BEHAV_LABELS[1]]["recall"]\n        f1[i] = report[GENERIC_BEHAV_LABELS[1]]["f1-score"]\n        accuracies[i] = report["accuracy"]\n    # Making figure\n    fig, ax = plt.subplots(figsize=(10, 7))\n    sns.lineplot(x=pcutoffs, y=precisions, label="precision", ax=ax)\n    sns.lineplot(x=pcutoffs, y=recalls, label="recall", ax=ax)\n    sns.lineplot(x=pcutoffs, y=f1, label="f1", ax=ax)\n    sns.lineplot(x=pcutoffs, y=accuracies, label="accuracy", ax=ax)\n    return fig\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.eval_report",
      title:
        "<code>eval_report(y_true, y_pred)</code>  <code>staticmethod</code>",
      text: '<p>summary</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>@staticmethod\ndef eval_report(y_true: pd.Series, y_pred: pd.Series) -&gt; dict:\n    """\n    __summary__\n    """\n    return classification_report(\n        y_true,\n        y_pred,\n        target_names=GENERIC_BEHAV_LABELS,\n        output_dict=True,\n    )\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.import_data",
      title: "<code>import_data(x_dir, y_dir, overwrite=False)</code>",
      text: '<p>Importing data from extracted features and labelled behaviours dataframes.</p> <p>Parameters:</p> Name Type Description Default <code>x_dir</code> <code>str</code> <p>description</p> required <code>y_dir</code> <code>str</code> <p>description</p> required Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>def import_data(self, x_dir: str, y_dir: str, overwrite=False) -&gt; None:\n    """\n    Importing data from extracted features and labelled behaviours dataframes.\n\n    Parameters\n    ----------\n    x_dir :\n        _description_\n    y_dir :\n        _description_\n    """\n    # For each x and y directory\n    for in_dir, id in ((x_dir, X_ID), (y_dir, Y_ID)):\n        out_dir = os.path.join(self.root_dir, id)\n        os.makedirs(out_dir, exist_ok=True)\n        # Copying each file to model root directory\n        for fp in os.listdir(in_dir):\n            in_fp = os.path.join(in_dir, fp)\n            out_fp = os.path.join(out_dir, fp)\n            # If not overwriting and out file already exists, then skip\n            if not overwrite and os.path.exists(out_fp):\n                continue\n            # Copying file\n            shutil.copyfile(in_fp, out_fp)\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.load",
      title: "<code>load(configs_fp)</code>  <code>classmethod</code>",
      text: '<p>Reads the model from the expected model file.</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>@classmethod\ndef load(cls, configs_fp: str) -&gt; BehavClassifier:\n    """\n    Reads the model from the expected model file.\n    """\n    if not os.path.isfile(configs_fp):\n        raise FileNotFoundError(f"The model file does not exist: {configs_fp}")\n    return cls(configs_fp)\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.pipeline_build",
      title: "<code>pipeline_build(clf_init_f)</code>",
      text: '<p>Makes a classifier and saves it to the model\'s root directory.</p> <p>Callable is a method from <code>ClfTemplates</code>.</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>def pipeline_build(self, clf_init_f: Callable) -&gt; None:\n    """\n    Makes a classifier and saves it to the model\'s root directory.\n\n    Callable is a method from `ClfTemplates`.\n    """\n    # Preparing data\n    x, y, ind_train, ind_test = self.prepare_data_training_pipeline()\n    # Initialising the model\n    self.clf = clf_init_f()\n    # Training the model\n    history = self.clf.fit(\n        x=x,\n        y=y,\n        index=ind_train,\n        batch_size=self.configs.batch_size,\n        epochs=self.configs.epochs,\n        val_split=self.configs.val_split,\n    )\n    # Saving history\n    self.clf_eval_save_history(history)\n    # Evaluating the model\n    self.clf_eval(x, y, ind_test)\n    # Updating the model configs\n    configs = self.configs\n    configs.clf_structure = clf_init_f.__name__\n    configs.write_json(self.configs_fp)\n    # Saving the model to disk\n    self.clf_save()\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.pipeline_run",
      title: "<code>pipeline_run(x)</code>",
      text: '<p>Given the unprocessed features dataframe, runs the model pipeline to make predictions.</p> <p>Pipeline is: - Preprocess <code>x</code> df. Refer to behavysis_pipeline.behav_classifier.BehavClassifier.preproc_x for details. - Makes predictions and returns the predicted behaviours.</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>def pipeline_run(self, x: pd.DataFrame) -&gt; pd.DataFrame:\n    """\n    Given the unprocessed features dataframe, runs the model pipeline to make predictions.\n\n    Pipeline is:\n    - Preprocess `x` df. Refer to\n    [behavysis_pipeline.behav_classifier.BehavClassifier.preproc_x][] for details.\n    - Makes predictions and returns the predicted behaviours.\n    """\n    # Saving index for later\n    index = x.index\n    # Preprocessing features\n    x = self.prepare_data(x)\n    # Loading the model\n    self.clf_load()\n    # Making predictions\n    y_eval = self.clf_predict(x, self.configs.batch_size)\n    # Settings the index\n    y_eval.index = index\n    # Returning predictions\n    return y_eval\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.prepare_data",
      title: "<code>prepare_data(x)</code>",
      text: '<p>Prepares novel (<code>x</code> only) data, given the <code>x</code> pd.DataFrame.</p> <p>Performs the following: - Preprocesses x df. Refer to <code>preprocess_x</code> for details. - Makes the X windowed array, for each index.</p> <p>Returns:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>Features array in the format: <code>(samples, window, features)</code></p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>def prepare_data(self, x: pd.DataFrame) -&gt; np.ndarray:\n    """\n    Prepares novel (`x` only) data, given the `x` pd.DataFrame.\n\n    Performs the following:\n    - Preprocesses x df. Refer to `preprocess_x` for details.\n    - Makes the X windowed array, for each index.\n\n    Returns\n    -------\n    x : np.ndarray\n        Features array in the format: `(samples, window, features)`\n    """\n    # Preprocessing x df\n    x = self.preproc_x(x, self.preproc_fp)\n    # Returning x\n    return x\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.prepare_data_training",
      title: "<code>prepare_data_training()</code>",
      text: '<p>Prepares the data (<code>x</code> and <code>y</code>) in the model for training. Data is taken from the model\'s <code>x</code> and <code>y</code> dirs.</p> <p>Performs the following: - Combining dfs from x and y directories (individual experiment data) - Ensures the x and y dfs have the same index, and are in the same row order - Preprocesses x df. Refer to <code>preprocess_x</code> for details. - Selects the y class (given in the configs file) from the y df. - Preprocesses y df. Refer to <code>preprocess_y</code> for details.</p> <p>Returns:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>Features array in the format: <code>(samples, window, features)</code></p> <code>y</code> <code>ndarray</code> <p>Outcomes array in the format: <code>(samples, class)</code></p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>def prepare_data_training(self) -&gt; tuple[np.ndarray, np.ndarray]:\n    """\n    Prepares the data (`x` and `y`) in the model for training.\n    Data is taken from the model\'s `x` and `y` dirs.\n\n    Performs the following:\n    - Combining dfs from x and y directories (individual experiment data)\n    - Ensures the x and y dfs have the same index, and are in the same row order\n    - Preprocesses x df. Refer to `preprocess_x` for details.\n    - Selects the y class (given in the configs file) from the y df.\n    - Preprocesses y df. Refer to `preprocess_y` for details.\n\n    Returns\n    -------\n    x : np.ndarray\n        Features array in the format: `(samples, window, features)`\n    y : np.ndarray\n        Outcomes array in the format: `(samples, class)`\n    """\n    # Combining dfs from x and y directories (individual experiment data)\n    x, y = self.combine_dfs()\n    # Fitting the preprocessor pipeline\n    self.preproc_x_fit(x, self.preproc_fp)\n    # Preprocessing x df\n    x = self.preproc_x(x, self.preproc_fp)\n    # Preprocessing y df\n    y = self.wrangle_columns_y(y)[self.configs.behaviour_name].values\n    y = self.preproc_y(y)\n    # Returning x, y, and index to use\n    return x, y\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.prepare_data_training_pipeline",
      title: "<code>prepare_data_training_pipeline()</code>",
      text: '<p>Prepares the data for the training pipeline.</p> <p>Performs the following: - Preprocesses <code>x</code> and <code>y</code> data. Refer to <code>prepare_data_training</code> for details. - Splits into training and test indexes.     - The training indexes are undersampled to the ratio given in the configs.</p> <p>Returns:     A tuple containing four numpy arrays:     - x: The input data.     - y: The target labels.     - ind_train: The indexes for the training data.     - ind_test: The indexes for the testing data.</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>def prepare_data_training_pipeline(\n    self,\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    """\n    Prepares the data for the training pipeline.\n\n    Performs the following:\n    - Preprocesses `x` and `y` data. Refer to `prepare_data_training` for details.\n    - Splits into training and test indexes.\n        - The training indexes are undersampled to the ratio given in the configs.\n\n    Returns:\n        A tuple containing four numpy arrays:\n        - x: The input data.\n        - y: The target labels.\n        - ind_train: The indexes for the training data.\n        - ind_test: The indexes for the testing data.\n    """\n    # Preparing data\n    x, y = self.prepare_data_training()\n    # Getting entire index\n    index = np.arange(x.shape[0])\n    # Splitting into train and test indexes\n    ind_train, ind_test = train_test_split(\n        index,\n        test_size=self.configs.test_split,\n        stratify=y[index],\n    )\n    # Undersampling training index\n    ind_train = self.undersample(\n        ind_train, y[ind_train], self.configs.undersample_ratio\n    )\n    # Return\n    return x, y, ind_train, ind_test\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.preproc_x",
      title: "<code>preproc_x(x, preproc_fp)</code>  <code>staticmethod</code>",
      text: '<p>The preprocessing steps are: - MinMax scaling (using previously fitted MinMaxScaler)</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>@staticmethod\ndef preproc_x(x: np.ndarray, preproc_fp: str) -&gt; np.ndarray:\n    """\n    The preprocessing steps are:\n    - MinMax scaling (using previously fitted MinMaxScaler)\n    """\n    # Loading in pipeline\n    preproc_pipe = joblib.load(preproc_fp)\n    # Uses trained fit for preprocessing new data\n    x = preproc_pipe.transform(x)\n    # Returning df\n    return x\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.preproc_x_fit",
      title:
        "<code>preproc_x_fit(x, preproc_fp)</code>  <code>staticmethod</code>",
      text: '<p>summary</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>@staticmethod\ndef preproc_x_fit(x: np.ndarray, preproc_fp: str) -&gt; None:\n    """\n    __summary__\n    """\n    # Making pipeline\n    preproc_pipe = Pipeline(steps=[("MinMaxScaler", MinMaxScaler())])\n    # Fitting pipeline\n    preproc_pipe.fit(x)\n    # Saving pipeline\n    joblib.dump(preproc_pipe, preproc_fp)\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.preproc_y",
      title: "<code>preproc_y(y)</code>  <code>staticmethod</code>",
      text: '<p>The preprocessing steps are: - Imputing NaN values with 0 - Setting -1 to 0 - Converting the MultiIndex columns from <code>(behav, outcome)</code> to <code>{behav}__{outcome}</code>, by expanding the <code>actual</code> and all specific outcome columns of each behav.</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>@staticmethod\ndef preproc_y(y: np.ndarray) -&gt; np.ndarray:\n    """\n    The preprocessing steps are:\n    - Imputing NaN values with 0\n    - Setting -1 to 0\n    - Converting the MultiIndex columns from `(behav, outcome)` to `{behav}__{outcome}`,\n    by expanding the `actual` and all specific outcome columns of each behav.\n    """\n    # Imputing NaN values with 0\n    y = np.nan_to_num(y, nan=0)\n    # Setting -1 to 0 (i.e. "undecided" to "no behaviour")\n    y = np.maximum(y, 0)\n    # Returning arr\n    return y\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifier.wrangle_columns_y",
      title: "<code>wrangle_columns_y(y)</code>  <code>staticmethod</code>",
      text: '<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>DataFrame</code> <p>description</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>description</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>@staticmethod\ndef wrangle_columns_y(y: pd.DataFrame) -&gt; pd.DataFrame:\n    """\n    _summary_\n\n    Parameters\n    ----------\n    y :\n        _description_\n\n    Returns\n    -------\n    :\n        _description_\n    """\n    # Filtering out the prob and pred columns (in the `outcomes` level)\n    cols_filter = np.isin(\n        y.columns.get_level_values(BehavCN.OUTCOMES.value),\n        [BehavColumns.PROB.value, BehavColumns.PRED.value],\n        invert=True,\n    )\n    y = y.loc[:, cols_filter]\n    # Converting MultiIndex columns to single columns by\n    # setting the column names from `(behav, outcome)` to `{behav}__{outcome}`\n    y.columns = [\n        f"{i[0]}" if i[1] == BehavColumns.ACTUAL.value else f"{i[0]}__{i[1]}"\n        for i in y.columns\n    ]\n    return y\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier.html#behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifierConfigs",
      title:
        "<code>behavysis_pipeline.behav_classifier.behav_classifier.BehavClassifierConfigs</code>",
      text: '<p>               Bases: <code>PydanticBaseModel</code></p> <p>summary</p> Source code in <code>behavysis_pipeline/behav_classifier/behav_classifier.py</code> <pre><code>class BehavClassifierConfigs(PydanticBaseModel):\n    """_summary_"""\n\n    model_config = ConfigDict(extra="forbid")\n\n    behaviour_name: str = "BehaviourName"\n    seed: int = 42\n    undersample_ratio: float = 0.1\n\n    clf_structure: str = "clf"  # Classifier type (defined in ClfTemplates)\n    pcutoff: float = 0.5\n    test_split: float = 0.2\n    val_split: float = 0.2\n    batch_size: int = 256\n    epochs: int = 50\n</code></pre>',
    },
    {
      location: "reference/behav_classifier_templates.html",
      title: "Behav classifier templates",
      text: "",
    },
    {
      location:
        "reference/behav_classifier_templates.html#behavysis_pipeline.behav_classifier.clf_templates",
      title: "<code>behavysis_pipeline.behav_classifier.clf_templates</code>",
      text: "<p>summary</p>",
    },
    {
      location:
        "reference/behav_classifier_templates.html#behavysis_pipeline.behav_classifier.clf_templates.CNN1",
      title: "<code>CNN1</code>",
      text: '<p>               Bases: <code>BaseTorchModel</code></p> <p>x features is (samples, window, features). y outcome is (samples, class).</p> Source code in <code>behavysis_pipeline/behav_classifier/clf_templates.py</code> <pre><code>class CNN1(BaseTorchModel):\n    """\n    x features is (samples, window, features).\n    y outcome is (samples, class).\n    """\n\n    def __init__(self):\n        # Initialising the parent class\n        super().__init__(546, 10)\n        # Define the layers\n        self.conv1 = nn.Conv1d(self.nfeatures, 64, kernel_size=2)\n        self.relu1 = nn.ReLU()\n        # self.maxpool1 = nn.MaxPool1d(kernel_size=2)\n        self.flatten = nn.Flatten()\n\n        flat_size = self.window_frames * 2 + 1\n        flat_size = flat_size - 1\n        # flat_size = (flat_size - 2) // 2\n        flat_size = flat_size * 64\n\n        self.fc1 = nn.Linear(flat_size, 64)\n        self.relu3 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 1)\n        self.sigmoid1 = nn.Sigmoid()\n        # Define the loss function and optimizer\n        self.criterion: nn.Module = nn.BCELoss()\n        self.optimizer: optim.Optimizer = optim.Adam(self.parameters())\n        # Setting the device (GPU or CPU)\n        self.device = self.device\n\n    def forward(self, x):\n        out = x\n        out = self.conv1(out)\n        out = self.relu1(out)\n        # out = self.maxpool1(out)\n        out = self.flatten(out)\n        out = self.fc1(out)\n        out = self.relu3(out)\n        out = self.dropout1(out)\n        out = self.fc2(out)\n        out = self.sigmoid1(out)\n        return out\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier_templates.html#behavysis_pipeline.behav_classifier.clf_templates.CNN2",
      title: "<code>CNN2</code>",
      text: '<p>               Bases: <code>BaseTorchModel</code></p> <p>x features is (samples, window, features). y outcome is (samples, class).</p> Source code in <code>behavysis_pipeline/behav_classifier/clf_templates.py</code> <pre><code>class CNN2(BaseTorchModel):\n    """\n    x features is (samples, window, features).\n    y outcome is (samples, class).\n    """\n\n    def __init__(self):\n        # Initialising the parent class\n        super().__init__(546, 10)\n        # Define the layers\n        self.conv1 = nn.Conv1d(self.nfeatures, 64, kernel_size=3)\n        self.relu1 = nn.ReLU()\n        self.maxpool1 = nn.MaxPool1d(kernel_size=2)\n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3)\n        self.relu2 = nn.ReLU()\n        self.maxpool2 = nn.MaxPool1d(kernel_size=2)\n        self.flatten = nn.Flatten()\n\n        flat_size = self.window_frames * 2 + 1\n        flat_size = (flat_size - 2) // 2\n        flat_size = (flat_size - 2) // 2\n        flat_size = flat_size * 32\n\n        self.fc1 = nn.Linear(flat_size, 64)\n        self.relu3 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 1)\n        self.sigmoid1 = nn.Sigmoid()\n        # Define the loss function and optimizer\n        self.criterion: nn.Module = nn.BCELoss()\n        self.optimizer: optim.Optimizer = optim.Adam(self.parameters())\n        # Setting the device (GPU or CPU)\n        self.device = self.device\n\n    def forward(self, x):\n        out = x\n        out = self.conv1(out)\n        out = self.relu1(out)\n        out = self.maxpool1(out)\n        out = self.conv2(out)\n        out = self.relu2(out)\n        out = self.maxpool2(out)\n        out = self.flatten(out)\n        out = self.fc1(out)\n        out = self.relu3(out)\n        out = self.dropout1(out)\n        out = self.fc2(out)\n        out = self.sigmoid1(out)\n        return out\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier_templates.html#behavysis_pipeline.behav_classifier.clf_templates.DNN1",
      title: "<code>DNN1</code>",
      text: '<p>               Bases: <code>BaseTorchModel</code></p> <p>x features is (samples, window, features). y outcome is (samples, class).</p> Source code in <code>behavysis_pipeline/behav_classifier/clf_templates.py</code> <pre><code>class DNN1(BaseTorchModel):\n    """\n    x features is (samples, window, features).\n    y outcome is (samples, class).\n    """\n\n    def __init__(self):\n        # Initialising the parent class\n        super().__init__(546, 0)\n        # Input shape\n        flat_size = self.window_frames * 2 + 1\n        flat_size = flat_size * self.nfeatures\n        # Define the layers\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(flat_size, 64)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 1)\n        self.sigmoid1 = nn.Sigmoid()\n        # Define the loss function and optimizer\n        self.criterion: nn.Module = nn.BCELoss()\n        self.optimizer: optim.Optimizer = optim.Adam(self.parameters())\n        # Setting the device (GPU or CPU)\n        self.device = self.device\n\n    def forward(self, x):\n        out = x\n        out = self.flatten(out)\n        out = self.fc1(out)\n        out = self.relu1(out)\n        out = self.dropout1(out)\n        out = self.fc2(out)\n        out = self.sigmoid1(out)\n        return out\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier_templates.html#behavysis_pipeline.behav_classifier.clf_templates.DNN2",
      title: "<code>DNN2</code>",
      text: '<p>               Bases: <code>BaseTorchModel</code></p> <p>x features is (samples, window, features). y outcome is (samples, class).</p> Source code in <code>behavysis_pipeline/behav_classifier/clf_templates.py</code> <pre><code>class DNN2(BaseTorchModel):\n    """\n    x features is (samples, window, features).\n    y outcome is (samples, class).\n    """\n\n    def __init__(self):\n        # Initialising the parent class\n        super().__init__(546, 0)\n        # Input shape\n        flat_size = self.window_frames * 2 + 1\n        flat_size = flat_size * self.nfeatures\n        # Define the layers\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(flat_size, 32)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(32, 1)\n        self.sigmoid1 = nn.Sigmoid()\n        # Define the loss function and optimizer\n        self.criterion: nn.Module = nn.BCELoss()\n        self.optimizer: optim.Optimizer = optim.Adam(self.parameters())\n        # Setting the device (GPU or CPU)\n        self.device = self.device\n\n    def forward(self, x):\n        out = x\n        out = self.flatten(out)\n        out = self.fc1(out)\n        out = self.relu1(out)\n        out = self.dropout1(out)\n        out = self.fc2(out)\n        out = self.sigmoid1(out)\n        return out\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier_templates.html#behavysis_pipeline.behav_classifier.clf_templates.DNN3",
      title: "<code>DNN3</code>",
      text: '<p>               Bases: <code>BaseTorchModel</code></p> <p>x features is (samples, window, features). y outcome is (samples, class).</p> Source code in <code>behavysis_pipeline/behav_classifier/clf_templates.py</code> <pre><code>class DNN3(BaseTorchModel):\n    """\n    x features is (samples, window, features).\n    y outcome is (samples, class).\n    """\n\n    def __init__(self):\n        # Initialising the parent class\n        super().__init__(546, 0)\n        # Input shape\n        flat_size = self.window_frames * 2 + 1\n        flat_size = flat_size * self.nfeatures\n        # Define the layers\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(flat_size, 256)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(256, 64)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(64, 1)\n        self.sigmoid1 = nn.Sigmoid()\n        # Define the loss function and optimizer\n        self.criterion: nn.Module = nn.BCELoss()\n        self.optimizer: optim.Optimizer = optim.Adam(self.parameters())\n        # Setting the device (GPU or CPU)\n        self.device = self.device\n\n    def forward(self, x):\n        out = x\n        out = self.flatten(out)\n        out = self.fc1(out)\n        out = self.relu1(out)\n        out = self.dropout1(out)\n        out = self.fc2(out)\n        out = self.relu2(out)\n        out = self.dropout2(out)\n        out = self.fc3(out)\n        out = self.sigmoid1(out)\n        return out\n</code></pre>',
    },
    {
      location:
        "reference/behav_classifier_templates.html#behavysis_pipeline.behav_classifier.clf_templates.RF1",
      title: "<code>RF1</code>",
      text: '<p>               Bases: <code>RandomForestClassifier</code></p> <p>x features is (samples, features). y outcome is (samples, class).</p> Source code in <code>behavysis_pipeline/behav_classifier/clf_templates.py</code> <pre><code>class RF1(RandomForestClassifier):\n    """\n    x features is (samples, features).\n    y outcome is (samples, class).\n    """\n\n    def __init__(self):\n        super().__init__(\n            n_estimators=2000,\n            max_depth=3,\n            random_state=0,\n            n_jobs=16,\n            verbose=1,\n        )\n\n    def fit(self, x, y, *args, **kwargs):\n        super().fit(x, y)\n        return self\n\n    def predict(self, x, *args, **kwargs):\n        return super().predict_proba(x)[:, 1]\n</code></pre>',
    },
    {
      location: "reference/behavysis_pipeline.html",
      title: "Behavysis pipeline",
      text: "",
    },
    {
      location: "reference/behavysis_pipeline.html#behavysis_pipeline",
      title: "<code>behavysis_pipeline</code>",
      text: "<p>This package is used to interprets and interprets lab mice behaviour using computer vision. The package allows users to perform the entire analytics pipeline from raw lab footage to interpretable plotted and tabulated data for different analysises. This pipeline includes:</p> <ul> <li>Formatting raw videos to a desired mp4 format (e.g. user defined fps and resolution)</li> <li>Performing stance detection on the mp4 file to generate an annotated mp4 file and file that tabulates the x-y coordinates of the subject's body points in each video frame. DeepLabCut is used to perform this.</li> <li>Preprocessing the coordinates file</li> <li>Extracting meaningful data analysis from the preprocessed coordinates file</li> </ul>",
    },
    { location: "reference/experiment.html", title: "Experiment", text: "" },
    {
      location:
        "reference/experiment.html#behavysis_pipeline.pipeline.experiment.Experiment",
      title: "<code>behavysis_pipeline.pipeline.experiment.Experiment</code>",
      text: '<p>Behavysis Pipeline class for a single experiment.</p> <p>Encompasses the entire process including: - Raw mp4 file import. - mp4 file formatting (px and fps). - DLC keypoints inference. - Feature wrangling (start time detection, more features like average body position). - Interpretable behaviour results. - Other quantitative analysis.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>description</p> required <code>root_dir</code> <code>str</code> <p>description</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>ValueError: <code>root_dir</code> does not exist or <code>name</code> does not exist in the <code>root_dir</code> folder.</p> Source code in <code>behavysis_pipeline/pipeline/experiment.py</code> <pre><code>class Experiment:\n    """\n    Behavysis Pipeline class for a single experiment.\n\n    Encompasses the entire process including:\n    - Raw mp4 file import.\n    - mp4 file formatting (px and fps).\n    - DLC keypoints inference.\n    - Feature wrangling (start time detection, more features like average body position).\n    - Interpretable behaviour results.\n    - Other quantitative analysis.\n\n    Parameters\n    ----------\n    name : str\n        _description_\n    root_dir : str\n        _description_\n\n    Raises\n    ------\n    ValueError\n        ValueError: `root_dir` does not exist or `name` does not exist in the `root_dir` folder.\n    """\n\n    def __init__(self, name: str, root_dir: str) -&gt; None:\n        """\n        Make a Experiment instance.\n        """\n        # Assertion: root_dir mus\u2020 exist\n        if not os.path.isdir(root_dir):\n            raise ValueError(\n                f\'Cannot find the project folder named "{root_dir}".\\n\'\n                + "Please specify a folder that exists."\n            )\n        # Assertion: name must correspond to at least one file in root_dir\n        file_exists_ls = [\n            os.path.isfile(os.path.join(root_dir, f.value, f"{name}{FILE_EXTS[f]}"))\n            for f in Folders\n        ]\n        if not np.any(file_exists_ls):\n            raise ValueError(\n                f\'No files named "{name}" exist in "{root_dir}".\\n\'\n                + f\'Please specify a file that exists in "{root_dir}", in one of the\'\n                + " following folder WITH the correct file extension name:\\n"\n                + "    - "\n                + "\\n    - ".join(DFMixin.enum_to_list(Folders))\n            )\n        self.name = name\n        self.root_dir = os.path.abspath(root_dir)\n\n    #####################################################################\n    #               GET/CHECK FILEPATH METHODS\n    #####################################################################\n\n    def get_fp(self, folder_str: str) -&gt; str:\n        """\n        Returns the experiment\'s file path from the given folder.\n\n        Parameters\n        ----------\n        folder_str : str\n            The folder to return the experiment document\'s filepath for.\n\n        Returns\n        -------\n        str\n            The experiment document\'s filepath.\n\n        Raises\n        ------\n        ValueError\n            ValueError: Folder name is not valid. Refer to FOLDERS constant for valid folder names.\n        """\n        # Getting folder enum from string\n        folder = next((f for f in Folders if folder_str == f.value), None)\n        # Assertion: The given folder name must be valid\n        if not folder:\n            raise ValueError(\n                f\'"{folder_str}" is not a valid experiment folder name.\\n\'\n                + "Please only specify one of the following folders:\\n"\n                + "    - "\n                + "\\n    - ".join([f.value for f in Folders])\n            )\n        # Getting experiment filepath for given folder\n        fp = os.path.join(\n            self.root_dir, folder.value, f"{self.name}{FILE_EXTS[folder]}"\n        )\n        # Making a folder if it does not exist\n        os.makedirs(os.path.split(fp)[0], exist_ok=True)\n        # Returning filepath\n        return fp\n\n    #####################################################################\n    #               EXPERIMENT PROCESSING SCAFFOLD METHODS\n    #####################################################################\n\n    def _process_scaffold(\n        self,\n        funcs: tuple[Callable, ...],\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; dict[str, str]:\n        """\n        All processing runs through here.\n        This method ensures that the stdout and diagnostics dict are correctly generated.\n\n        Parameters\n        ----------\n        funcs : tuple[Callable, ...]\n            List of functions.\n\n        Returns\n        -------\n        dict[str, str]\n            Diagnostics dictionary, with description of each function\'s outcome.\n\n        Notes\n        -----\n        Each func in `funcs` is called in the form:\n        ```\n        func(*args, **kwargs)\n        ```\n        """\n        logging.info(f"Processing experiment: {self.name}")\n        # Setting up diagnostics dict\n        dd = {"experiment": self.name}\n        # Running functions and saving outcome to diagnostics dict\n        for f in funcs:\n            # Running each func and saving outcome\n            try:\n                dd[f.__name__] = f(*args, **kwargs)\n                dd[f.__name__] += f"SUCCESS: {DiagnosticsMixin.success_msg()}\\n"\n            except Exception as e:\n                dd[f.__name__] = f"ERROR: {e}"\n            logging.info(f"{f.__name__}: {dd[f.__name__]}")\n        logging.info(STR_DIV)\n        return dd\n\n    #####################################################################\n    #                        CONFIG FILE METHODS\n    #####################################################################\n\n    def update_configs(self, default_configs_fp: str, overwrite: str) -&gt; dict:\n        """\n        Initialises the JSON config files with the given configurations in `configs`.\n        It can be specified whether or not to overwrite existing configuration values.\n\n        Parameters\n        ----------\n        default_configs_fp : str\n            The JSON configs filepath to add/overwrite to the experiment\'s current configs file.\n        overwrite : {"set", "reset"}\n            Specifies how to overwrite existing configurations.\n            If `add`, only parameters in `configs` not already in the config files are added.\n            If `set`, all parameters in `configs` are set in the config files (overwriting).\n            If `reset`, the config files are completely replaced by `configs`.\n\n        Returns\n        -------\n        dict\n            Diagnostics dictionary, with description of each function\'s outcome.\n        """\n        return self._process_scaffold(\n            (UpdateConfigs.update_configs,),\n            configs_fp=self.get_fp(Folders.CONFIGS.value),\n            default_configs_fp=default_configs_fp,\n            overwrite=overwrite,\n        )\n\n    #####################################################################\n    #                    FORMATTING VIDEO METHODS\n    #####################################################################\n\n    def format_vid(self, funcs: tuple[Callable, ...], overwrite: bool) -&gt; dict:\n        """\n        Formats the video with ffmpeg to fit the formatted configs (e.g. fps and resolution_px).\n        Once the formatted video is produced, the configs dict and *configs.json file are\n        updated with the formatted video\'s metadata.\n\n        Parameters\n        ----------\n        funcs : tuple[Callable, ...]\n            _description_\n        overwrite : bool\n            Whether to overwrite the output file (if it exists).\n\n        Returns\n        -------\n        dict\n            Diagnostics dictionary, with description of each function\'s outcome.\n\n        Notes\n        -----\n        Can call any methods from `FormatVid`.\n        """\n        return self._process_scaffold(\n            funcs,\n            in_fp=self.get_fp(Folders.RAW_VID.value),\n            out_fp=self.get_fp(Folders.FORMATTED_VID.value),\n            configs_fp=self.get_fp(Folders.CONFIGS.value),\n            overwrite=overwrite,\n        )\n\n    #####################################################################\n    #                        DLC METHODS\n    #####################################################################\n\n    def run_dlc(self, gputouse: int, overwrite: bool) -&gt; dict:\n        """\n        Run the DLC model on the formatted video to generate a DLC annotated video\n        and DLC h5 file for all experiments.\n\n        Parameters\n        ----------\n        gputouse : int\n            _description_\n        overwrite : bool\n            Whether to overwrite the output file (if it exists).\n\n        Returns\n        -------\n        dict\n            Diagnostics dictionary, with description of each function\'s outcome.\n\n        Notes\n        -----\n        Can call any methods from `RunDLC`.\n        """\n        return self._process_scaffold(\n            (RunDLC.ma_dlc_analyse_single,),\n            in_fp=self.get_fp(Folders.FORMATTED_VID.value),\n            out_fp=self.get_fp(Folders.DLC.value),\n            configs_fp=self.get_fp(Folders.CONFIGS.value),\n            temp_dir=os.path.join(self.root_dir, TEMP_DIR),\n            gputouse=gputouse,\n            overwrite=overwrite,\n        )\n\n    def calculate_params(self, funcs: tuple[Callable, ...]) -&gt; dict:\n        """\n        A pipeline to calculate the parameters of the raw DLC file, which will\n        assist in preprocessing the DLC data.\n\n        Parameters\n        ----------\n        funcs : Tuple[Callable, ...]\n            _description_\n\n        Returns\n        -------\n        Dict\n            Diagnostics dictionary, with description of each function\'s outcome.\n\n        Notes\n        -----\n        Can call any methods from `CalculateParams`.\n        """\n        return self._process_scaffold(\n            funcs,\n            dlc_fp=self.get_fp(Folders.DLC.value),\n            configs_fp=self.get_fp(Folders.CONFIGS.value),\n        )\n\n    def preprocess(self, funcs: tuple[Callable, ...], overwrite: bool) -&gt; dict:\n        """\n        A preprocessing pipeline method to convert raw DLC data into preprocessed\n        DLC data that is ready for ML analysis.\n        All functs passed in must have the format func(df, dict) -&gt; df. Possible funcs\n        are given in preprocessing.py\n        The preprocessed data is saved to the project\'s preprocessed folder.\n\n        Parameters\n        ----------\n        funcs : tuple[Callable, ...]\n            _description_\n        overwrite : bool\n            Whether to overwrite the output file (if it exists).\n\n        Returns\n        -------\n        dict\n            Diagnostics dictionary, with description of each function\'s outcome.\n\n        Notes\n        -----\n        Can call any methods from `Preprocess`.\n        """\n        # Exporting 3_dlc df to 4_preprocessed folder\n        dd = self._process_scaffold(\n            (Export.feather_2_feather,),\n            src_fp=self.get_fp(Folders.DLC.value),\n            out_fp=self.get_fp(Folders.PREPROCESSED.value),\n            overwrite=overwrite,\n        )\n        # If there is an error, OR warning (indicates not to ovewrite), then return early\n        res = dd["feather_2_feather"]\n        if res.startswith("ERROR") or res.startswith("WARNING"):\n            return dd\n        # Feeding through preprocessing functions\n        return self._process_scaffold(\n            funcs,\n            in_fp=self.get_fp(Folders.PREPROCESSED.value),\n            out_fp=self.get_fp(Folders.PREPROCESSED.value),\n            configs_fp=self.get_fp(Folders.CONFIGS.value),\n            overwrite=True,\n        )\n\n    #####################################################################\n    #                 SIMBA BEHAVIOUR CLASSIFICATION METHODS\n    #####################################################################\n\n    def extract_features(self, overwrite: bool) -&gt; dict:\n        """\n        Extracts features from the preprocessed dlc file to generate many more features.\n        This dataframe of derived features will be input for a ML classifier to detect\n        particularly trained behaviours.\n\n        Parameters\n        ----------\n        overwrite : bool\n            Whether to overwrite the output file (if it exists).\n\n        Returns\n        -------\n        dict\n            Diagnostics dictionary, with description of each function\'s outcome.\n        """\n        return self._process_scaffold(\n            (ExtractFeatures.extract_features,),\n            dlc_fp=self.get_fp(Folders.PREPROCESSED.value),\n            out_fp=self.get_fp(Folders.FEATURES_EXTRACTED.value),\n            configs_fp=self.get_fp(Folders.CONFIGS.value),\n            temp_dir=os.path.join(self.root_dir, TEMP_DIR),\n            overwrite=overwrite,\n        )\n\n    def classify_behaviours(self, overwrite: bool) -&gt; dict:\n        """\n        Given model config files in the BehavClassifier format, generates beahviour predidctions\n        on the given extracted features dataframe.\n\n        Parameters\n        ----------\n        overwrite : bool\n            Whether to overwrite the output file (if it exists).\n\n        Returns\n        -------\n        dict\n            Diagnostics dictionary, with description of each function\'s outcome.\n        """\n        return self._process_scaffold(\n            (ClassifyBehaviours.classify_behaviours,),\n            features_fp=self.get_fp(Folders.FEATURES_EXTRACTED.value),\n            out_fp=self.get_fp(Folders.PREDICTED_BEHAVS.value),\n            configs_fp=self.get_fp(Folders.CONFIGS.value),\n            overwrite=overwrite,\n        )\n\n    def export_behaviours(self, overwrite: bool) -&gt; dict:\n        """\n        _summary_\n\n        Parameters\n        ----------\n        overwrite : bool\n            _description_\n\n        Returns\n        -------\n        dict\n            _description_\n        """\n        # Exporting 6_predicted_behavs df to 7_scored_behavs folder\n        return self._process_scaffold(\n            (Export.predbehav_2_scoredbehav,),\n            src_fp=self.get_fp(Folders.PREDICTED_BEHAVS.value),\n            out_fp=self.get_fp(Folders.SCORED_BEHAVS.value),\n            configs_fp=self.get_fp(Folders.CONFIGS.value),\n            overwrite=overwrite,\n        )\n\n    #####################################################################\n    #                     SIMPLE ANALYSIS METHODS\n    #####################################################################\n\n    def analyse(self, funcs: tuple[Callable, ...]) -&gt; dict:\n        """\n        An ML pipeline method to analyse the preprocessed DLC data.\n        Possible funcs are given in analysis.py.\n        The preprocessed data is saved to the project\'s analysis folder.\n\n        Parameters\n        ----------\n        funcs : tuple[Callable, ...]\n            _description_\n\n        Returns\n        -------\n        dict\n            Diagnostics dictionary, with description of each function\'s outcome.\n\n        Notes\n        -----\n        Can call any methods from `Analyse`.\n        """\n        return self._process_scaffold(\n            funcs,\n            dlc_fp=self.get_fp(Folders.PREPROCESSED.value),\n            ANALYSE_DIR=os.path.join(self.root_dir, ANALYSE_DIR),\n            configs_fp=self.get_fp(Folders.CONFIGS.value),\n        )\n\n    def behav_analyse(self) -&gt; dict:\n        """\n        An ML pipeline method to analyse the preprocessed DLC data.\n        Possible funcs are given in analysis.py.\n        The preprocessed data is saved to the project\'s analysis folder.\n\n        Returns\n        -------\n        dict\n            Diagnostics dictionary, with description of each function\'s outcome.\n\n        Notes\n        -----\n        Can call any methods from `Analyse`.\n        """\n        return self._process_scaffold(\n            (BehavAnalyse.behav_analysis,),\n            behavs_fp=self.get_fp(Folders.SCORED_BEHAVS.value),\n            ANALYSE_DIR=os.path.join(self.root_dir, ANALYSE_DIR),\n            configs_fp=self.get_fp(Folders.CONFIGS.value),\n        )\n\n    #####################################################################\n    #           EVALUATING DLC ANALYSIS AND BEHAV CLASSIFICATION\n    #####################################################################\n\n    def export_feather(self, in_dir: str, out_dir: str, overwrite: bool) -&gt; dict:\n        """\n        _summary_\n\n        Parameters\n        ----------\n        in_dir : str\n            _description_\n        out_dir : str\n            _description_\n\n        Returns\n        -------\n        dict\n            _description_\n        """\n        return self._process_scaffold(\n            (Export.feather_2_csv,),\n            in_fp=self.get_fp(in_dir),\n            out_fp=os.path.join(out_dir, f"{self.name}.csv"),\n            overwrite=overwrite,\n        )\n\n    def evaluate(self, funcs, overwrite: bool) -&gt; dict:\n        """\n        Evaluating preprocessed DLC data and scored_behavs data.\n\n        Parameters\n        ----------\n        funcs : _type_\n            _description_\n        overwrite : bool\n            Whether to overwrite the output file (if it exists).\n\n        Returns\n        -------\n        dict\n            Diagnostics dictionary, with description of each function\'s outcome.\n        """\n        return self._process_scaffold(\n            funcs,\n            vid_fp=self.get_fp(Folders.FORMATTED_VID.value),\n            dlc_fp=self.get_fp(Folders.PREPROCESSED.value),\n            behavs_fp=self.get_fp(Folders.SCORED_BEHAVS.value),\n            out_dir=os.path.join(self.root_dir, EVALUATE_DIR),\n            configs_fp=self.get_fp(Folders.CONFIGS.value),\n            overwrite=overwrite,\n        )\n</code></pre>',
    },
    {
      location:
        "reference/experiment.html#behavysis_pipeline.pipeline.experiment.Experiment.__init__",
      title: "<code>__init__(name, root_dir)</code>",
      text: '<p>Make a Experiment instance.</p> Source code in <code>behavysis_pipeline/pipeline/experiment.py</code> <pre><code>def __init__(self, name: str, root_dir: str) -&gt; None:\n    """\n    Make a Experiment instance.\n    """\n    # Assertion: root_dir mus\u2020 exist\n    if not os.path.isdir(root_dir):\n        raise ValueError(\n            f\'Cannot find the project folder named "{root_dir}".\\n\'\n            + "Please specify a folder that exists."\n        )\n    # Assertion: name must correspond to at least one file in root_dir\n    file_exists_ls = [\n        os.path.isfile(os.path.join(root_dir, f.value, f"{name}{FILE_EXTS[f]}"))\n        for f in Folders\n    ]\n    if not np.any(file_exists_ls):\n        raise ValueError(\n            f\'No files named "{name}" exist in "{root_dir}".\\n\'\n            + f\'Please specify a file that exists in "{root_dir}", in one of the\'\n            + " following folder WITH the correct file extension name:\\n"\n            + "    - "\n            + "\\n    - ".join(DFMixin.enum_to_list(Folders))\n        )\n    self.name = name\n    self.root_dir = os.path.abspath(root_dir)\n</code></pre>',
    },
    {
      location:
        "reference/experiment.html#behavysis_pipeline.pipeline.experiment.Experiment.analyse",
      title: "<code>analyse(funcs)</code>",
      text: '<p>An ML pipeline method to analyse the preprocessed DLC data. Possible funcs are given in analysis.py. The preprocessed data is saved to the project\'s analysis folder.</p> <p>Parameters:</p> Name Type Description Default <code>funcs</code> <code>tuple[Callable, ...]</code> <p>description</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Diagnostics dictionary, with description of each function\'s outcome.</p> Notes <p>Can call any methods from <code>Analyse</code>.</p> Source code in <code>behavysis_pipeline/pipeline/experiment.py</code> <pre><code>def analyse(self, funcs: tuple[Callable, ...]) -&gt; dict:\n    """\n    An ML pipeline method to analyse the preprocessed DLC data.\n    Possible funcs are given in analysis.py.\n    The preprocessed data is saved to the project\'s analysis folder.\n\n    Parameters\n    ----------\n    funcs : tuple[Callable, ...]\n        _description_\n\n    Returns\n    -------\n    dict\n        Diagnostics dictionary, with description of each function\'s outcome.\n\n    Notes\n    -----\n    Can call any methods from `Analyse`.\n    """\n    return self._process_scaffold(\n        funcs,\n        dlc_fp=self.get_fp(Folders.PREPROCESSED.value),\n        ANALYSE_DIR=os.path.join(self.root_dir, ANALYSE_DIR),\n        configs_fp=self.get_fp(Folders.CONFIGS.value),\n    )\n</code></pre>',
    },
    {
      location:
        "reference/experiment.html#behavysis_pipeline.pipeline.experiment.Experiment.behav_analyse",
      title: "<code>behav_analyse()</code>",
      text: '<p>An ML pipeline method to analyse the preprocessed DLC data. Possible funcs are given in analysis.py. The preprocessed data is saved to the project\'s analysis folder.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Diagnostics dictionary, with description of each function\'s outcome.</p> Notes <p>Can call any methods from <code>Analyse</code>.</p> Source code in <code>behavysis_pipeline/pipeline/experiment.py</code> <pre><code>def behav_analyse(self) -&gt; dict:\n    """\n    An ML pipeline method to analyse the preprocessed DLC data.\n    Possible funcs are given in analysis.py.\n    The preprocessed data is saved to the project\'s analysis folder.\n\n    Returns\n    -------\n    dict\n        Diagnostics dictionary, with description of each function\'s outcome.\n\n    Notes\n    -----\n    Can call any methods from `Analyse`.\n    """\n    return self._process_scaffold(\n        (BehavAnalyse.behav_analysis,),\n        behavs_fp=self.get_fp(Folders.SCORED_BEHAVS.value),\n        ANALYSE_DIR=os.path.join(self.root_dir, ANALYSE_DIR),\n        configs_fp=self.get_fp(Folders.CONFIGS.value),\n    )\n</code></pre>',
    },
    {
      location:
        "reference/experiment.html#behavysis_pipeline.pipeline.experiment.Experiment.calculate_params",
      title: "<code>calculate_params(funcs)</code>",
      text: '<p>A pipeline to calculate the parameters of the raw DLC file, which will assist in preprocessing the DLC data.</p> <p>Parameters:</p> Name Type Description Default <code>funcs</code> <code>Tuple[Callable, ...]</code> <p>description</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>Diagnostics dictionary, with description of each function\'s outcome.</p> Notes <p>Can call any methods from <code>CalculateParams</code>.</p> Source code in <code>behavysis_pipeline/pipeline/experiment.py</code> <pre><code>def calculate_params(self, funcs: tuple[Callable, ...]) -&gt; dict:\n    """\n    A pipeline to calculate the parameters of the raw DLC file, which will\n    assist in preprocessing the DLC data.\n\n    Parameters\n    ----------\n    funcs : Tuple[Callable, ...]\n        _description_\n\n    Returns\n    -------\n    Dict\n        Diagnostics dictionary, with description of each function\'s outcome.\n\n    Notes\n    -----\n    Can call any methods from `CalculateParams`.\n    """\n    return self._process_scaffold(\n        funcs,\n        dlc_fp=self.get_fp(Folders.DLC.value),\n        configs_fp=self.get_fp(Folders.CONFIGS.value),\n    )\n</code></pre>',
    },
    {
      location:
        "reference/experiment.html#behavysis_pipeline.pipeline.experiment.Experiment.classify_behaviours",
      title: "<code>classify_behaviours(overwrite)</code>",
      text: '<p>Given model config files in the BehavClassifier format, generates beahviour predidctions on the given extracted features dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file (if it exists).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Diagnostics dictionary, with description of each function\'s outcome.</p> Source code in <code>behavysis_pipeline/pipeline/experiment.py</code> <pre><code>def classify_behaviours(self, overwrite: bool) -&gt; dict:\n    """\n    Given model config files in the BehavClassifier format, generates beahviour predidctions\n    on the given extracted features dataframe.\n\n    Parameters\n    ----------\n    overwrite : bool\n        Whether to overwrite the output file (if it exists).\n\n    Returns\n    -------\n    dict\n        Diagnostics dictionary, with description of each function\'s outcome.\n    """\n    return self._process_scaffold(\n        (ClassifyBehaviours.classify_behaviours,),\n        features_fp=self.get_fp(Folders.FEATURES_EXTRACTED.value),\n        out_fp=self.get_fp(Folders.PREDICTED_BEHAVS.value),\n        configs_fp=self.get_fp(Folders.CONFIGS.value),\n        overwrite=overwrite,\n    )\n</code></pre>',
    },
    {
      location:
        "reference/experiment.html#behavysis_pipeline.pipeline.experiment.Experiment.evaluate",
      title: "<code>evaluate(funcs, overwrite)</code>",
      text: '<p>Evaluating preprocessed DLC data and scored_behavs data.</p> <p>Parameters:</p> Name Type Description Default <code>funcs</code> <code>_type_</code> <p>description</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file (if it exists).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Diagnostics dictionary, with description of each function\'s outcome.</p> Source code in <code>behavysis_pipeline/pipeline/experiment.py</code> <pre><code>def evaluate(self, funcs, overwrite: bool) -&gt; dict:\n    """\n    Evaluating preprocessed DLC data and scored_behavs data.\n\n    Parameters\n    ----------\n    funcs : _type_\n        _description_\n    overwrite : bool\n        Whether to overwrite the output file (if it exists).\n\n    Returns\n    -------\n    dict\n        Diagnostics dictionary, with description of each function\'s outcome.\n    """\n    return self._process_scaffold(\n        funcs,\n        vid_fp=self.get_fp(Folders.FORMATTED_VID.value),\n        dlc_fp=self.get_fp(Folders.PREPROCESSED.value),\n        behavs_fp=self.get_fp(Folders.SCORED_BEHAVS.value),\n        out_dir=os.path.join(self.root_dir, EVALUATE_DIR),\n        configs_fp=self.get_fp(Folders.CONFIGS.value),\n        overwrite=overwrite,\n    )\n</code></pre>',
    },
    {
      location:
        "reference/experiment.html#behavysis_pipeline.pipeline.experiment.Experiment.export_behaviours",
      title: "<code>export_behaviours(overwrite)</code>",
      text: '<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>description</p> required <p>Returns:</p> Type Description <code>dict</code> <p>description</p> Source code in <code>behavysis_pipeline/pipeline/experiment.py</code> <pre><code>def export_behaviours(self, overwrite: bool) -&gt; dict:\n    """\n    _summary_\n\n    Parameters\n    ----------\n    overwrite : bool\n        _description_\n\n    Returns\n    -------\n    dict\n        _description_\n    """\n    # Exporting 6_predicted_behavs df to 7_scored_behavs folder\n    return self._process_scaffold(\n        (Export.predbehav_2_scoredbehav,),\n        src_fp=self.get_fp(Folders.PREDICTED_BEHAVS.value),\n        out_fp=self.get_fp(Folders.SCORED_BEHAVS.value),\n        configs_fp=self.get_fp(Folders.CONFIGS.value),\n        overwrite=overwrite,\n    )\n</code></pre>',
    },
    {
      location:
        "reference/experiment.html#behavysis_pipeline.pipeline.experiment.Experiment.export_feather",
      title: "<code>export_feather(in_dir, out_dir, overwrite)</code>",
      text: '<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>in_dir</code> <code>str</code> <p>description</p> required <code>out_dir</code> <code>str</code> <p>description</p> required <p>Returns:</p> Type Description <code>dict</code> <p>description</p> Source code in <code>behavysis_pipeline/pipeline/experiment.py</code> <pre><code>def export_feather(self, in_dir: str, out_dir: str, overwrite: bool) -&gt; dict:\n    """\n    _summary_\n\n    Parameters\n    ----------\n    in_dir : str\n        _description_\n    out_dir : str\n        _description_\n\n    Returns\n    -------\n    dict\n        _description_\n    """\n    return self._process_scaffold(\n        (Export.feather_2_csv,),\n        in_fp=self.get_fp(in_dir),\n        out_fp=os.path.join(out_dir, f"{self.name}.csv"),\n        overwrite=overwrite,\n    )\n</code></pre>',
    },
    {
      location:
        "reference/experiment.html#behavysis_pipeline.pipeline.experiment.Experiment.extract_features",
      title: "<code>extract_features(overwrite)</code>",
      text: '<p>Extracts features from the preprocessed dlc file to generate many more features. This dataframe of derived features will be input for a ML classifier to detect particularly trained behaviours.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file (if it exists).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Diagnostics dictionary, with description of each function\'s outcome.</p> Source code in <code>behavysis_pipeline/pipeline/experiment.py</code> <pre><code>def extract_features(self, overwrite: bool) -&gt; dict:\n    """\n    Extracts features from the preprocessed dlc file to generate many more features.\n    This dataframe of derived features will be input for a ML classifier to detect\n    particularly trained behaviours.\n\n    Parameters\n    ----------\n    overwrite : bool\n        Whether to overwrite the output file (if it exists).\n\n    Returns\n    -------\n    dict\n        Diagnostics dictionary, with description of each function\'s outcome.\n    """\n    return self._process_scaffold(\n        (ExtractFeatures.extract_features,),\n        dlc_fp=self.get_fp(Folders.PREPROCESSED.value),\n        out_fp=self.get_fp(Folders.FEATURES_EXTRACTED.value),\n        configs_fp=self.get_fp(Folders.CONFIGS.value),\n        temp_dir=os.path.join(self.root_dir, TEMP_DIR),\n        overwrite=overwrite,\n    )\n</code></pre>',
    },
    {
      location:
        "reference/experiment.html#behavysis_pipeline.pipeline.experiment.Experiment.format_vid",
      title: "<code>format_vid(funcs, overwrite)</code>",
      text: '<p>Formats the video with ffmpeg to fit the formatted configs (e.g. fps and resolution_px). Once the formatted video is produced, the configs dict and *configs.json file are updated with the formatted video\'s metadata.</p> <p>Parameters:</p> Name Type Description Default <code>funcs</code> <code>tuple[Callable, ...]</code> <p>description</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file (if it exists).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Diagnostics dictionary, with description of each function\'s outcome.</p> Notes <p>Can call any methods from <code>FormatVid</code>.</p> Source code in <code>behavysis_pipeline/pipeline/experiment.py</code> <pre><code>def format_vid(self, funcs: tuple[Callable, ...], overwrite: bool) -&gt; dict:\n    """\n    Formats the video with ffmpeg to fit the formatted configs (e.g. fps and resolution_px).\n    Once the formatted video is produced, the configs dict and *configs.json file are\n    updated with the formatted video\'s metadata.\n\n    Parameters\n    ----------\n    funcs : tuple[Callable, ...]\n        _description_\n    overwrite : bool\n        Whether to overwrite the output file (if it exists).\n\n    Returns\n    -------\n    dict\n        Diagnostics dictionary, with description of each function\'s outcome.\n\n    Notes\n    -----\n    Can call any methods from `FormatVid`.\n    """\n    return self._process_scaffold(\n        funcs,\n        in_fp=self.get_fp(Folders.RAW_VID.value),\n        out_fp=self.get_fp(Folders.FORMATTED_VID.value),\n        configs_fp=self.get_fp(Folders.CONFIGS.value),\n        overwrite=overwrite,\n    )\n</code></pre>',
    },
    {
      location:
        "reference/experiment.html#behavysis_pipeline.pipeline.experiment.Experiment.get_fp",
      title: "<code>get_fp(folder_str)</code>",
      text: '<p>Returns the experiment\'s file path from the given folder.</p> <p>Parameters:</p> Name Type Description Default <code>folder_str</code> <code>str</code> <p>The folder to return the experiment document\'s filepath for.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The experiment document\'s filepath.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>ValueError: Folder name is not valid. Refer to FOLDERS constant for valid folder names.</p> Source code in <code>behavysis_pipeline/pipeline/experiment.py</code> <pre><code>def get_fp(self, folder_str: str) -&gt; str:\n    """\n    Returns the experiment\'s file path from the given folder.\n\n    Parameters\n    ----------\n    folder_str : str\n        The folder to return the experiment document\'s filepath for.\n\n    Returns\n    -------\n    str\n        The experiment document\'s filepath.\n\n    Raises\n    ------\n    ValueError\n        ValueError: Folder name is not valid. Refer to FOLDERS constant for valid folder names.\n    """\n    # Getting folder enum from string\n    folder = next((f for f in Folders if folder_str == f.value), None)\n    # Assertion: The given folder name must be valid\n    if not folder:\n        raise ValueError(\n            f\'"{folder_str}" is not a valid experiment folder name.\\n\'\n            + "Please only specify one of the following folders:\\n"\n            + "    - "\n            + "\\n    - ".join([f.value for f in Folders])\n        )\n    # Getting experiment filepath for given folder\n    fp = os.path.join(\n        self.root_dir, folder.value, f"{self.name}{FILE_EXTS[folder]}"\n    )\n    # Making a folder if it does not exist\n    os.makedirs(os.path.split(fp)[0], exist_ok=True)\n    # Returning filepath\n    return fp\n</code></pre>',
    },
    {
      location:
        "reference/experiment.html#behavysis_pipeline.pipeline.experiment.Experiment.preprocess",
      title: "<code>preprocess(funcs, overwrite)</code>",
      text: '<p>A preprocessing pipeline method to convert raw DLC data into preprocessed DLC data that is ready for ML analysis. All functs passed in must have the format func(df, dict) -&gt; df. Possible funcs are given in preprocessing.py The preprocessed data is saved to the project\'s preprocessed folder.</p> <p>Parameters:</p> Name Type Description Default <code>funcs</code> <code>tuple[Callable, ...]</code> <p>description</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file (if it exists).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Diagnostics dictionary, with description of each function\'s outcome.</p> Notes <p>Can call any methods from <code>Preprocess</code>.</p> Source code in <code>behavysis_pipeline/pipeline/experiment.py</code> <pre><code>def preprocess(self, funcs: tuple[Callable, ...], overwrite: bool) -&gt; dict:\n    """\n    A preprocessing pipeline method to convert raw DLC data into preprocessed\n    DLC data that is ready for ML analysis.\n    All functs passed in must have the format func(df, dict) -&gt; df. Possible funcs\n    are given in preprocessing.py\n    The preprocessed data is saved to the project\'s preprocessed folder.\n\n    Parameters\n    ----------\n    funcs : tuple[Callable, ...]\n        _description_\n    overwrite : bool\n        Whether to overwrite the output file (if it exists).\n\n    Returns\n    -------\n    dict\n        Diagnostics dictionary, with description of each function\'s outcome.\n\n    Notes\n    -----\n    Can call any methods from `Preprocess`.\n    """\n    # Exporting 3_dlc df to 4_preprocessed folder\n    dd = self._process_scaffold(\n        (Export.feather_2_feather,),\n        src_fp=self.get_fp(Folders.DLC.value),\n        out_fp=self.get_fp(Folders.PREPROCESSED.value),\n        overwrite=overwrite,\n    )\n    # If there is an error, OR warning (indicates not to ovewrite), then return early\n    res = dd["feather_2_feather"]\n    if res.startswith("ERROR") or res.startswith("WARNING"):\n        return dd\n    # Feeding through preprocessing functions\n    return self._process_scaffold(\n        funcs,\n        in_fp=self.get_fp(Folders.PREPROCESSED.value),\n        out_fp=self.get_fp(Folders.PREPROCESSED.value),\n        configs_fp=self.get_fp(Folders.CONFIGS.value),\n        overwrite=True,\n    )\n</code></pre>',
    },
    {
      location:
        "reference/experiment.html#behavysis_pipeline.pipeline.experiment.Experiment.run_dlc",
      title: "<code>run_dlc(gputouse, overwrite)</code>",
      text: '<p>Run the DLC model on the formatted video to generate a DLC annotated video and DLC h5 file for all experiments.</p> <p>Parameters:</p> Name Type Description Default <code>gputouse</code> <code>int</code> <p>description</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file (if it exists).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Diagnostics dictionary, with description of each function\'s outcome.</p> Notes <p>Can call any methods from <code>RunDLC</code>.</p> Source code in <code>behavysis_pipeline/pipeline/experiment.py</code> <pre><code>def run_dlc(self, gputouse: int, overwrite: bool) -&gt; dict:\n    """\n    Run the DLC model on the formatted video to generate a DLC annotated video\n    and DLC h5 file for all experiments.\n\n    Parameters\n    ----------\n    gputouse : int\n        _description_\n    overwrite : bool\n        Whether to overwrite the output file (if it exists).\n\n    Returns\n    -------\n    dict\n        Diagnostics dictionary, with description of each function\'s outcome.\n\n    Notes\n    -----\n    Can call any methods from `RunDLC`.\n    """\n    return self._process_scaffold(\n        (RunDLC.ma_dlc_analyse_single,),\n        in_fp=self.get_fp(Folders.FORMATTED_VID.value),\n        out_fp=self.get_fp(Folders.DLC.value),\n        configs_fp=self.get_fp(Folders.CONFIGS.value),\n        temp_dir=os.path.join(self.root_dir, TEMP_DIR),\n        gputouse=gputouse,\n        overwrite=overwrite,\n    )\n</code></pre>',
    },
    {
      location:
        "reference/experiment.html#behavysis_pipeline.pipeline.experiment.Experiment.update_configs",
      title: "<code>update_configs(default_configs_fp, overwrite)</code>",
      text: '<p>Initialises the JSON config files with the given configurations in <code>configs</code>. It can be specified whether or not to overwrite existing configuration values.</p> <p>Parameters:</p> Name Type Description Default <code>default_configs_fp</code> <code>str</code> <p>The JSON configs filepath to add/overwrite to the experiment\'s current configs file.</p> required <code>overwrite</code> <code>(\'set\', \'reset\')</code> <p>Specifies how to overwrite existing configurations. If <code>add</code>, only parameters in <code>configs</code> not already in the config files are added. If <code>set</code>, all parameters in <code>configs</code> are set in the config files (overwriting). If <code>reset</code>, the config files are completely replaced by <code>configs</code>.</p> <code>"set"</code> <p>Returns:</p> Type Description <code>dict</code> <p>Diagnostics dictionary, with description of each function\'s outcome.</p> Source code in <code>behavysis_pipeline/pipeline/experiment.py</code> <pre><code>def update_configs(self, default_configs_fp: str, overwrite: str) -&gt; dict:\n    """\n    Initialises the JSON config files with the given configurations in `configs`.\n    It can be specified whether or not to overwrite existing configuration values.\n\n    Parameters\n    ----------\n    default_configs_fp : str\n        The JSON configs filepath to add/overwrite to the experiment\'s current configs file.\n    overwrite : {"set", "reset"}\n        Specifies how to overwrite existing configurations.\n        If `add`, only parameters in `configs` not already in the config files are added.\n        If `set`, all parameters in `configs` are set in the config files (overwriting).\n        If `reset`, the config files are completely replaced by `configs`.\n\n    Returns\n    -------\n    dict\n        Diagnostics dictionary, with description of each function\'s outcome.\n    """\n    return self._process_scaffold(\n        (UpdateConfigs.update_configs,),\n        configs_fp=self.get_fp(Folders.CONFIGS.value),\n        default_configs_fp=default_configs_fp,\n        overwrite=overwrite,\n    )\n</code></pre>',
    },
    { location: "reference/processes.html", title: "Processes", text: "" },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.CalculateParams",
      title: "<code>behavysis_pipeline.processes.CalculateParams</code>",
      text: '<p>summary</p> Source code in <code>behavysis_pipeline/processes/calculate_params.py</code> <pre><code>class CalculateParams:\n    """__summary__"""\n\n    @staticmethod\n    def start_frame(\n        dlc_fp: str,\n        configs_fp: str,\n    ) -&gt; str:\n        """\n        Determine the starting frame of the experiment based on when the subject "likely" entered\n        the footage.\n\n        This is done by looking at a sliding window of time. If the median likelihood of the subject\n        existing in each frame across the sliding window is greater than the defined pcutoff, then\n        the determine this as the start time.\n\n        Notes\n        -----\n        The config file must contain the following parameters:\n        ```\n        - user\n            - calculate_params\n                - start_frame\n                    - window_sec: float\n                    - pcutoff: float\n        ```\n        """\n        outcome = ""\n        # Getting necessary config parameters\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs_filt = Model_check_exists(**configs.user.calculate_params.start_frame)\n        bpts = configs.get_ref(configs_filt.bodyparts)\n        window_sec = configs.get_ref(configs_filt.window_sec)\n        pcutoff = configs.get_ref(configs_filt.pcutoff)\n        fps = configs.auto.formatted_vid.fps\n        # Asserting that the necessary auto configs are valid\n        assert fps is not None, "fps is None. Please calculate fps first."\n        # Deriving more parameters\n        window_frames = int(np.round(fps * window_sec, 0))\n        # Loading dataframe\n        dlc_df = KeypointsMixin.clean_headings(KeypointsMixin.read_feather(dlc_fp))\n        # Getting likehoods of subject (given bpts) existing in each frame\n        df_lhoods = calc_likelihoods(dlc_df, bpts, window_frames)\n        # Determining start time. Start frame is the first frame of the rolling window\'s range\n        df_lhoods["exists"] = df_lhoods["rolling"] &gt; pcutoff\n        # Getting when subject first and last exists in video\n        start_frame = 0\n        if np.all(df_lhoods["exists"] == 0):\n            # If subject never exists (i.e. no True values in exist column), then raise warning\n            outcome += (\n                "WARNING: The subject was not detected in any frames - using the first frame."\n                + "Please check the video.\\n"\n            )\n        else:\n            start_frame = df_lhoods[df_lhoods["exists"]].index[0]\n        # Writing to configs\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs.auto.start_frame = start_frame\n        # configs.auto.start_sec = start_frame / fps\n        configs.write_json(configs_fp)\n        return outcome\n\n    @staticmethod\n    def stop_frame(dlc_fp: str, configs_fp: str) -&gt; str:\n        """\n        Calculates the end time according to the following equation:\n\n        ```\n        stop_frame = start_frame + experiment_duration\n        ```\n\n        Notes\n        -----\n        The config file must contain the following parameters:\n        ```\n        TODO\n        ```\n        """\n        outcome = ""\n        # Getting necessary config parameters\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs_filt = Model_stop_frame(**configs.user.calculate_params.stop_frame)\n        dur_sec = configs.get_ref(configs_filt.dur_sec)\n        start_frame = configs.auto.start_frame\n        fps = configs.auto.formatted_vid.fps\n        auto_stop_frame = configs.auto.formatted_vid.total_frames\n        # Asserting that the necessary auto configs are valid\n        assert (\n            start_frame is not None\n        ), "start_frame is None. Please calculate start_frame first."\n        assert fps is not None, "fps is None. Please calculate fps first."\n        assert (\n            auto_stop_frame is not None\n        ), "total_frames is None. Please calculate total_frames first."\n        # Calculating stop_frame\n        dur_frames = int(dur_sec * fps)\n        stop_frame = start_frame + dur_frames\n        # Make a warning if the use-specified dur_sec is larger than the duration of the video.\n        if auto_stop_frame is None:\n            outcome += (\n                "WARNING: The length of the video itself has not been calculated yet."\n            )\n        elif stop_frame &gt; auto_stop_frame:\n            outcome += (\n                "WARNING: The user specified dur_sec in the configs file is greater "\n                + "than the actual length of the video. Please check to see if this video is "\n                + "too short or if the dur_sec value is incorrect.\\n"\n            )\n        # Writing to config\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs.auto.stop_frame = stop_frame\n        # configs.auto.stop_sec = stop_frame / fps\n        configs.write_json(configs_fp)\n        return outcome\n\n    @staticmethod\n    def exp_dur(dlc_fp: str, configs_fp: str) -&gt; str:\n        """\n        Calculates the duration in seconds, from the time the specified bodyparts appeared\n        to the time they disappeared.\n        Appear/disappear is calculated from likelihood.\n        """\n        outcome = ""\n        # Getting necessary config parameters\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs_filt = Model_check_exists(**configs.user.calculate_params.exp_dur)\n        bpts = configs.get_ref(configs_filt.bodyparts)\n        window_sec = configs.get_ref(configs_filt.window_sec)\n        pcutoff = configs.get_ref(configs_filt.pcutoff)\n        fps = configs.auto.formatted_vid.fps\n        # Asserting that the necessary auto configs are valid\n        assert fps is not None, "fps is None. Please calculate fps first."\n        # Deriving more parameters\n        window_frames = int(np.round(fps * window_sec, 0))\n        # Loading dataframe\n        dlc_df = KeypointsMixin.clean_headings(KeypointsMixin.read_feather(dlc_fp))\n        # Getting likehoods of subject (given bpts) existing in each frame\n        df_lhoods = calc_likelihoods(dlc_df, bpts, window_frames)\n        # Determining start time. Start frame is the first frame of the rolling window\'s range\n        df_lhoods["exists"] = df_lhoods["rolling"] &gt; pcutoff\n        # Getting when subject first and last exists in video\n        exp_dur_frames = 0\n        if np.all(df_lhoods["exists"] == 0):\n            # If subject never exists (i.e. no True values in exist column), then raise warning\n            outcome += (\n                "WARNING: The subject was not detected in any frames - using the first frame."\n                + "Please check the video.\\n"\n            )\n        else:\n            start_frame = df_lhoods[df_lhoods["exists"]].index[0]\n            stop_frame = df_lhoods[df_lhoods["exists"]].index[-1]\n            exp_dur_frames = stop_frame - start_frame\n        # Writing to configs\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs.auto.exp_dur_frames = exp_dur_frames\n        # configs.auto.exp_dur_secs = exp_dur_frames / fps\n        configs.write_json(configs_fp)\n        return outcome\n\n    @staticmethod\n    def px_per_mm(dlc_fp: str, configs_fp: str) -&gt; str:\n        """\n        Calculates the pixels per mm conversion for the video.\n\n        This is done by averaging the (x, y) coordinates of each corner,\n        finding the average x difference for the widths in pixels and y distance\n        for the heights in pixels,\n        dividing these pixel distances by their respective mm distances\n        (from the *config.json file),\n        and taking the average of these width and height conversions to estimate\n        the px to mm\n        conversion.\n\n        Notes\n        -----\n        The config file must contain the following parameters:\n        ```\n        - user\n            - calculate_params\n                - px_per_mm\n                    - point_a: str\n                    - point_b: str\n                    - dist_mm: float\n        ```\n        """\n        outcome = ""\n        # Getting necessary config parameters\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs_filt = Model_px_per_mm(**configs.user.calculate_params.px_per_mm)\n        pt_a = configs.get_ref(configs_filt.pt_a)\n        pt_b = configs.get_ref(configs_filt.pt_b)\n        pcutoff = configs.get_ref(configs_filt.pcutoff)\n        dist_mm = configs.get_ref(configs_filt.dist_mm)\n        # Loading dataframe\n        dlc_df = KeypointsMixin.clean_headings(KeypointsMixin.read_feather(dlc_fp))\n        # Imputing missing values with 0 (only really relevant for `likelihood` columns)\n        dlc_df = dlc_df.fillna(0)\n        # Checking that the two reference points are valid\n        KeypointsMixin.check_bpts_exist(dlc_df, [pt_a, pt_b])\n        # Getting calibration points (x, y, likelihood) values\n        pt_a_df = dlc_df[IndivColumns.SINGLE.value, pt_a]\n        pt_b_df = dlc_df[IndivColumns.SINGLE.value, pt_b]\n        # Interpolating points which are below a likelihood threshold (linear)\n        pt_a_df.loc[pt_a_df[Coords.LIKELIHOOD.value] &lt; pcutoff] = np.nan\n        pt_a_df = pt_a_df.interpolate(method="linear", axis=0).bfill()\n        pt_b_df.loc[pt_b_df[Coords.LIKELIHOOD.value] &lt; pcutoff] = np.nan\n        pt_b_df = pt_b_df.interpolate(method="linear", axis=0).bfill()\n        # Getting distance between calibration points\n        dist_px = np.nanmean(\n            np.sqrt(\n                np.square(pt_a_df["x"] - pt_b_df["x"])\n                + np.square(pt_a_df["y"] - pt_b_df["y"])\n            )\n        )\n        # Finding pixels per mm conversion, using the given arena width and height as calibration\n        px_per_mm = dist_px / dist_mm\n        # Saving to configs file\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs.auto.px_per_mm = px_per_mm\n        configs.write_json(configs_fp)\n        return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.CalculateParams.exp_dur",
      title:
        "<code>exp_dur(dlc_fp, configs_fp)</code>  <code>staticmethod</code>",
      text: '<p>Calculates the duration in seconds, from the time the specified bodyparts appeared to the time they disappeared. Appear/disappear is calculated from likelihood.</p> Source code in <code>behavysis_pipeline/processes/calculate_params.py</code> <pre><code>@staticmethod\ndef exp_dur(dlc_fp: str, configs_fp: str) -&gt; str:\n    """\n    Calculates the duration in seconds, from the time the specified bodyparts appeared\n    to the time they disappeared.\n    Appear/disappear is calculated from likelihood.\n    """\n    outcome = ""\n    # Getting necessary config parameters\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs_filt = Model_check_exists(**configs.user.calculate_params.exp_dur)\n    bpts = configs.get_ref(configs_filt.bodyparts)\n    window_sec = configs.get_ref(configs_filt.window_sec)\n    pcutoff = configs.get_ref(configs_filt.pcutoff)\n    fps = configs.auto.formatted_vid.fps\n    # Asserting that the necessary auto configs are valid\n    assert fps is not None, "fps is None. Please calculate fps first."\n    # Deriving more parameters\n    window_frames = int(np.round(fps * window_sec, 0))\n    # Loading dataframe\n    dlc_df = KeypointsMixin.clean_headings(KeypointsMixin.read_feather(dlc_fp))\n    # Getting likehoods of subject (given bpts) existing in each frame\n    df_lhoods = calc_likelihoods(dlc_df, bpts, window_frames)\n    # Determining start time. Start frame is the first frame of the rolling window\'s range\n    df_lhoods["exists"] = df_lhoods["rolling"] &gt; pcutoff\n    # Getting when subject first and last exists in video\n    exp_dur_frames = 0\n    if np.all(df_lhoods["exists"] == 0):\n        # If subject never exists (i.e. no True values in exist column), then raise warning\n        outcome += (\n            "WARNING: The subject was not detected in any frames - using the first frame."\n            + "Please check the video.\\n"\n        )\n    else:\n        start_frame = df_lhoods[df_lhoods["exists"]].index[0]\n        stop_frame = df_lhoods[df_lhoods["exists"]].index[-1]\n        exp_dur_frames = stop_frame - start_frame\n    # Writing to configs\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs.auto.exp_dur_frames = exp_dur_frames\n    # configs.auto.exp_dur_secs = exp_dur_frames / fps\n    configs.write_json(configs_fp)\n    return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.CalculateParams.px_per_mm",
      title:
        "<code>px_per_mm(dlc_fp, configs_fp)</code>  <code>staticmethod</code>",
      text: '<p>Calculates the pixels per mm conversion for the video.</p> <p>This is done by averaging the (x, y) coordinates of each corner, finding the average x difference for the widths in pixels and y distance for the heights in pixels, dividing these pixel distances by their respective mm distances (from the *config.json file), and taking the average of these width and height conversions to estimate the px to mm conversion.</p> Notes <p>The config file must contain the following parameters: <pre><code>- user\n    - calculate_params\n        - px_per_mm\n            - point_a: str\n            - point_b: str\n            - dist_mm: float\n</code></pre></p> Source code in <code>behavysis_pipeline/processes/calculate_params.py</code> <pre><code>@staticmethod\ndef px_per_mm(dlc_fp: str, configs_fp: str) -&gt; str:\n    """\n    Calculates the pixels per mm conversion for the video.\n\n    This is done by averaging the (x, y) coordinates of each corner,\n    finding the average x difference for the widths in pixels and y distance\n    for the heights in pixels,\n    dividing these pixel distances by their respective mm distances\n    (from the *config.json file),\n    and taking the average of these width and height conversions to estimate\n    the px to mm\n    conversion.\n\n    Notes\n    -----\n    The config file must contain the following parameters:\n    ```\n    - user\n        - calculate_params\n            - px_per_mm\n                - point_a: str\n                - point_b: str\n                - dist_mm: float\n    ```\n    """\n    outcome = ""\n    # Getting necessary config parameters\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs_filt = Model_px_per_mm(**configs.user.calculate_params.px_per_mm)\n    pt_a = configs.get_ref(configs_filt.pt_a)\n    pt_b = configs.get_ref(configs_filt.pt_b)\n    pcutoff = configs.get_ref(configs_filt.pcutoff)\n    dist_mm = configs.get_ref(configs_filt.dist_mm)\n    # Loading dataframe\n    dlc_df = KeypointsMixin.clean_headings(KeypointsMixin.read_feather(dlc_fp))\n    # Imputing missing values with 0 (only really relevant for `likelihood` columns)\n    dlc_df = dlc_df.fillna(0)\n    # Checking that the two reference points are valid\n    KeypointsMixin.check_bpts_exist(dlc_df, [pt_a, pt_b])\n    # Getting calibration points (x, y, likelihood) values\n    pt_a_df = dlc_df[IndivColumns.SINGLE.value, pt_a]\n    pt_b_df = dlc_df[IndivColumns.SINGLE.value, pt_b]\n    # Interpolating points which are below a likelihood threshold (linear)\n    pt_a_df.loc[pt_a_df[Coords.LIKELIHOOD.value] &lt; pcutoff] = np.nan\n    pt_a_df = pt_a_df.interpolate(method="linear", axis=0).bfill()\n    pt_b_df.loc[pt_b_df[Coords.LIKELIHOOD.value] &lt; pcutoff] = np.nan\n    pt_b_df = pt_b_df.interpolate(method="linear", axis=0).bfill()\n    # Getting distance between calibration points\n    dist_px = np.nanmean(\n        np.sqrt(\n            np.square(pt_a_df["x"] - pt_b_df["x"])\n            + np.square(pt_a_df["y"] - pt_b_df["y"])\n        )\n    )\n    # Finding pixels per mm conversion, using the given arena width and height as calibration\n    px_per_mm = dist_px / dist_mm\n    # Saving to configs file\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs.auto.px_per_mm = px_per_mm\n    configs.write_json(configs_fp)\n    return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.CalculateParams.start_frame",
      title:
        "<code>start_frame(dlc_fp, configs_fp)</code>  <code>staticmethod</code>",
      text: '<p>Determine the starting frame of the experiment based on when the subject "likely" entered the footage.</p> <p>This is done by looking at a sliding window of time. If the median likelihood of the subject existing in each frame across the sliding window is greater than the defined pcutoff, then the determine this as the start time.</p> Notes <p>The config file must contain the following parameters: <pre><code>- user\n    - calculate_params\n        - start_frame\n            - window_sec: float\n            - pcutoff: float\n</code></pre></p> Source code in <code>behavysis_pipeline/processes/calculate_params.py</code> <pre><code>@staticmethod\ndef start_frame(\n    dlc_fp: str,\n    configs_fp: str,\n) -&gt; str:\n    """\n    Determine the starting frame of the experiment based on when the subject "likely" entered\n    the footage.\n\n    This is done by looking at a sliding window of time. If the median likelihood of the subject\n    existing in each frame across the sliding window is greater than the defined pcutoff, then\n    the determine this as the start time.\n\n    Notes\n    -----\n    The config file must contain the following parameters:\n    ```\n    - user\n        - calculate_params\n            - start_frame\n                - window_sec: float\n                - pcutoff: float\n    ```\n    """\n    outcome = ""\n    # Getting necessary config parameters\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs_filt = Model_check_exists(**configs.user.calculate_params.start_frame)\n    bpts = configs.get_ref(configs_filt.bodyparts)\n    window_sec = configs.get_ref(configs_filt.window_sec)\n    pcutoff = configs.get_ref(configs_filt.pcutoff)\n    fps = configs.auto.formatted_vid.fps\n    # Asserting that the necessary auto configs are valid\n    assert fps is not None, "fps is None. Please calculate fps first."\n    # Deriving more parameters\n    window_frames = int(np.round(fps * window_sec, 0))\n    # Loading dataframe\n    dlc_df = KeypointsMixin.clean_headings(KeypointsMixin.read_feather(dlc_fp))\n    # Getting likehoods of subject (given bpts) existing in each frame\n    df_lhoods = calc_likelihoods(dlc_df, bpts, window_frames)\n    # Determining start time. Start frame is the first frame of the rolling window\'s range\n    df_lhoods["exists"] = df_lhoods["rolling"] &gt; pcutoff\n    # Getting when subject first and last exists in video\n    start_frame = 0\n    if np.all(df_lhoods["exists"] == 0):\n        # If subject never exists (i.e. no True values in exist column), then raise warning\n        outcome += (\n            "WARNING: The subject was not detected in any frames - using the first frame."\n            + "Please check the video.\\n"\n        )\n    else:\n        start_frame = df_lhoods[df_lhoods["exists"]].index[0]\n    # Writing to configs\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs.auto.start_frame = start_frame\n    # configs.auto.start_sec = start_frame / fps\n    configs.write_json(configs_fp)\n    return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.CalculateParams.stop_frame",
      title:
        "<code>stop_frame(dlc_fp, configs_fp)</code>  <code>staticmethod</code>",
      text: '<p>Calculates the end time according to the following equation:</p> <pre><code>stop_frame = start_frame + experiment_duration\n</code></pre> Notes <p>The config file must contain the following parameters: <pre><code>TODO\n</code></pre></p> Source code in <code>behavysis_pipeline/processes/calculate_params.py</code> <pre><code>@staticmethod\ndef stop_frame(dlc_fp: str, configs_fp: str) -&gt; str:\n    """\n    Calculates the end time according to the following equation:\n\n    ```\n    stop_frame = start_frame + experiment_duration\n    ```\n\n    Notes\n    -----\n    The config file must contain the following parameters:\n    ```\n    TODO\n    ```\n    """\n    outcome = ""\n    # Getting necessary config parameters\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs_filt = Model_stop_frame(**configs.user.calculate_params.stop_frame)\n    dur_sec = configs.get_ref(configs_filt.dur_sec)\n    start_frame = configs.auto.start_frame\n    fps = configs.auto.formatted_vid.fps\n    auto_stop_frame = configs.auto.formatted_vid.total_frames\n    # Asserting that the necessary auto configs are valid\n    assert (\n        start_frame is not None\n    ), "start_frame is None. Please calculate start_frame first."\n    assert fps is not None, "fps is None. Please calculate fps first."\n    assert (\n        auto_stop_frame is not None\n    ), "total_frames is None. Please calculate total_frames first."\n    # Calculating stop_frame\n    dur_frames = int(dur_sec * fps)\n    stop_frame = start_frame + dur_frames\n    # Make a warning if the use-specified dur_sec is larger than the duration of the video.\n    if auto_stop_frame is None:\n        outcome += (\n            "WARNING: The length of the video itself has not been calculated yet."\n        )\n    elif stop_frame &gt; auto_stop_frame:\n        outcome += (\n            "WARNING: The user specified dur_sec in the configs file is greater "\n            + "than the actual length of the video. Please check to see if this video is "\n            + "too short or if the dur_sec value is incorrect.\\n"\n        )\n    # Writing to config\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs.auto.stop_frame = stop_frame\n    # configs.auto.stop_sec = stop_frame / fps\n    configs.write_json(configs_fp)\n    return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.ClassifyBehaviours",
      title: "<code>behavysis_pipeline.processes.ClassifyBehaviours</code>",
      text: '<p>summary</p> Source code in <code>behavysis_pipeline/processes/classify_behaviours.py</code> <pre><code>class ClassifyBehaviours:\n    """__summary__"""\n\n    @staticmethod\n    @IOMixin.overwrite_check()\n    def classify_behaviours(\n        features_fp: str,\n        out_fp: str,\n        configs_fp: str,\n        overwrite: bool,\n    ) -&gt; str:\n        """\n        Given model config files in the BehavClassifier format, generates beahviour predidctions\n        on the given extracted features dataframe.\n\n        Parameters\n        ----------\n        features_fp : str\n            _description_\n        out_fp : str\n            _description_\n        configs_fp : str\n            _description_\n        overwrite : bool\n            Whether to overwrite the output file (if it exists).\n\n        Returns\n        -------\n        str\n            Description of the function\'s outcome.\n\n        Notes\n        -----\n        The config file must contain the following parameters:\n        ```\n        - user\n            - classify_behaviours\n                - models: list[str]\n        ```\n        Where the `models` list is a list of `model_config.json` filepaths.\n        """\n        outcome = ""\n        # Getting necessary config parameters\n        configs = ExperimentConfigs.read_json(configs_fp)\n        models_ls = configs.user.classify_behaviours\n        # Getting features data\n        features_df = DFMixin.read_feather(features_fp)\n        # Initialising y_preds df\n        # Getting predictions for each classifier model and saving\n        # in a list of pd.DataFrames\n        behav_preds_ls = np.zeros(len(models_ls), dtype="object")\n        for i, model_config in enumerate(models_ls):\n            # Getting model (clf, pcutoff, min_window_frames)\n            model_fp = configs.get_ref(model_config.model_fp)\n            model = BehavClassifier.load(model_fp)\n            pcutoff = configs.get_ref(model_config.pcutoff)\n            pcutoff = model.configs.pcutoff if pcutoff is None else pcutoff\n            min_window_frames = configs.get_ref(model_config.min_window_frames)\n            # Running the clf pipeline\n            df_i = model.pipeline_run(features_df)\n            # Getting prob and pred column names\n            prob_col = (model.configs.behaviour_name, BehavColumns.PROB.value)\n            pred_col = (model.configs.behaviour_name, BehavColumns.PRED.value)\n            # Using pcutoff to get binary predictions\n            df_i[pred_col] = (df_i[prob_col] &gt; pcutoff).astype(int)\n            # Filling in small non-behav bouts\n            df_i[pred_col] = merge_bouts(df_i[pred_col], min_window_frames)\n            # Adding model predictions df to list\n            behav_preds_ls[i] = df_i\n            # Logging outcome\n            outcome += f"Completed {model.configs.behaviour_name} classification.\\n"\n        # Concatenating predictions to a single dataframe\n        behavs_df = pd.concat(behav_preds_ls, axis=1)\n        # Setting the index and column names\n        behavs_df.index.names = DFMixin.enum_to_list(BehavIN)\n        behavs_df.columns.names = DFMixin.enum_to_list(BehavCN)\n        # Checking df\n        BehavMixin.check_df(behavs_df)\n        # Saving behav_preds df\n        DFMixin.write_feather(behavs_df, out_fp)\n        # Returning outcome\n        return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.ClassifyBehaviours.classify_behaviours",
      title:
        "<code>classify_behaviours(features_fp, out_fp, configs_fp, overwrite)</code>  <code>staticmethod</code>",
      text: '<p>Given model config files in the BehavClassifier format, generates beahviour predidctions on the given extracted features dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>features_fp</code> <code>str</code> <p>description</p> required <code>out_fp</code> <code>str</code> <p>description</p> required <code>configs_fp</code> <code>str</code> <p>description</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file (if it exists).</p> required <p>Returns:</p> Type Description <code>str</code> <p>Description of the function\'s outcome.</p> Notes <p>The config file must contain the following parameters: <pre><code>- user\n    - classify_behaviours\n        - models: list[str]\n</code></pre> Where the <code>models</code> list is a list of <code>model_config.json</code> filepaths.</p> Source code in <code>behavysis_pipeline/processes/classify_behaviours.py</code> <pre><code>@staticmethod\n@IOMixin.overwrite_check()\ndef classify_behaviours(\n    features_fp: str,\n    out_fp: str,\n    configs_fp: str,\n    overwrite: bool,\n) -&gt; str:\n    """\n    Given model config files in the BehavClassifier format, generates beahviour predidctions\n    on the given extracted features dataframe.\n\n    Parameters\n    ----------\n    features_fp : str\n        _description_\n    out_fp : str\n        _description_\n    configs_fp : str\n        _description_\n    overwrite : bool\n        Whether to overwrite the output file (if it exists).\n\n    Returns\n    -------\n    str\n        Description of the function\'s outcome.\n\n    Notes\n    -----\n    The config file must contain the following parameters:\n    ```\n    - user\n        - classify_behaviours\n            - models: list[str]\n    ```\n    Where the `models` list is a list of `model_config.json` filepaths.\n    """\n    outcome = ""\n    # Getting necessary config parameters\n    configs = ExperimentConfigs.read_json(configs_fp)\n    models_ls = configs.user.classify_behaviours\n    # Getting features data\n    features_df = DFMixin.read_feather(features_fp)\n    # Initialising y_preds df\n    # Getting predictions for each classifier model and saving\n    # in a list of pd.DataFrames\n    behav_preds_ls = np.zeros(len(models_ls), dtype="object")\n    for i, model_config in enumerate(models_ls):\n        # Getting model (clf, pcutoff, min_window_frames)\n        model_fp = configs.get_ref(model_config.model_fp)\n        model = BehavClassifier.load(model_fp)\n        pcutoff = configs.get_ref(model_config.pcutoff)\n        pcutoff = model.configs.pcutoff if pcutoff is None else pcutoff\n        min_window_frames = configs.get_ref(model_config.min_window_frames)\n        # Running the clf pipeline\n        df_i = model.pipeline_run(features_df)\n        # Getting prob and pred column names\n        prob_col = (model.configs.behaviour_name, BehavColumns.PROB.value)\n        pred_col = (model.configs.behaviour_name, BehavColumns.PRED.value)\n        # Using pcutoff to get binary predictions\n        df_i[pred_col] = (df_i[prob_col] &gt; pcutoff).astype(int)\n        # Filling in small non-behav bouts\n        df_i[pred_col] = merge_bouts(df_i[pred_col], min_window_frames)\n        # Adding model predictions df to list\n        behav_preds_ls[i] = df_i\n        # Logging outcome\n        outcome += f"Completed {model.configs.behaviour_name} classification.\\n"\n    # Concatenating predictions to a single dataframe\n    behavs_df = pd.concat(behav_preds_ls, axis=1)\n    # Setting the index and column names\n    behavs_df.index.names = DFMixin.enum_to_list(BehavIN)\n    behavs_df.columns.names = DFMixin.enum_to_list(BehavCN)\n    # Checking df\n    BehavMixin.check_df(behavs_df)\n    # Saving behav_preds df\n    DFMixin.write_feather(behavs_df, out_fp)\n    # Returning outcome\n    return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.Evaluate",
      title: "<code>behavysis_pipeline.processes.Evaluate</code>",
      text: '<p>summary</p> Source code in <code>behavysis_pipeline/processes/evaluate.py</code> <pre><code>class Evaluate:\n    """__summary__"""\n\n    ###############################################################################################\n    #               MAKE KEYPOINTS PLOTS\n    ###############################################################################################\n\n    @staticmethod\n    def keypoints_plot(\n        vid_fp: str,\n        dlc_fp: str,\n        behavs_fp: str,\n        out_dir: str,\n        configs_fp: str,\n        overwrite: bool,\n    ) -&gt; str:\n        """\n        Make keypoints evaluation plot of likelihood of each bodypart through time.\n        """\n        outcome = ""\n        name = IOMixin.get_name(dlc_fp)\n        out_dir = os.path.join(out_dir, Evaluate.keypoints_plot.__name__)\n        out_fp = os.path.join(out_dir, f"{name}.png")\n        os.makedirs(out_dir, exist_ok=True)\n        # If overwrite is False, checking if we should skip processing\n        if not overwrite and os.path.exists(out_fp):\n            return DiagnosticsMixin.warning_msg()\n\n        # Getting necessary config parameters\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs_filt = configs.user.evaluate.keypoints_plot\n        bpts = configs.get_ref(configs_filt.bodyparts)\n        fps = configs.auto.formatted_vid.fps\n\n        # Read the file\n        df = KeypointsMixin.clean_headings(KeypointsMixin.read_feather(dlc_fp))\n        # Checking the bodyparts specified in the configs exist in the dataframe\n        KeypointsMixin.check_bpts_exist(df, bpts)\n        # Making data-long ways\n        idx = pd.IndexSlice\n        df = (\n            df.loc[:, idx[:, bpts]]\n            .stack([KeypointsCN.INDIVIDUALS.value, KeypointsCN.BODYPARTS.value])\n            .reset_index()\n        )\n        # Adding the timestamp column\n        df["timestamp"] = df[BehavIN.FRAME.value] / fps\n        # Making plot\n        g = sns.FacetGrid(\n            df,\n            row=KeypointsCN.INDIVIDUALS.value,\n            height=5,\n            aspect=10,\n        )\n        g.map_dataframe(\n            sns.lineplot,\n            x="timestamp",\n            y=Coords.LIKELIHOOD.value,\n            hue=KeypointsCN.BODYPARTS.value,\n            alpha=0.4,\n        )\n        g.add_legend()\n        # Saving plot\n        g.savefig(out_fp)\n        g.figure.clf()\n        # Returning outcome string\n        return outcome\n\n    ###############################################################################################\n    # MAKE BEHAVIOUR PLOTS\n    ###############################################################################################\n\n    @staticmethod\n    def behav_plot(\n        vid_fp: str,\n        dlc_fp: str,\n        behavs_fp: str,\n        out_dir: str,\n        configs_fp: str,\n        overwrite: bool,\n    ) -&gt; str:\n        """\n        Make behaviour evaluation plot of the predicted and actual behaviours through time.\n        """\n        outcome = ""\n        name = IOMixin.get_name(behavs_fp)\n        out_dir = os.path.join(out_dir, Evaluate.behav_plot.__name__)\n        out_fp = os.path.join(out_dir, f"{name}.png")\n        os.makedirs(out_dir, exist_ok=True)\n        # If overwrite is False, checking if we should skip processing\n        if not overwrite and os.path.exists(out_fp):\n            return DiagnosticsMixin.warning_msg()\n\n        # Getting necessary config parameters\n        configs = ExperimentConfigs.read_json(configs_fp)\n        # configs_filt = configs.user.evaluate.behav_plot\n        fps = configs.auto.formatted_vid.fps\n\n        # Read the file\n        df = BehavMixin.read_feather(behavs_fp)\n        # Making data-long ways\n        df = (\n            df.stack([BehavCN.BEHAVIOURS.value, BehavCN.OUTCOMES.value])\n            .reset_index()\n            .rename(columns={0: "value"})\n        )\n        # Adding the timestamp column\n        df["timestamp"] = df[BehavIN.FRAME.value] / fps\n        # Making plot\n        g = sns.FacetGrid(\n            df,\n            row=BehavCN.BEHAVIOURS.value,\n            height=5,\n            aspect=10,\n        )\n        g.map_dataframe(\n            sns.lineplot,\n            x="timestamp",\n            y="value",\n            hue=BehavCN.OUTCOMES.value,\n            alpha=0.4,\n        )\n        g.add_legend()\n        # Saving plot\n        g.savefig(out_fp)\n        g.figure.clf()\n        # Returning outcome string\n        return outcome\n\n    ###############################################################################################\n    #               MAKE KEYPOINTS VIDEO\n    ###############################################################################################\n\n    @staticmethod\n    def eval_vid(\n        vid_fp: str,\n        dlc_fp: str,\n        behavs_fp: str,\n        out_dir: str,\n        configs_fp: str,\n        overwrite: bool,\n    ) -&gt; str:\n        """\n        Run the DLC model on the formatted video to generate a DLC annotated video and DLC file for\n        all experiments. The DLC model\'s config.yaml filepath must be specified in the `config_path`\n        parameter in the `user` section of the config file.\n        """\n\n        outcome = ""\n        name = IOMixin.get_name(vid_fp)\n        out_dir = os.path.join(out_dir, Evaluate.eval_vid.__name__)\n        out_fp = os.path.join(out_dir, f"{name}.mp4")\n        os.makedirs(out_dir, exist_ok=True)\n        # If overwrite is False, checking if we should skip processing\n        if not overwrite and os.path.exists(out_fp):\n            return DiagnosticsMixin.warning_msg()\n\n        # Getting necessary config parameters\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs_filt = configs.user.evaluate.eval_vid\n        funcs_names = configs.get_ref(configs_filt.funcs)\n        pcutoff = configs.get_ref(configs_filt.pcutoff)\n        colour_level = configs.get_ref(configs_filt.colour_level)\n        radius = configs.get_ref(configs_filt.radius)\n        cmap = configs.get_ref(configs_filt.cmap)\n\n        # Modifying dlc_df and making list of how to select dlc_df components to optimise processing\n        dlc_df = KeypointsMixin.clean_headings(KeypointsMixin.read_feather(dlc_fp))\n        # Filtering out IndivColumns.PROCESS.value columns\n        if IndivColumns.PROCESS.value in dlc_df.columns.unique("individuals"):\n            dlc_df.drop(columns=IndivColumns.PROCESS.value, level="individuals")\n        # Getting (indivs, bpts) MultiIndex\n        indivs_bpts_ls = dlc_df.columns.droplevel("coords").unique()\n        # Rounding and converting to correct dtypes - "x" and "y" values are ints\n        dlc_df = dlc_df.fillna(0)\n        columns = dlc_df.columns[\n            dlc_df.columns.get_level_values("coords").isin(["x", "y"])\n        ]\n        dlc_df[columns] = dlc_df[columns].round(0).astype(int)\n        # Changing the columns MultiIndex to a single-level index. For speedup\n        dlc_df.columns = [\n            f"{indiv}_{bpt}_{coord}" for indiv, bpt, coord in dlc_df.columns\n        ]\n        # Making the corresponding colours list for each bodypart instance\n        # (colours depend on indiv/bpt)\n        colours_i, _ = pd.factorize(indivs_bpts_ls.get_level_values(colour_level))\n        colours = (plt.get_cmap(cmap)(colours_i / colours_i.max()) * 255)[\n            :, [2, 1, 0, 3]\n        ]\n\n        # Getting behavs df\n        try:\n            behavs_df = BehavMixin.read_feather(behavs_fp)\n        except FileNotFoundError:\n            outcome += (\n                "WARNING: behavs file not found or could not be loaded."\n                + "Disregarding behaviour."\n                + "If you have run the behaviour classifier, please check this file.\\n"\n            )\n            behavs_df = BehavMixin.init_df(dlc_df.index)\n        # Getting list of behaviours\n        behavs_ls = behavs_df.columns.unique("behaviours")\n        # Making sure all relevant behaviour outcome columns exist\n        for behav in behavs_ls:\n            for i in BehavColumns:\n                i = i.value\n                if (behav, i) not in behavs_df:\n                    behavs_df[(behav, i)] = 0\n        # Changing the columns MultiIndex to a single-level index. For speedup\n        behavs_df.columns = [\n            f"{behav}_{outcome}" for behav, outcome in behavs_df.columns\n        ]\n\n        # MAKING ANNOTATED VIDEO\n        # Settings the funcs for how to annotate the video\n        funcs: list[Callable[[np.ndarray, int], np.ndarray]] = list()\n        for f_name in funcs_names:\n            if f_name == "johansson":\n                outcome += f"Added {f_name} to video. \\n"\n                funcs.append(lambda frame, i: annot_johansson(frame))\n            elif f_name == "keypoints":\n                outcome += f"Added {f_name} to video. \\n"\n                funcs.append(\n                    lambda frame, i: annot_keypoints(\n                        frame, dlc_df.loc[i], indivs_bpts_ls, colours, pcutoff, radius\n                    )\n                )\n            elif f_name == "behavs":\n                outcome += f"Added {f_name} to video. \\n"\n                funcs.append(\n                    lambda frame, i: annot_behav(frame, behavs_df.loc[i], behavs_ls)\n                )\n            else:\n                continue\n        # Open the input video\n        in_cap = cv2.VideoCapture(vid_fp)\n        fps = in_cap.get(cv2.CAP_PROP_FPS)\n        width = int(in_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(in_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        total_frames = int(in_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        # Define the codec and create VideoWriter object\n        out_cap = cv2.VideoWriter(\n            out_fp, cv2.VideoWriter_fourcc(*"mp4v"), fps, (width, height)\n        )\n        # Annotating each frame using the created functions\n        outcome += annotate(in_cap, out_cap, funcs, total_frames)\n        # Release video objects\n        in_cap.release()\n        out_cap.release()\n        # Returning outcome string\n        return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.Evaluate.behav_plot",
      title:
        "<code>behav_plot(vid_fp, dlc_fp, behavs_fp, out_dir, configs_fp, overwrite)</code>  <code>staticmethod</code>",
      text: '<p>Make behaviour evaluation plot of the predicted and actual behaviours through time.</p> Source code in <code>behavysis_pipeline/processes/evaluate.py</code> <pre><code>@staticmethod\ndef behav_plot(\n    vid_fp: str,\n    dlc_fp: str,\n    behavs_fp: str,\n    out_dir: str,\n    configs_fp: str,\n    overwrite: bool,\n) -&gt; str:\n    """\n    Make behaviour evaluation plot of the predicted and actual behaviours through time.\n    """\n    outcome = ""\n    name = IOMixin.get_name(behavs_fp)\n    out_dir = os.path.join(out_dir, Evaluate.behav_plot.__name__)\n    out_fp = os.path.join(out_dir, f"{name}.png")\n    os.makedirs(out_dir, exist_ok=True)\n    # If overwrite is False, checking if we should skip processing\n    if not overwrite and os.path.exists(out_fp):\n        return DiagnosticsMixin.warning_msg()\n\n    # Getting necessary config parameters\n    configs = ExperimentConfigs.read_json(configs_fp)\n    # configs_filt = configs.user.evaluate.behav_plot\n    fps = configs.auto.formatted_vid.fps\n\n    # Read the file\n    df = BehavMixin.read_feather(behavs_fp)\n    # Making data-long ways\n    df = (\n        df.stack([BehavCN.BEHAVIOURS.value, BehavCN.OUTCOMES.value])\n        .reset_index()\n        .rename(columns={0: "value"})\n    )\n    # Adding the timestamp column\n    df["timestamp"] = df[BehavIN.FRAME.value] / fps\n    # Making plot\n    g = sns.FacetGrid(\n        df,\n        row=BehavCN.BEHAVIOURS.value,\n        height=5,\n        aspect=10,\n    )\n    g.map_dataframe(\n        sns.lineplot,\n        x="timestamp",\n        y="value",\n        hue=BehavCN.OUTCOMES.value,\n        alpha=0.4,\n    )\n    g.add_legend()\n    # Saving plot\n    g.savefig(out_fp)\n    g.figure.clf()\n    # Returning outcome string\n    return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.Evaluate.eval_vid",
      title:
        "<code>eval_vid(vid_fp, dlc_fp, behavs_fp, out_dir, configs_fp, overwrite)</code>  <code>staticmethod</code>",
      text: '<p>Run the DLC model on the formatted video to generate a DLC annotated video and DLC file for all experiments. The DLC model\'s config.yaml filepath must be specified in the <code>config_path</code> parameter in the <code>user</code> section of the config file.</p> Source code in <code>behavysis_pipeline/processes/evaluate.py</code> <pre><code>@staticmethod\ndef eval_vid(\n    vid_fp: str,\n    dlc_fp: str,\n    behavs_fp: str,\n    out_dir: str,\n    configs_fp: str,\n    overwrite: bool,\n) -&gt; str:\n    """\n    Run the DLC model on the formatted video to generate a DLC annotated video and DLC file for\n    all experiments. The DLC model\'s config.yaml filepath must be specified in the `config_path`\n    parameter in the `user` section of the config file.\n    """\n\n    outcome = ""\n    name = IOMixin.get_name(vid_fp)\n    out_dir = os.path.join(out_dir, Evaluate.eval_vid.__name__)\n    out_fp = os.path.join(out_dir, f"{name}.mp4")\n    os.makedirs(out_dir, exist_ok=True)\n    # If overwrite is False, checking if we should skip processing\n    if not overwrite and os.path.exists(out_fp):\n        return DiagnosticsMixin.warning_msg()\n\n    # Getting necessary config parameters\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs_filt = configs.user.evaluate.eval_vid\n    funcs_names = configs.get_ref(configs_filt.funcs)\n    pcutoff = configs.get_ref(configs_filt.pcutoff)\n    colour_level = configs.get_ref(configs_filt.colour_level)\n    radius = configs.get_ref(configs_filt.radius)\n    cmap = configs.get_ref(configs_filt.cmap)\n\n    # Modifying dlc_df and making list of how to select dlc_df components to optimise processing\n    dlc_df = KeypointsMixin.clean_headings(KeypointsMixin.read_feather(dlc_fp))\n    # Filtering out IndivColumns.PROCESS.value columns\n    if IndivColumns.PROCESS.value in dlc_df.columns.unique("individuals"):\n        dlc_df.drop(columns=IndivColumns.PROCESS.value, level="individuals")\n    # Getting (indivs, bpts) MultiIndex\n    indivs_bpts_ls = dlc_df.columns.droplevel("coords").unique()\n    # Rounding and converting to correct dtypes - "x" and "y" values are ints\n    dlc_df = dlc_df.fillna(0)\n    columns = dlc_df.columns[\n        dlc_df.columns.get_level_values("coords").isin(["x", "y"])\n    ]\n    dlc_df[columns] = dlc_df[columns].round(0).astype(int)\n    # Changing the columns MultiIndex to a single-level index. For speedup\n    dlc_df.columns = [\n        f"{indiv}_{bpt}_{coord}" for indiv, bpt, coord in dlc_df.columns\n    ]\n    # Making the corresponding colours list for each bodypart instance\n    # (colours depend on indiv/bpt)\n    colours_i, _ = pd.factorize(indivs_bpts_ls.get_level_values(colour_level))\n    colours = (plt.get_cmap(cmap)(colours_i / colours_i.max()) * 255)[\n        :, [2, 1, 0, 3]\n    ]\n\n    # Getting behavs df\n    try:\n        behavs_df = BehavMixin.read_feather(behavs_fp)\n    except FileNotFoundError:\n        outcome += (\n            "WARNING: behavs file not found or could not be loaded."\n            + "Disregarding behaviour."\n            + "If you have run the behaviour classifier, please check this file.\\n"\n        )\n        behavs_df = BehavMixin.init_df(dlc_df.index)\n    # Getting list of behaviours\n    behavs_ls = behavs_df.columns.unique("behaviours")\n    # Making sure all relevant behaviour outcome columns exist\n    for behav in behavs_ls:\n        for i in BehavColumns:\n            i = i.value\n            if (behav, i) not in behavs_df:\n                behavs_df[(behav, i)] = 0\n    # Changing the columns MultiIndex to a single-level index. For speedup\n    behavs_df.columns = [\n        f"{behav}_{outcome}" for behav, outcome in behavs_df.columns\n    ]\n\n    # MAKING ANNOTATED VIDEO\n    # Settings the funcs for how to annotate the video\n    funcs: list[Callable[[np.ndarray, int], np.ndarray]] = list()\n    for f_name in funcs_names:\n        if f_name == "johansson":\n            outcome += f"Added {f_name} to video. \\n"\n            funcs.append(lambda frame, i: annot_johansson(frame))\n        elif f_name == "keypoints":\n            outcome += f"Added {f_name} to video. \\n"\n            funcs.append(\n                lambda frame, i: annot_keypoints(\n                    frame, dlc_df.loc[i], indivs_bpts_ls, colours, pcutoff, radius\n                )\n            )\n        elif f_name == "behavs":\n            outcome += f"Added {f_name} to video. \\n"\n            funcs.append(\n                lambda frame, i: annot_behav(frame, behavs_df.loc[i], behavs_ls)\n            )\n        else:\n            continue\n    # Open the input video\n    in_cap = cv2.VideoCapture(vid_fp)\n    fps = in_cap.get(cv2.CAP_PROP_FPS)\n    width = int(in_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(in_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(in_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    # Define the codec and create VideoWriter object\n    out_cap = cv2.VideoWriter(\n        out_fp, cv2.VideoWriter_fourcc(*"mp4v"), fps, (width, height)\n    )\n    # Annotating each frame using the created functions\n    outcome += annotate(in_cap, out_cap, funcs, total_frames)\n    # Release video objects\n    in_cap.release()\n    out_cap.release()\n    # Returning outcome string\n    return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.Evaluate.keypoints_plot",
      title:
        "<code>keypoints_plot(vid_fp, dlc_fp, behavs_fp, out_dir, configs_fp, overwrite)</code>  <code>staticmethod</code>",
      text: '<p>Make keypoints evaluation plot of likelihood of each bodypart through time.</p> Source code in <code>behavysis_pipeline/processes/evaluate.py</code> <pre><code>@staticmethod\ndef keypoints_plot(\n    vid_fp: str,\n    dlc_fp: str,\n    behavs_fp: str,\n    out_dir: str,\n    configs_fp: str,\n    overwrite: bool,\n) -&gt; str:\n    """\n    Make keypoints evaluation plot of likelihood of each bodypart through time.\n    """\n    outcome = ""\n    name = IOMixin.get_name(dlc_fp)\n    out_dir = os.path.join(out_dir, Evaluate.keypoints_plot.__name__)\n    out_fp = os.path.join(out_dir, f"{name}.png")\n    os.makedirs(out_dir, exist_ok=True)\n    # If overwrite is False, checking if we should skip processing\n    if not overwrite and os.path.exists(out_fp):\n        return DiagnosticsMixin.warning_msg()\n\n    # Getting necessary config parameters\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs_filt = configs.user.evaluate.keypoints_plot\n    bpts = configs.get_ref(configs_filt.bodyparts)\n    fps = configs.auto.formatted_vid.fps\n\n    # Read the file\n    df = KeypointsMixin.clean_headings(KeypointsMixin.read_feather(dlc_fp))\n    # Checking the bodyparts specified in the configs exist in the dataframe\n    KeypointsMixin.check_bpts_exist(df, bpts)\n    # Making data-long ways\n    idx = pd.IndexSlice\n    df = (\n        df.loc[:, idx[:, bpts]]\n        .stack([KeypointsCN.INDIVIDUALS.value, KeypointsCN.BODYPARTS.value])\n        .reset_index()\n    )\n    # Adding the timestamp column\n    df["timestamp"] = df[BehavIN.FRAME.value] / fps\n    # Making plot\n    g = sns.FacetGrid(\n        df,\n        row=KeypointsCN.INDIVIDUALS.value,\n        height=5,\n        aspect=10,\n    )\n    g.map_dataframe(\n        sns.lineplot,\n        x="timestamp",\n        y=Coords.LIKELIHOOD.value,\n        hue=KeypointsCN.BODYPARTS.value,\n        alpha=0.4,\n    )\n    g.add_legend()\n    # Saving plot\n    g.savefig(out_fp)\n    g.figure.clf()\n    # Returning outcome string\n    return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.ExtractFeatures",
      title: "<code>behavysis_pipeline.processes.ExtractFeatures</code>",
      text: '<p>summary</p> Source code in <code>behavysis_pipeline/processes/extract_features.py</code> <pre><code>class ExtractFeatures:\n    """__summary__"""\n\n    @staticmethod\n    @IOMixin.overwrite_check()\n    def extract_features(\n        dlc_fp: str,\n        out_fp: str,\n        configs_fp: str,\n        temp_dir: str,\n        overwrite: bool,\n    ) -&gt; str:\n        """\n        Extracting features from preprocessed DLC dataframe using SimBA\n        processes.\n\n        Parameters\n        ----------\n        dlc_fp : str\n            Preprocessed DLC filepath.\n        out_fp : str\n            Filepath to save extracted_features dataframe.\n        configs_fp : str\n            Configs JSON filepath.\n        temp_dir : str\n            Temporary directory path. Used during intermediate SimBA processes.\n        overwrite : bool\n            Whether to overwrite the out_fp file (if it exists).\n\n        Returns\n        -------\n        str\n            The outcome of the process.\n        """\n        outcome = ""\n        # Getting directory and file paths\n        name = IOMixin.get_name(dlc_fp)\n        cpid = MultiprocMixin.get_cpid()\n        configs_dir = os.path.split(configs_fp)[0]\n        simba_in_dir = os.path.join(temp_dir, f"input_{cpid}")\n        simba_dir = os.path.join(temp_dir, f"simba_proj_{cpid}")\n        features_from_dir = os.path.join(\n            simba_dir, "project_folder", "csv", "features_extracted"\n        )\n        # Preparing dlc dfs for input to SimBA project\n        os.makedirs(simba_in_dir, exist_ok=True)\n        simba_in_fp = os.path.join(simba_in_dir, f"{name}.csv")\n        # Selecting bodyparts for SimBA (8 bpts, 2 indivs)\n        df = KeypointsMixin.read_feather(dlc_fp)\n        df = select_cols(df, configs_fp)\n        # Saving dlc frame to place in the SimBA features extraction df\n        index = df.index\n        # Need to remove index name for SimBA to import correctly\n        df.index.name = None\n        # Saving as csv\n        df.to_csv(simba_in_fp)\n        # Removing simba folder (if it exists)\n        IOMixin.silent_rm(simba_dir)\n        # Running SimBA env and script to run SimBA feature extraction\n        outcome += run_simba_subproc(\n            simba_dir, simba_in_dir, configs_dir, temp_dir, cpid\n        )\n        # Exporting SimBA feature extraction csv to feather\n        simba_out_fp = os.path.join(features_from_dir, f"{name}.csv")\n        export_2_feather(simba_out_fp, out_fp, index)\n        # Removing temp folders (simba_in_dir, simba_dir)\n        IOMixin.silent_rm(simba_in_dir)\n        IOMixin.silent_rm(simba_dir)\n        # Returning outcome\n        return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.ExtractFeatures.extract_features",
      title:
        "<code>extract_features(dlc_fp, out_fp, configs_fp, temp_dir, overwrite)</code>  <code>staticmethod</code>",
      text: '<p>Extracting features from preprocessed DLC dataframe using SimBA processes.</p> <p>Parameters:</p> Name Type Description Default <code>dlc_fp</code> <code>str</code> <p>Preprocessed DLC filepath.</p> required <code>out_fp</code> <code>str</code> <p>Filepath to save extracted_features dataframe.</p> required <code>configs_fp</code> <code>str</code> <p>Configs JSON filepath.</p> required <code>temp_dir</code> <code>str</code> <p>Temporary directory path. Used during intermediate SimBA processes.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the out_fp file (if it exists).</p> required <p>Returns:</p> Type Description <code>str</code> <p>The outcome of the process.</p> Source code in <code>behavysis_pipeline/processes/extract_features.py</code> <pre><code>@staticmethod\n@IOMixin.overwrite_check()\ndef extract_features(\n    dlc_fp: str,\n    out_fp: str,\n    configs_fp: str,\n    temp_dir: str,\n    overwrite: bool,\n) -&gt; str:\n    """\n    Extracting features from preprocessed DLC dataframe using SimBA\n    processes.\n\n    Parameters\n    ----------\n    dlc_fp : str\n        Preprocessed DLC filepath.\n    out_fp : str\n        Filepath to save extracted_features dataframe.\n    configs_fp : str\n        Configs JSON filepath.\n    temp_dir : str\n        Temporary directory path. Used during intermediate SimBA processes.\n    overwrite : bool\n        Whether to overwrite the out_fp file (if it exists).\n\n    Returns\n    -------\n    str\n        The outcome of the process.\n    """\n    outcome = ""\n    # Getting directory and file paths\n    name = IOMixin.get_name(dlc_fp)\n    cpid = MultiprocMixin.get_cpid()\n    configs_dir = os.path.split(configs_fp)[0]\n    simba_in_dir = os.path.join(temp_dir, f"input_{cpid}")\n    simba_dir = os.path.join(temp_dir, f"simba_proj_{cpid}")\n    features_from_dir = os.path.join(\n        simba_dir, "project_folder", "csv", "features_extracted"\n    )\n    # Preparing dlc dfs for input to SimBA project\n    os.makedirs(simba_in_dir, exist_ok=True)\n    simba_in_fp = os.path.join(simba_in_dir, f"{name}.csv")\n    # Selecting bodyparts for SimBA (8 bpts, 2 indivs)\n    df = KeypointsMixin.read_feather(dlc_fp)\n    df = select_cols(df, configs_fp)\n    # Saving dlc frame to place in the SimBA features extraction df\n    index = df.index\n    # Need to remove index name for SimBA to import correctly\n    df.index.name = None\n    # Saving as csv\n    df.to_csv(simba_in_fp)\n    # Removing simba folder (if it exists)\n    IOMixin.silent_rm(simba_dir)\n    # Running SimBA env and script to run SimBA feature extraction\n    outcome += run_simba_subproc(\n        simba_dir, simba_in_dir, configs_dir, temp_dir, cpid\n    )\n    # Exporting SimBA feature extraction csv to feather\n    simba_out_fp = os.path.join(features_from_dir, f"{name}.csv")\n    export_2_feather(simba_out_fp, out_fp, index)\n    # Removing temp folders (simba_in_dir, simba_dir)\n    IOMixin.silent_rm(simba_in_dir)\n    IOMixin.silent_rm(simba_dir)\n    # Returning outcome\n    return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.FormatVid",
      title: "<code>behavysis_pipeline.processes.FormatVid</code>",
      text: '<p>Class for formatting videos based on given parameters.</p> Source code in <code>behavysis_pipeline/processes/format_vid.py</code> <pre><code>class FormatVid:\n    """\n    Class for formatting videos based on given parameters.\n    """\n\n    @staticmethod\n    @IOMixin.overwrite_check()\n    def format_vid(in_fp: str, out_fp: str, configs_fp: str, overwrite: bool) -&gt; str:\n        """\n        Formats the input video with the given parameters.\n\n        Parameters\n        ----------\n        in_fp : str\n            The input video filepath.\n        out_fp : str\n            The output video filepath.\n        configs_fp : str\n            The JSON configs filepath.\n        overwrite : bool\n            Whether to overwrite the output file (if it exists).\n\n        Returns\n        -------\n        str\n            Description of the function\'s outcome.\n        """\n        outcome = ""\n        # Finding all necessary config parameters for video formatting\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs_filt = configs.user.format_vid\n\n        # Processing the video\n        outcome += ProcessVidMixin.process_vid(\n            in_fp=in_fp,\n            out_fp=out_fp,\n            height_px=configs_filt.height_px,\n            width_px=configs_filt.width_px,\n            fps=configs_filt.fps,\n            start_sec=configs_filt.start_sec,\n            stop_sec=configs_filt.stop_sec,\n        )\n\n        # Saving video metadata to configs dict\n        outcome += FormatVid.get_vid_metadata(in_fp, out_fp, configs_fp, overwrite)\n        return outcome\n\n    @staticmethod\n    def get_vid_metadata(\n        in_fp: str, out_fp: str, configs_fp: str, overwrite: bool\n    ) -&gt; str:\n        """\n        Finds the video metadata/parameters for either the raw or formatted video,\n        and stores this data in the experiment\'s config file.\n\n        Parameters\n        ----------\n        in_fp : str\n            The input video filepath.\n        out_fp : str\n            The output video filepath.\n        configs_fp : str\n            The JSON configs filepath.\n        overwrite : bool\n            Whether to overwrite the output file (if it exists). IGNORED\n\n        Returns\n        -------\n        str\n            Description of the function\'s outcome.\n        """\n        outcome = ""\n\n        # Saving video metadata to configs dict\n        configs = ExperimentConfigs.read_json(configs_fp)\n        for ftype, fp in (("raw_vid", in_fp), ("formatted_vid", out_fp)):\n            try:\n                setattr(configs.auto, ftype, ProcessVidMixin.get_vid_metadata(fp))\n            except ValueError as e:\n                outcome += f"WARNING: {str(e)}\\n"\n        outcome += "Video metadata stored in config file.\\n"\n        configs.write_json(configs_fp)\n        return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.FormatVid.format_vid",
      title:
        "<code>format_vid(in_fp, out_fp, configs_fp, overwrite)</code>  <code>staticmethod</code>",
      text: '<p>Formats the input video with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>in_fp</code> <code>str</code> <p>The input video filepath.</p> required <code>out_fp</code> <code>str</code> <p>The output video filepath.</p> required <code>configs_fp</code> <code>str</code> <p>The JSON configs filepath.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file (if it exists).</p> required <p>Returns:</p> Type Description <code>str</code> <p>Description of the function\'s outcome.</p> Source code in <code>behavysis_pipeline/processes/format_vid.py</code> <pre><code>@staticmethod\n@IOMixin.overwrite_check()\ndef format_vid(in_fp: str, out_fp: str, configs_fp: str, overwrite: bool) -&gt; str:\n    """\n    Formats the input video with the given parameters.\n\n    Parameters\n    ----------\n    in_fp : str\n        The input video filepath.\n    out_fp : str\n        The output video filepath.\n    configs_fp : str\n        The JSON configs filepath.\n    overwrite : bool\n        Whether to overwrite the output file (if it exists).\n\n    Returns\n    -------\n    str\n        Description of the function\'s outcome.\n    """\n    outcome = ""\n    # Finding all necessary config parameters for video formatting\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs_filt = configs.user.format_vid\n\n    # Processing the video\n    outcome += ProcessVidMixin.process_vid(\n        in_fp=in_fp,\n        out_fp=out_fp,\n        height_px=configs_filt.height_px,\n        width_px=configs_filt.width_px,\n        fps=configs_filt.fps,\n        start_sec=configs_filt.start_sec,\n        stop_sec=configs_filt.stop_sec,\n    )\n\n    # Saving video metadata to configs dict\n    outcome += FormatVid.get_vid_metadata(in_fp, out_fp, configs_fp, overwrite)\n    return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.FormatVid.get_vid_metadata",
      title:
        "<code>get_vid_metadata(in_fp, out_fp, configs_fp, overwrite)</code>  <code>staticmethod</code>",
      text: '<p>Finds the video metadata/parameters for either the raw or formatted video, and stores this data in the experiment\'s config file.</p> <p>Parameters:</p> Name Type Description Default <code>in_fp</code> <code>str</code> <p>The input video filepath.</p> required <code>out_fp</code> <code>str</code> <p>The output video filepath.</p> required <code>configs_fp</code> <code>str</code> <p>The JSON configs filepath.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file (if it exists). IGNORED</p> required <p>Returns:</p> Type Description <code>str</code> <p>Description of the function\'s outcome.</p> Source code in <code>behavysis_pipeline/processes/format_vid.py</code> <pre><code>@staticmethod\ndef get_vid_metadata(\n    in_fp: str, out_fp: str, configs_fp: str, overwrite: bool\n) -&gt; str:\n    """\n    Finds the video metadata/parameters for either the raw or formatted video,\n    and stores this data in the experiment\'s config file.\n\n    Parameters\n    ----------\n    in_fp : str\n        The input video filepath.\n    out_fp : str\n        The output video filepath.\n    configs_fp : str\n        The JSON configs filepath.\n    overwrite : bool\n        Whether to overwrite the output file (if it exists). IGNORED\n\n    Returns\n    -------\n    str\n        Description of the function\'s outcome.\n    """\n    outcome = ""\n\n    # Saving video metadata to configs dict\n    configs = ExperimentConfigs.read_json(configs_fp)\n    for ftype, fp in (("raw_vid", in_fp), ("formatted_vid", out_fp)):\n        try:\n            setattr(configs.auto, ftype, ProcessVidMixin.get_vid_metadata(fp))\n        except ValueError as e:\n            outcome += f"WARNING: {str(e)}\\n"\n    outcome += "Video metadata stored in config file.\\n"\n    configs.write_json(configs_fp)\n    return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.Preprocess",
      title: "<code>behavysis_pipeline.processes.Preprocess</code>",
      text: '<p>summary</p> Source code in <code>behavysis_pipeline/processes/preprocess.py</code> <pre><code>class Preprocess:\n    """_summary_"""\n\n    @staticmethod\n    @IOMixin.overwrite_check()\n    def start_stop_trim(\n        in_fp: str, out_fp: str, configs_fp: str, overwrite: bool\n    ) -&gt; str:\n        """\n        Filters the rows of a DLC formatted dataframe to include only rows within the start\n        and end time of the experiment, given a corresponding configs dict.\n\n        Parameters\n        ----------\n        in_fp : str\n            The file path of the input DLC formatted dataframe.\n        out_fp : str\n            The file path of the output trimmed dataframe.\n        configs_fp : str\n            The file path of the configs dict.\n        overwrite : bool\n            If True, overwrite the output file if it already exists. If False, skip processing\n            if the output file already exists.\n\n        Returns\n        -------\n        str\n            An outcome message indicating the result of the trimming process.\n\n        Notes\n        -----\n        The config file must contain the following parameters:\n        ```\n        - user\n            - preprocess\n                - start_stop_trim\n                    - start_frame: int\n                    - stop_frame: int\n        ```\n        """\n        outcome = ""\n        # Getting necessary config parameters\n        configs = ExperimentConfigs.read_json(configs_fp)\n        start_frame = configs.auto.start_frame\n        stop_frame = configs.auto.stop_frame\n\n        # Reading file\n        df = KeypointsMixin.read_feather(in_fp)\n\n        # Trimming dataframe\n        df = df.loc[start_frame:stop_frame, :]\n\n        # Writing file\n        DFMixin.write_feather(df, out_fp)\n\n        return outcome\n\n    @staticmethod\n    @IOMixin.overwrite_check()\n    def interpolate(in_fp: str, out_fp: str, configs_fp: str, overwrite: bool) -&gt; str:\n        """\n        "Smooths" out noticeable jitter of points, where the likelihood (and accuracy) of\n        a point\'s coordinates are low (e.g., when the subject\'s head goes out of view). It\n        does this by linearly interpolating the frames of a body part that are below a given\n        likelihood pcutoff.\n\n        Notes\n        -----\n        The config file must contain the following parameters:\n        ```\n        - user\n            - preprocess\n                - interpolate\n                    - pcutoff: float\n        ```\n        """\n        outcome = ""\n        # Getting necessary config parameters\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs_filt = Model_interpolate(**configs.user.preprocess.interpolate)\n        # Reading file\n        df = KeypointsMixin.read_feather(in_fp)\n        # Gettings the unique groups of (individual, bodypart) groups.\n        unique_cols = df.columns.droplevel(["coords"]).unique()\n        # Setting low-likelihood points to Nan to later interpolate\n        for scorer, indiv, bp in unique_cols:\n            # Imputing Nan likelihood points with 0\n            df[(scorer, indiv, bp, Coords.LIKELIHOOD.value)].fillna(\n                value=0, inplace=True\n            )\n            # Setting x and y coordinates of points that have low likelihood to Nan\n            to_remove = (\n                df[(scorer, indiv, bp, Coords.LIKELIHOOD.value)] &lt; configs_filt.pcutoff\n            )\n            df.loc[to_remove, (scorer, indiv, bp, Coords.X.value)] = np.nan\n            df.loc[to_remove, (scorer, indiv, bp, Coords.Y.value)] = np.nan\n        # linearly interpolating Nan x and y points.\n        # Also backfilling points at the start.\n        # Also forward filling points at the end.\n        # Also imputing nan points with 0 (if the ENTIRE column is nan, then it\'s imputed)\n        df = df.interpolate(method="linear", axis=0).bfill().ffill()\n        # if df.isnull().values.any() then the entire column is nan (print warning)\n        df = df.fillna(0)\n        # Writing file\n        DFMixin.write_feather(df, out_fp)\n        return outcome\n\n    @staticmethod\n    @IOMixin.overwrite_check()\n    def refine_ids(in_fp: str, out_fp: str, configs_fp: str, overwrite: bool) -&gt; str:\n        """\n        Ensures that the identity is correctly tracked for maDLC.\n        Assumes interpolatePoints and calcBodyCentre has already been run.\n\n        Notes\n        -----\n        The config file must contain the following parameters:\n        ```\n        - user\n            - preprocess\n                - refine_ids\n                    - marked: str\n                    - unmarked: str\n                    - marking: str\n                    - window_sec: float\n                    - metric: ["current", "rolling", "binned"]\n        ```\n        """\n        outcome = ""\n        # Reading file\n        df = KeypointsMixin.read_feather(in_fp)\n        # Getting necessary config parameters\n        configs = ExperimentConfigs.read_json(configs_fp)\n        configs_filt = Model_refine_ids(**configs.user.preprocess.refine_ids)\n        marked = configs.get_ref(configs_filt.marked)\n        unmarked = configs.get_ref(configs_filt.unmarked)\n        marking = configs.get_ref(configs_filt.marking)\n        window_sec = configs.get_ref(configs_filt.window_sec)\n        bpts = configs.get_ref(configs_filt.bodyparts)\n        metric = configs.get_ref(configs_filt.metric)\n        fps = configs.auto.formatted_vid.fps\n        # Calculating more parameters\n        window_frames = int(np.round(fps * window_sec, 0))\n        # Error checking for invalid/non-existent column names marked, unmarked, and marking\n        for column, level in [\n            (marked, "individuals"),\n            (unmarked, "individuals"),\n            (marking, "bodyparts"),\n        ]:\n            if column not in df.columns.unique(level):\n                raise ValueError(\n                    f\'The marking value in the config file, "{column}",\'\n                    + " is not a column name in the DLC file."\n                )\n        # Checking that bodyparts are all valid\n        KeypointsMixin.check_bpts_exist(df, bpts)\n        # Calculating the distances between the bodycentres and the marking\n        df_aggr = aggregate_df(df, marking, [marked, unmarked], bpts)\n        # Getting "to_switch" decision series for each frame\n        df_switch = decice_switch(df_aggr, window_frames, marked, unmarked)\n        # Updating df with the switched values\n        df_switched = switch_identities(df, df_switch[metric], marked, unmarked)\n        # Writing to file\n        DFMixin.write_feather(df_switched, out_fp)\n        return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.Preprocess.interpolate",
      title:
        "<code>interpolate(in_fp, out_fp, configs_fp, overwrite)</code>  <code>staticmethod</code>",
      text: '<p>"Smooths" out noticeable jitter of points, where the likelihood (and accuracy) of a point\'s coordinates are low (e.g., when the subject\'s head goes out of view). It does this by linearly interpolating the frames of a body part that are below a given likelihood pcutoff.</p> Notes <p>The config file must contain the following parameters: <pre><code>- user\n    - preprocess\n        - interpolate\n            - pcutoff: float\n</code></pre></p> Source code in <code>behavysis_pipeline/processes/preprocess.py</code> <pre><code>@staticmethod\n@IOMixin.overwrite_check()\ndef interpolate(in_fp: str, out_fp: str, configs_fp: str, overwrite: bool) -&gt; str:\n    """\n    "Smooths" out noticeable jitter of points, where the likelihood (and accuracy) of\n    a point\'s coordinates are low (e.g., when the subject\'s head goes out of view). It\n    does this by linearly interpolating the frames of a body part that are below a given\n    likelihood pcutoff.\n\n    Notes\n    -----\n    The config file must contain the following parameters:\n    ```\n    - user\n        - preprocess\n            - interpolate\n                - pcutoff: float\n    ```\n    """\n    outcome = ""\n    # Getting necessary config parameters\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs_filt = Model_interpolate(**configs.user.preprocess.interpolate)\n    # Reading file\n    df = KeypointsMixin.read_feather(in_fp)\n    # Gettings the unique groups of (individual, bodypart) groups.\n    unique_cols = df.columns.droplevel(["coords"]).unique()\n    # Setting low-likelihood points to Nan to later interpolate\n    for scorer, indiv, bp in unique_cols:\n        # Imputing Nan likelihood points with 0\n        df[(scorer, indiv, bp, Coords.LIKELIHOOD.value)].fillna(\n            value=0, inplace=True\n        )\n        # Setting x and y coordinates of points that have low likelihood to Nan\n        to_remove = (\n            df[(scorer, indiv, bp, Coords.LIKELIHOOD.value)] &lt; configs_filt.pcutoff\n        )\n        df.loc[to_remove, (scorer, indiv, bp, Coords.X.value)] = np.nan\n        df.loc[to_remove, (scorer, indiv, bp, Coords.Y.value)] = np.nan\n    # linearly interpolating Nan x and y points.\n    # Also backfilling points at the start.\n    # Also forward filling points at the end.\n    # Also imputing nan points with 0 (if the ENTIRE column is nan, then it\'s imputed)\n    df = df.interpolate(method="linear", axis=0).bfill().ffill()\n    # if df.isnull().values.any() then the entire column is nan (print warning)\n    df = df.fillna(0)\n    # Writing file\n    DFMixin.write_feather(df, out_fp)\n    return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.Preprocess.refine_ids",
      title:
        "<code>refine_ids(in_fp, out_fp, configs_fp, overwrite)</code>  <code>staticmethod</code>",
      text: '<p>Ensures that the identity is correctly tracked for maDLC. Assumes interpolatePoints and calcBodyCentre has already been run.</p> Notes <p>The config file must contain the following parameters: <pre><code>- user\n    - preprocess\n        - refine_ids\n            - marked: str\n            - unmarked: str\n            - marking: str\n            - window_sec: float\n            - metric: ["current", "rolling", "binned"]\n</code></pre></p> Source code in <code>behavysis_pipeline/processes/preprocess.py</code> <pre><code>@staticmethod\n@IOMixin.overwrite_check()\ndef refine_ids(in_fp: str, out_fp: str, configs_fp: str, overwrite: bool) -&gt; str:\n    """\n    Ensures that the identity is correctly tracked for maDLC.\n    Assumes interpolatePoints and calcBodyCentre has already been run.\n\n    Notes\n    -----\n    The config file must contain the following parameters:\n    ```\n    - user\n        - preprocess\n            - refine_ids\n                - marked: str\n                - unmarked: str\n                - marking: str\n                - window_sec: float\n                - metric: ["current", "rolling", "binned"]\n    ```\n    """\n    outcome = ""\n    # Reading file\n    df = KeypointsMixin.read_feather(in_fp)\n    # Getting necessary config parameters\n    configs = ExperimentConfigs.read_json(configs_fp)\n    configs_filt = Model_refine_ids(**configs.user.preprocess.refine_ids)\n    marked = configs.get_ref(configs_filt.marked)\n    unmarked = configs.get_ref(configs_filt.unmarked)\n    marking = configs.get_ref(configs_filt.marking)\n    window_sec = configs.get_ref(configs_filt.window_sec)\n    bpts = configs.get_ref(configs_filt.bodyparts)\n    metric = configs.get_ref(configs_filt.metric)\n    fps = configs.auto.formatted_vid.fps\n    # Calculating more parameters\n    window_frames = int(np.round(fps * window_sec, 0))\n    # Error checking for invalid/non-existent column names marked, unmarked, and marking\n    for column, level in [\n        (marked, "individuals"),\n        (unmarked, "individuals"),\n        (marking, "bodyparts"),\n    ]:\n        if column not in df.columns.unique(level):\n            raise ValueError(\n                f\'The marking value in the config file, "{column}",\'\n                + " is not a column name in the DLC file."\n            )\n    # Checking that bodyparts are all valid\n    KeypointsMixin.check_bpts_exist(df, bpts)\n    # Calculating the distances between the bodycentres and the marking\n    df_aggr = aggregate_df(df, marking, [marked, unmarked], bpts)\n    # Getting "to_switch" decision series for each frame\n    df_switch = decice_switch(df_aggr, window_frames, marked, unmarked)\n    # Updating df with the switched values\n    df_switched = switch_identities(df, df_switch[metric], marked, unmarked)\n    # Writing to file\n    DFMixin.write_feather(df_switched, out_fp)\n    return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.Preprocess.start_stop_trim",
      title:
        "<code>start_stop_trim(in_fp, out_fp, configs_fp, overwrite)</code>  <code>staticmethod</code>",
      text: '<p>Filters the rows of a DLC formatted dataframe to include only rows within the start and end time of the experiment, given a corresponding configs dict.</p> <p>Parameters:</p> Name Type Description Default <code>in_fp</code> <code>str</code> <p>The file path of the input DLC formatted dataframe.</p> required <code>out_fp</code> <code>str</code> <p>The file path of the output trimmed dataframe.</p> required <code>configs_fp</code> <code>str</code> <p>The file path of the configs dict.</p> required <code>overwrite</code> <code>bool</code> <p>If True, overwrite the output file if it already exists. If False, skip processing if the output file already exists.</p> required <p>Returns:</p> Type Description <code>str</code> <p>An outcome message indicating the result of the trimming process.</p> Notes <p>The config file must contain the following parameters: <pre><code>- user\n    - preprocess\n        - start_stop_trim\n            - start_frame: int\n            - stop_frame: int\n</code></pre></p> Source code in <code>behavysis_pipeline/processes/preprocess.py</code> <pre><code>@staticmethod\n@IOMixin.overwrite_check()\ndef start_stop_trim(\n    in_fp: str, out_fp: str, configs_fp: str, overwrite: bool\n) -&gt; str:\n    """\n    Filters the rows of a DLC formatted dataframe to include only rows within the start\n    and end time of the experiment, given a corresponding configs dict.\n\n    Parameters\n    ----------\n    in_fp : str\n        The file path of the input DLC formatted dataframe.\n    out_fp : str\n        The file path of the output trimmed dataframe.\n    configs_fp : str\n        The file path of the configs dict.\n    overwrite : bool\n        If True, overwrite the output file if it already exists. If False, skip processing\n        if the output file already exists.\n\n    Returns\n    -------\n    str\n        An outcome message indicating the result of the trimming process.\n\n    Notes\n    -----\n    The config file must contain the following parameters:\n    ```\n    - user\n        - preprocess\n            - start_stop_trim\n                - start_frame: int\n                - stop_frame: int\n    ```\n    """\n    outcome = ""\n    # Getting necessary config parameters\n    configs = ExperimentConfigs.read_json(configs_fp)\n    start_frame = configs.auto.start_frame\n    stop_frame = configs.auto.stop_frame\n\n    # Reading file\n    df = KeypointsMixin.read_feather(in_fp)\n\n    # Trimming dataframe\n    df = df.loc[start_frame:stop_frame, :]\n\n    # Writing file\n    DFMixin.write_feather(df, out_fp)\n\n    return outcome\n</code></pre>',
    },
    {
      location: "reference/processes.html#behavysis_pipeline.processes.RunDLC",
      title: "<code>behavysis_pipeline.processes.RunDLC</code>",
      text: '<p>summary</p> Source code in <code>behavysis_pipeline/processes/run_dlc.py</code> <pre><code>class RunDLC:\n    """_summary_"""\n\n    @staticmethod\n    @IOMixin.overwrite_check()\n    def ma_dlc_analyse_single(\n        in_fp: str,\n        out_fp: str,\n        configs_fp: str,\n        temp_dir: str,\n        gputouse: int | None,\n        overwrite: bool,\n    ) -&gt; str:\n        """\n        Running custom DLC script to generate a DLC keypoints dataframe from a single video.\n        """\n        outcome = ""\n        # Specifying the GPU to use\n        gputouse = "None" if not gputouse else gputouse\n        # Getting model_fp\n        configs = ExperimentConfigs.read_json(configs_fp)\n        model_fp = configs.get_ref(configs.user.run_dlc.model_fp)\n        # Derive more parameters\n        dlc_out_dir = os.path.join(temp_dir, f"dlc_{gputouse}")\n        out_dir = os.path.dirname(out_fp)\n        # Making output directories\n        os.makedirs(dlc_out_dir, exist_ok=True)\n\n        # Assertion: the config.yaml file must exist.\n        if not os.path.isfile(model_fp):\n            raise ValueError(\n                f\'The given model_fp file does not exist: "{model_fp}".\\n\'\n                + \'Check this file and specify a DLC ".yaml" config file.\'\n            )\n\n        # Running the DLC subprocess (in a separate conda env)\n        run_dlc_subproc(model_fp, [in_fp], dlc_out_dir, temp_dir, gputouse)\n\n        # Exporting the h5 to feather the out_dir\n        export_2_feather(in_fp, dlc_out_dir, out_dir)\n        # IOMixin.silent_rm(dlc_out_dir)\n\n        return outcome\n\n    @staticmethod\n    def ma_dlc_analyse_batch(\n        in_fp_ls: list[str],\n        out_dir: str,\n        configs_dir: str,\n        temp_dir: str,\n        gputouse: int | None,\n        overwrite: bool,\n    ) -&gt; str:\n        """\n        Running custom DLC script to generate a DLC keypoints dataframe from a single video.\n        """\n        outcome = ""\n\n        # Specifying the GPU to use\n        # and making the output directory\n        if not gputouse:\n            gputouse = "None"\n        # Making output directories\n        dlc_out_dir = os.path.join(temp_dir, f"dlc_{gputouse}")\n        os.makedirs(dlc_out_dir, exist_ok=True)\n\n        # If overwrite is False, filtering for only experiments that need processing\n        if not overwrite:\n            # Getting only the in_fp_ls elements that do not exist in out_dir\n            in_fp_ls = [\n                i\n                for i in in_fp_ls\n                if not os.path.exists(\n                    os.path.join(out_dir, f"{IOMixin.get_name(i)}.feather")\n                )\n            ]\n\n        # If there are no videos to process, return\n        if len(in_fp_ls) == 0:\n            return outcome\n\n        # Getting the DLC model config path\n        # Getting the names of the files that need processing\n        dlc_fp_ls = [IOMixin.get_name(i) for i in in_fp_ls]\n        # Getting their corresponding configs_fp\n        dlc_fp_ls = [os.path.join(configs_dir, f"{i}.json") for i in dlc_fp_ls]\n        # Reading their configs\n        dlc_fp_ls = [ExperimentConfigs.read_json(i) for i in dlc_fp_ls]\n        # Getting their model_fp\n        dlc_fp_ls = [i.user.run_dlc.model_fp for i in dlc_fp_ls]\n        # Converting to a set\n        dlc_fp_set = set(dlc_fp_ls)\n        # Assertion: all model_fp must be the same\n        assert len(dlc_fp_set) == 1\n        # Getting the model_fp\n        model_fp = dlc_fp_set.pop()\n        # Assertion: the config.yaml file must exist.\n        assert os.path.isfile(model_fp), (\n            f\'The given model_fp file does not exist: "{model_fp}".\\n\'\n            + \'Check this file and specify a DLC ".yaml" config file.\'\n        )\n\n        # Running the DLC subprocess (in a separate conda env)\n        run_dlc_subproc(model_fp, in_fp_ls, dlc_out_dir, temp_dir, gputouse)\n\n        # Exporting the h5 to feather the out_dir\n        for in_fp in in_fp_ls:\n            export_2_feather(in_fp, dlc_out_dir, out_dir)\n        IOMixin.silent_rm(dlc_out_dir)\n        # Returning outcome\n        return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.RunDLC.ma_dlc_analyse_batch",
      title:
        "<code>ma_dlc_analyse_batch(in_fp_ls, out_dir, configs_dir, temp_dir, gputouse, overwrite)</code>  <code>staticmethod</code>",
      text: '<p>Running custom DLC script to generate a DLC keypoints dataframe from a single video.</p> Source code in <code>behavysis_pipeline/processes/run_dlc.py</code> <pre><code>@staticmethod\ndef ma_dlc_analyse_batch(\n    in_fp_ls: list[str],\n    out_dir: str,\n    configs_dir: str,\n    temp_dir: str,\n    gputouse: int | None,\n    overwrite: bool,\n) -&gt; str:\n    """\n    Running custom DLC script to generate a DLC keypoints dataframe from a single video.\n    """\n    outcome = ""\n\n    # Specifying the GPU to use\n    # and making the output directory\n    if not gputouse:\n        gputouse = "None"\n    # Making output directories\n    dlc_out_dir = os.path.join(temp_dir, f"dlc_{gputouse}")\n    os.makedirs(dlc_out_dir, exist_ok=True)\n\n    # If overwrite is False, filtering for only experiments that need processing\n    if not overwrite:\n        # Getting only the in_fp_ls elements that do not exist in out_dir\n        in_fp_ls = [\n            i\n            for i in in_fp_ls\n            if not os.path.exists(\n                os.path.join(out_dir, f"{IOMixin.get_name(i)}.feather")\n            )\n        ]\n\n    # If there are no videos to process, return\n    if len(in_fp_ls) == 0:\n        return outcome\n\n    # Getting the DLC model config path\n    # Getting the names of the files that need processing\n    dlc_fp_ls = [IOMixin.get_name(i) for i in in_fp_ls]\n    # Getting their corresponding configs_fp\n    dlc_fp_ls = [os.path.join(configs_dir, f"{i}.json") for i in dlc_fp_ls]\n    # Reading their configs\n    dlc_fp_ls = [ExperimentConfigs.read_json(i) for i in dlc_fp_ls]\n    # Getting their model_fp\n    dlc_fp_ls = [i.user.run_dlc.model_fp for i in dlc_fp_ls]\n    # Converting to a set\n    dlc_fp_set = set(dlc_fp_ls)\n    # Assertion: all model_fp must be the same\n    assert len(dlc_fp_set) == 1\n    # Getting the model_fp\n    model_fp = dlc_fp_set.pop()\n    # Assertion: the config.yaml file must exist.\n    assert os.path.isfile(model_fp), (\n        f\'The given model_fp file does not exist: "{model_fp}".\\n\'\n        + \'Check this file and specify a DLC ".yaml" config file.\'\n    )\n\n    # Running the DLC subprocess (in a separate conda env)\n    run_dlc_subproc(model_fp, in_fp_ls, dlc_out_dir, temp_dir, gputouse)\n\n    # Exporting the h5 to feather the out_dir\n    for in_fp in in_fp_ls:\n        export_2_feather(in_fp, dlc_out_dir, out_dir)\n    IOMixin.silent_rm(dlc_out_dir)\n    # Returning outcome\n    return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.RunDLC.ma_dlc_analyse_single",
      title:
        "<code>ma_dlc_analyse_single(in_fp, out_fp, configs_fp, temp_dir, gputouse, overwrite)</code>  <code>staticmethod</code>",
      text: '<p>Running custom DLC script to generate a DLC keypoints dataframe from a single video.</p> Source code in <code>behavysis_pipeline/processes/run_dlc.py</code> <pre><code>@staticmethod\n@IOMixin.overwrite_check()\ndef ma_dlc_analyse_single(\n    in_fp: str,\n    out_fp: str,\n    configs_fp: str,\n    temp_dir: str,\n    gputouse: int | None,\n    overwrite: bool,\n) -&gt; str:\n    """\n    Running custom DLC script to generate a DLC keypoints dataframe from a single video.\n    """\n    outcome = ""\n    # Specifying the GPU to use\n    gputouse = "None" if not gputouse else gputouse\n    # Getting model_fp\n    configs = ExperimentConfigs.read_json(configs_fp)\n    model_fp = configs.get_ref(configs.user.run_dlc.model_fp)\n    # Derive more parameters\n    dlc_out_dir = os.path.join(temp_dir, f"dlc_{gputouse}")\n    out_dir = os.path.dirname(out_fp)\n    # Making output directories\n    os.makedirs(dlc_out_dir, exist_ok=True)\n\n    # Assertion: the config.yaml file must exist.\n    if not os.path.isfile(model_fp):\n        raise ValueError(\n            f\'The given model_fp file does not exist: "{model_fp}".\\n\'\n            + \'Check this file and specify a DLC ".yaml" config file.\'\n        )\n\n    # Running the DLC subprocess (in a separate conda env)\n    run_dlc_subproc(model_fp, [in_fp], dlc_out_dir, temp_dir, gputouse)\n\n    # Exporting the h5 to feather the out_dir\n    export_2_feather(in_fp, dlc_out_dir, out_dir)\n    # IOMixin.silent_rm(dlc_out_dir)\n\n    return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.UpdateConfigs",
      title: "<code>behavysis_pipeline.processes.UpdateConfigs</code>",
      text: '<p>summary</p> Source code in <code>behavysis_pipeline/processes/update_configs.py</code> <pre><code>class UpdateConfigs:\n    """_summary_"""\n\n    @staticmethod\n    def update_configs(\n        configs_fp: str,\n        default_configs_fp: str,\n        overwrite: Literal["user", "all"],\n    ) -&gt; str:\n        """\n        Initialises the config files with the given `default_configs`.\n        The different types of overwriting are:\n        - "user": Only the user parameters are updated.\n        - "all": All parameters are updated.\n\n        Parameters\n        ----------\n        configs_fp : str\n            The filepath of the existing config file.\n        default_configs_fp : str\n            The filepath of the default config file to use.\n        overwrite : Literal["user", "all"]\n            Specifies how to update the config files.\n\n        Returns\n        -------\n        str\n            Description of the function\'s outcome.\n        """\n        outcome = ""\n        # Parsing in the experiment\'s existing JSON configs\n        try:\n            configs = ExperimentConfigs.read_json(configs_fp)\n        except (FileNotFoundError, ValidationError):\n            configs = ExperimentConfigs()\n        # Reading in the new configs from the given configs_fp\n        default_configs = ExperimentConfigs.read_json(default_configs_fp)\n        # Overwriting the configs file (with given method)\n        if overwrite == "user":\n            configs.user = default_configs.user\n            configs.ref = default_configs.ref\n            outcome += "Updating user and ref configs.\\n"\n        elif overwrite == "all":\n            configs = default_configs\n            outcome += "Updating all configs.\\n"\n        else:\n            raise ValueError(\n                f\'Invalid value "{overwrite}" passed to function. \'\n                + \'The value must be either "user", or "all".\'\n            )\n        # Writing new configs to JSON file\n        configs.write_json(configs_fp)\n        return outcome\n</code></pre>',
    },
    {
      location:
        "reference/processes.html#behavysis_pipeline.processes.UpdateConfigs.update_configs",
      title:
        "<code>update_configs(configs_fp, default_configs_fp, overwrite)</code>  <code>staticmethod</code>",
      text: '<p>Initialises the config files with the given <code>default_configs</code>. The different types of overwriting are: - "user": Only the user parameters are updated. - "all": All parameters are updated.</p> <p>Parameters:</p> Name Type Description Default <code>configs_fp</code> <code>str</code> <p>The filepath of the existing config file.</p> required <code>default_configs_fp</code> <code>str</code> <p>The filepath of the default config file to use.</p> required <code>overwrite</code> <code>Literal[\'user\', \'all\']</code> <p>Specifies how to update the config files.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Description of the function\'s outcome.</p> Source code in <code>behavysis_pipeline/processes/update_configs.py</code> <pre><code>@staticmethod\ndef update_configs(\n    configs_fp: str,\n    default_configs_fp: str,\n    overwrite: Literal["user", "all"],\n) -&gt; str:\n    """\n    Initialises the config files with the given `default_configs`.\n    The different types of overwriting are:\n    - "user": Only the user parameters are updated.\n    - "all": All parameters are updated.\n\n    Parameters\n    ----------\n    configs_fp : str\n        The filepath of the existing config file.\n    default_configs_fp : str\n        The filepath of the default config file to use.\n    overwrite : Literal["user", "all"]\n        Specifies how to update the config files.\n\n    Returns\n    -------\n    str\n        Description of the function\'s outcome.\n    """\n    outcome = ""\n    # Parsing in the experiment\'s existing JSON configs\n    try:\n        configs = ExperimentConfigs.read_json(configs_fp)\n    except (FileNotFoundError, ValidationError):\n        configs = ExperimentConfigs()\n    # Reading in the new configs from the given configs_fp\n    default_configs = ExperimentConfigs.read_json(default_configs_fp)\n    # Overwriting the configs file (with given method)\n    if overwrite == "user":\n        configs.user = default_configs.user\n        configs.ref = default_configs.ref\n        outcome += "Updating user and ref configs.\\n"\n    elif overwrite == "all":\n        configs = default_configs\n        outcome += "Updating all configs.\\n"\n    else:\n        raise ValueError(\n            f\'Invalid value "{overwrite}" passed to function. \'\n            + \'The value must be either "user", or "all".\'\n        )\n    # Writing new configs to JSON file\n    configs.write_json(configs_fp)\n    return outcome\n</code></pre>',
    },
    { location: "reference/project.html", title: "Project", text: "" },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project",
      title: "<code>behavysis_pipeline.pipeline.project.Project</code>",
      text: '<p>A project is used to process and analyse many experiments at the same time.</p> <p>Expected filesystem hierarchy of project directory is below: <pre><code>    - dir\n        - 0_configs\n            - exp1.json\n            - exp2.json\n            - ...\n        - 1_raw_vid\n            - .mp4\n            - exp2.mp4\n            - ...\n        - 2_formatted_vid\n            - exp1.mp4\n            - exp2.mp4\n            - ...\n        - 3_dlc\n            - exp1.feather\n            - exp2.feather\n            - ...\n        - 4_preprocessed\n            - exp1.feather\n            - exp2.feather\n            - ...\n        - 5_features_extracted\n            - exp1.feather\n            - exp2.feather\n            - ...\n        - 6_predicted_behavs\n            - exp1.feather\n            - exp2.feather\n            - ...\n        - 7_scored_behavs\n            - exp1.feather\n            - exp2.feather\n            - ...\n        - diagnostics\n            - &lt;outputs for every tranformation&gt;.csv\n        - analysis\n            - thigmotaxis\n                - fbf\n                    - exp1.feather\n                    - exp2.feather\n                    - ...\n                - summary\n                    - exp1.feather\n                    - exp2.feather\n                    - ...\n                    - __ALL.feather\n                - binned_5\n                    - exp1.feather\n                    - exp2.feather\n                    - ...\n                    - __ALL.feather\n                - binned_5_plot\n                    - exp1.feather\n                    - exp2.feather\n                    - ...\n                    - __ALL.feather\n                - binned_30\n                    - exp1.feather\n                    - exp2.feather\n                    - ...\n                    - __ALL.feather\n                - binned_30_plot\n                    - exp1.feather\n                    - exp2.feather\n                    - ...\n                    - __ALL.feather\n                - binned_custom\n                    - exp1.feather\n                    - exp2.feather\n                    - ...\n                    - __ALL.feather\n                - binned_custom_plot\n                    - exp1.feather\n                    - exp2.feather\n                    - ...\n                    - __ALL.feather\n            - speed\n                - fbf\n                - summary\n                - binned_5\n                - binned_5_plot\n                - ...\n            - EPM\n            - SFC\n            - 3Chamber\n            - Withdrawal\n            - ...\n        - evaluate\n            - keypoints_plot\n                - exp1.feather\n                - exp2.feather\n                - ...\n            - eval_vid\n                - exp1.feather\n                - exp2.feather\n                - ...\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>root_dir</code> <code>str</code> <pre><code>The filepath of the project directory. Can be relative to\ncurrent dir or absolute dir.\n</code></pre> <p>experiments : dict[str, Experiment]     The experiments that have been loaded into the project. nprocs : int     The number of processes to use for multiprocessing.</p> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>class Project:\n    """\n    A project is used to process and analyse many experiments at the same time.\n\n    Expected filesystem hierarchy of project directory is below:\n    ```\n        - dir\n            - 0_configs\n                - exp1.json\n                - exp2.json\n                - ...\n            - 1_raw_vid\n                - .mp4\n                - exp2.mp4\n                - ...\n            - 2_formatted_vid\n                - exp1.mp4\n                - exp2.mp4\n                - ...\n            - 3_dlc\n                - exp1.feather\n                - exp2.feather\n                - ...\n            - 4_preprocessed\n                - exp1.feather\n                - exp2.feather\n                - ...\n            - 5_features_extracted\n                - exp1.feather\n                - exp2.feather\n                - ...\n            - 6_predicted_behavs\n                - exp1.feather\n                - exp2.feather\n                - ...\n            - 7_scored_behavs\n                - exp1.feather\n                - exp2.feather\n                - ...\n            - diagnostics\n                - &lt;outputs for every tranformation&gt;.csv\n            - analysis\n                - thigmotaxis\n                    - fbf\n                        - exp1.feather\n                        - exp2.feather\n                        - ...\n                    - summary\n                        - exp1.feather\n                        - exp2.feather\n                        - ...\n                        - __ALL.feather\n                    - binned_5\n                        - exp1.feather\n                        - exp2.feather\n                        - ...\n                        - __ALL.feather\n                    - binned_5_plot\n                        - exp1.feather\n                        - exp2.feather\n                        - ...\n                        - __ALL.feather\n                    - binned_30\n                        - exp1.feather\n                        - exp2.feather\n                        - ...\n                        - __ALL.feather\n                    - binned_30_plot\n                        - exp1.feather\n                        - exp2.feather\n                        - ...\n                        - __ALL.feather\n                    - binned_custom\n                        - exp1.feather\n                        - exp2.feather\n                        - ...\n                        - __ALL.feather\n                    - binned_custom_plot\n                        - exp1.feather\n                        - exp2.feather\n                        - ...\n                        - __ALL.feather\n                - speed\n                    - fbf\n                    - summary\n                    - binned_5\n                    - binned_5_plot\n                    - ...\n                - EPM\n                - SFC\n                - 3Chamber\n                - Withdrawal\n                - ...\n            - evaluate\n                - keypoints_plot\n                    - exp1.feather\n                    - exp2.feather\n                    - ...\n                - eval_vid\n                    - exp1.feather\n                    - exp2.feather\n                    - ...\n    ```\n\n    Attributes\n    ----------\n        root_dir : str\n            The filepath of the project directory. Can be relative to\n            current dir or absolute dir.\n        experiments : dict[str, Experiment]\n            The experiments that have been loaded into the project.\n        nprocs : int\n            The number of processes to use for multiprocessing.\n    """\n\n    root_dir: str\n    experiments: dict[str, Experiment]\n    nprocs: int\n\n    def __init__(self, root_dir: str) -&gt; None:\n        """\n        Make a Project instance.\n\n        Parameters\n        ----------\n        root_dir : str\n            The filepath of the project directory. Can be relative to\n            current dir or absolute dir.\n        """\n        # Assertion: project directory must exist\n        if not os.path.isdir(root_dir):\n            raise ValueError(\n                f\'Error: The folder, "{root_dir}" does not exist.\\n\'\n                + "Please specify a folder that exists. Ensure you have the correct"\n                + "forward-slashes or back-slashes for the path name."\n            )\n        self.root_dir = os.path.abspath(root_dir)\n        self.experiments = {}\n        self.nprocs = 4\n\n    #####################################################################\n    #               MAKING PROJECTS\n    #####################################################################\n\n    @classmethod\n    def make_project(clf, root_dir, overwrite: bool = False) -&gt; None:\n        """\n        Makes a project instance and imports all experiments in the project folder.\n\n        Also copies the `run.py` script and `default_configs.json` to the root_dir.\n        """\n        # Making the project root folder\n        os.makedirs(root_dir, exist_ok=True)\n        # Making the project instance\n        proj = clf(root_dir)\n        # Making each subfolder\n        for f in Folders:\n            os.makedirs(os.path.join(proj.root_dir, f.value), exist_ok=True)\n        # Copying the default_configs.json and run.py files to the project folder\n        for i in ["default_configs.json", "run.py"]:\n            # Getting the file path\n            dst_fp = os.path.join(proj.root_dir, i)\n            # If not overwrite and file exists, then don\'t overwrite\n            if not overwrite and os.path.exists(dst_fp):\n                continue\n            # Saving the template to the file\n            IOMixin.save_template(\n                i,\n                "behavysis_pipeline",\n                "script_templates",\n                dst_fp,\n            )\n        # returning the project instance\n        return proj\n\n    #####################################################################\n    #               GETTER METHODS\n    #####################################################################\n\n    def get_experiment(self, name: str) -&gt; Experiment:\n        """\n        Gets the experiment with the given name\n\n        Parameters\n        ----------\n        name : str\n            The experiment name.\n\n        Returns\n        -------\n        Experiment\n            The experiment.\n\n        Raises\n        ------\n        ValueError\n            Experiment with the given name does not exist.\n        """\n        if name in self.experiments:\n            return self.experiments[name]\n        raise ValueError(\n            f\'Experiment with the name "{name}" does not exist in the project.\'\n        )\n\n    def get_experiments(self) -&gt; list[Experiment]:\n        """\n        Gets the ordered (natsorted) list of Experiment instances in the Project.\n\n        Returns\n        -------\n        list[Experiment]\n            The list of all Experiment instances stored in the Project instance.\n        """\n        return [self.experiments[i] for i in natsorted(self.experiments)]\n\n    #####################################################################\n    #               PROJECT PROCESSING SCAFFOLD METHODS\n    #####################################################################\n\n    @staticmethod\n    def _process_scaffold_mp_worker(args_tuple: tuple):\n        method, exp, args, kwargs = args_tuple\n        return method(exp, *args, **kwargs)\n\n    def _process_scaffold_mp(\n        self, method: Callable, *args: Any, **kwargs: Any\n    ) -&gt; list[dict]:\n        """\n        Processes an experiment with the given `Experiment` method and records\n        the diagnostics of the process in a MULTI-PROCESSING way.\n\n        Parameters\n        ----------\n        method : Callable\n            The `Experiment` class method to run.\n\n        Notes\n        -----\n        Can call any `Experiment` methods instance.\n        Effectively, `method` gets called with:\n        ```\n        # exp is a Experiment instance\n        method(exp, *args, **kwargs)\n        ```\n        """\n        # Create a Pool of processes\n        with Pool(processes=self.nprocs) as p:\n            # Apply method to each experiment in self.get_experiments() in parallel\n            return p.map(\n                Project._process_scaffold_mp_worker,\n                [(method, exp, args, kwargs) for exp in self.get_experiments()],\n            )\n\n    def _process_scaffold_sp(\n        self, method: Callable, *args: Any, **kwargs: Any\n    ) -&gt; list[dict]:\n        """\n        Processes an experiment with the given `Experiment` method and records\n        the diagnostics of the process in a SINGLE-PROCESSING way.\n\n        Parameters\n        ----------\n        method : Callable\n            The experiment `Experiment` class method to run.\n\n        Notes\n        -----\n        Can call any `Experiment` instance method.\n        Effectively, `method` gets called with:\n        ```\n        # exp is a Experiment instance\n        method(exp, *args, **kwargs)\n        ```\n        """\n        # Processing all experiments and storing process outcomes as list of dicts\n        return [method(exp, *args, **kwargs) for exp in self.get_experiments()]\n\n    def _process_scaffold(self, method: Callable, *args: Any, **kwargs: Any) -&gt; None:\n        """\n        Runs the given method on all experiments in the project.\n        """\n        # Choosing whether to run the scaffold function in single or multi-processing mode\n        if self.nprocs == 1:\n            scaffold_func = self._process_scaffold_sp\n        else:\n            scaffold_func = self._process_scaffold_mp\n        # Running the scaffold function\n        # Starting\n        logging.info("Running %s", method.__name__)\n        # Running\n        dd_ls = scaffold_func(method, *args, **kwargs)\n        # Processing all experiments\n        df = (\n            pd.DataFrame(dd_ls).set_index("experiment").sort_index(key=natsort_keygen())\n        )\n        # Updating the diagnostics file at each step\n        self.save_diagnostics(method.__name__, df)\n        # Finishing\n        logging.info("Finished %s!\\n%s\\n%s\\n", method.__name__, STR_DIV, STR_DIV)\n\n    #####################################################################\n    #               BATCH PROCESSING METHODS\n    #####################################################################\n\n    @functools.wraps(Experiment.update_configs)\n    def update_configs(self, *args, **kwargs) -&gt; None:\n        """\n        Batch processing corresponding to\n        [behavysis_pipeline.pipeline.experiment.Experiment.update_configs][]\n\n        Parameters\n        ----------\n        *args : tuple\n            args passed to process scaffold method.\n        **kwargs : dict\n            keyword args passed to process scaffold method.\n        """\n        method = Experiment.update_configs\n        self._process_scaffold(method, *args, **kwargs)\n\n    @functools.wraps(Experiment.format_vid)\n    def format_vid(self, *args, **kwargs) -&gt; None:\n        """\n        Batch processing corresponding to\n        [behavysis_pipeline.pipeline.experiment.Experiment.format_vid][]\n\n        Parameters\n        ----------\n        *args : tuple\n            args passed to process scaffold method.\n        **kwargs : dict\n            keyword args passed to process scaffold method.\n        """\n        method = Experiment.format_vid\n        self._process_scaffold(method, *args, **kwargs)\n\n    @functools.wraps(Experiment.run_dlc)\n    def run_dlc(self, gputouse: int = None, overwrite: bool = False) -&gt; None:\n        """\n        Batch processing corresponding to\n        [behavysis_pipeline.pipeline.experiment.Experiment.run_dlc][]\n\n        Uses a multiprocessing pool to run DLC on each batch of experiments with each GPU\n        natively as batch in the same spawned subprocess (a DLC subprocess is spawned).\n        This is a slight tweak from the regular method of running\n        each experiment separately with multiprocessing.\n\n        Parameters\n        ----------\n        gputouse : int, optional\n            The GPU ID to use for running DLC. If None, all GPUs are used.\n        overwrite : bool, optional\n            Whether to overwrite the DLC output files if they already exist.\n        """\n        # If gputouse is not specified, using all GPUs\n        if gputouse is None:\n            gputouse_ls = MultiprocMixin.get_gpu_ids()\n        else:\n            gputouse_ls = [gputouse]\n        nprocs = len(gputouse_ls)\n        # Getting the experiments to run DLC on\n        exp_ls = self.get_experiments()\n        # If overwrite is False, filtering for only experiments that need processing\n        if not overwrite:\n            exp_ls = [\n                exp\n                for exp in exp_ls\n                if not os.path.isfile(exp.get_fp(Folders.DLC.value))\n            ]\n\n        # Running DLC on each batch of experiments with each GPU (given allocated GPU ID)\n        # TODO: have error handling\n        exp_batches_ls = np.array_split(exp_ls, nprocs)\n        with Pool(processes=nprocs) as p:\n            p.starmap(\n                RunDLC.ma_dlc_analyse_batch,\n                [\n                    (\n                        [exp.get_fp(Folders.FORMATTED_VID.value) for exp in exp_batch],\n                        os.path.join(self.root_dir, Folders.DLC.value),\n                        os.path.join(self.root_dir, Folders.CONFIGS.value),\n                        os.path.join(self.root_dir, TEMP_DIR),\n                        gputouse,\n                        overwrite,\n                    )\n                    for gputouse, exp_batch in zip(gputouse_ls, exp_batches_ls)\n                ],\n            )\n\n    @functools.wraps(Experiment.calculate_params)\n    def calculate_params(self, *args, **kwargs) -&gt; None:\n        """\n        Batch processing corresponding to\n        [behavysis_pipeline.pipeline.experiment.Experiment.calculate_params][]\n\n        Parameters\n        ----------\n        *args : tuple\n            args passed to process scaffold method.\n        **kwargs : dict\n            keyword args passed to process scaffold method.\n        """\n        method = Experiment.calculate_params\n        self._process_scaffold(method, *args, **kwargs)\n\n    @functools.wraps(Experiment.preprocess)\n    def preprocess(self, *args, **kwargs) -&gt; None:\n        """\n        Batch processing corresponding to\n        [behavysis_pipeline.pipeline.experiment.Experiment.preprocess][]\n\n        Parameters\n        ----------\n        *args : tuple\n            args passed to process scaffold method.\n        **kwargs : dict\n            keyword args passed to process scaffold method.\n        """\n        method = Experiment.preprocess\n        self._process_scaffold(method, *args, **kwargs)\n\n    @functools.wraps(Experiment.extract_features)\n    def extract_features(self, *args, **kwargs) -&gt; None:\n        """\n        Batch processing corresponding to\n        [behavysis_pipeline.pipeline.experiment.Experiment.extract_features][]\n\n        Parameters\n        ----------\n        *args : tuple\n            args passed to process scaffold method.\n        **kwargs : dict\n            keyword args passed to process scaffold method.\n        """\n        method = Experiment.extract_features\n        self._process_scaffold(method, *args, **kwargs)\n\n    @functools.wraps(Experiment.classify_behaviours)\n    def classify_behaviours(self, *args, **kwargs) -&gt; None:\n        """\n        Batch processing corresponding to\n        [behavysis_pipeline.pipeline.experiment.Experiment.classify_behaviours][]\n\n        Parameters\n        ----------\n        *args : tuple\n            args passed to process scaffold method.\n        **kwargs : dict\n            keyword args passed to process scaffold method.\n        """\n        # TODO: handle reading the model file whilst in multiprocessing.\n        # Current fix is single processing.\n        nprocs = self.nprocs\n        self.nprocs = 1\n        method = Experiment.classify_behaviours\n        self._process_scaffold(method, *args, **kwargs)\n        self.nprocs = nprocs\n\n    @functools.wraps(Experiment.export_behaviours)\n    def export_behaviours(self, *args, **kwargs) -&gt; None:\n        """\n        Batch processing corresponding to\n        [behavysis_pipeline.pipeline.experiment.Experiment.export_behaviours][]\n\n        Parameters\n        ----------\n        *args : tuple\n            args passed to process scaffold method.\n        **kwargs : dict\n            keyword args passed to process scaffold method.\n        """\n        # TODO: handle reading the model file whilst in multiprocessing.\n        # Current fix is single processing.\n        nprocs = self.nprocs\n        self.nprocs = 1\n        method = Experiment.export_behaviours\n        self._process_scaffold(method, *args, **kwargs)\n        self.nprocs = nprocs\n\n    @functools.wraps(Experiment.export_feather)\n    def export_feather(self, *args, **kwargs) -&gt; None:\n        """\n        Batch processing corresponding to\n        [behavysis_pipeline.pipeline.experiment.Experiment.export_feather][]\n\n        Parameters\n        ----------\n        *args : tuple\n            args passed to process scaffold method.\n        **kwargs : dict\n            keyword args passed to process scaffold method.\n        """\n        method = Experiment.export_feather\n        self._process_scaffold(method, *args, **kwargs)\n\n    @functools.wraps(Experiment.evaluate)\n    def evaluate(self, *args, **kwargs) -&gt; None:\n        """\n        Batch processing corresponding to\n        [behavysis_pipeline.pipeline.experiment.Experiment.evaluate][]\n\n        Parameters\n        ----------\n        *args : tuple\n            args passed to process scaffold method.\n        **kwargs : dict\n            keyword args passed to process scaffold method.\n        """\n        method = Experiment.evaluate\n        self._process_scaffold(method, *args, **kwargs)\n\n    @functools.wraps(Experiment.analyse)\n    def analyse(self, *args, **kwargs) -&gt; None:\n        """\n        Batch processing corresponding to\n        [behavysis_pipeline.pipeline.experiment.Experiment.analyse][]\n\n        Parameters\n        ----------\n        *args : tuple\n            args passed to process scaffold method.\n        **kwargs : dict\n            keyword args passed to process scaffold method.\n        """\n        method = Experiment.analyse\n        self._process_scaffold(method, *args, **kwargs)\n\n    @functools.wraps(Experiment.behav_analyse)\n    def behav_analyse(self, *args, **kwargs) -&gt; None:\n        """\n        Batch processing corresponding to\n        [behavysis_pipeline.pipeline.experiment.Experiment.behav_analyse][]\n\n        Parameters\n        ----------\n        *args : tuple\n            args passed to process scaffold method.\n        **kwargs : dict\n            keyword args passed to process scaffold method.\n        """\n        method = Experiment.behav_analyse\n        self._process_scaffold(method, *args, **kwargs)\n\n    #####################################################################\n    #               DIAGNOSTICS DICT METHODS\n    #####################################################################\n\n    def load_diagnostics(self, name: str) -&gt; pd.DataFrame:\n        """\n        Reads the data from the diagnostics file with the given name.\n\n        Parameters\n        ----------\n        name : str\n            The name of the diagnostics file to overwrite and open.\n\n        Returns\n        -------\n        pd.DataFrame\n            The pandas DataFrame of the diagnostics file.\n        """\n        # Getting filepath\n        fp = os.path.join(self.root_dir, DIAGNOSTICS_DIR, f"{name}.csv")\n        # Reading from file\n        return DiagnosticsMixin.load_diagnostics(fp)\n\n    def save_diagnostics(self, name: str, df: pd.DataFrame) -&gt; None:\n        """\n        Writes the given data to a diagnostics file with the given name.\n\n        Parameters\n        ----------\n        name : str\n            The name of the diagnostics file to overwrite and open.\n        df : pd.DataFrame\n            The pandas DataFrame to write to the diagnostics file.\n        """\n        # Getting filepath\n        fp = os.path.join(self.root_dir, DIAGNOSTICS_DIR, f"{name}.csv")\n        # Writing to file\n        DiagnosticsMixin.save_diagnostics(df, fp)\n\n    #####################################################################\n    #               IMPORT EXPERIMENTS METHODS\n    #####################################################################\n\n    def import_experiment(self, name: str) -&gt; bool:\n        """\n        Adds an experiment with the given name to the .experiments dict.\n        The key of this experiment in the `self.experiments` dict is "dir/name".\n        If the experiment already exists in the project, it is not added.\n\n        Parameters\n        ----------\n        name : str\n            The experiment name.\n\n        Returns\n        -------\n        bool\n            Whether the experiment was imported or not.\n            True if imported, False if not.\n        """\n        if name not in self.experiments:\n            self.experiments[name] = Experiment(name, self.root_dir)\n            return True\n        return False\n\n    def import_experiments(self) -&gt; None:\n        """\n        Add all experiments in the project folder to the experiments dict.\n        The key of each experiment in the .experiments dict is "name".\n        Refer to Project.addExperiment() for details about how each experiment is added.\n        """\n        logging.info("Searching project folder: %s\\n", self.root_dir)\n        # Adding all experiments within given project dir\n        failed = []\n        for f in Folders:\n            folder = os.path.join(self.root_dir, f.value)\n            # If folder does not exist, skip\n            if not os.path.isdir(folder):\n                continue\n            # For each file in the folder\n            for j in natsorted(os.listdir(folder)):\n                if re.search(r"^\\.", j):  # do not add hidden files\n                    continue\n                name = IOMixin.get_name(j)\n                try:\n                    self.import_experiment(name)\n                except ValueError as e:  # do not add invalid files\n                    logging.info("failed: %s    --    %s:\\n%s", f.value, j, e)\n                    failed.append(name)\n        # Printing outcome of imported and failed experiments\n        logging.info("Experiments imported successfully:")\n        logging.info("%s\\n\\n", "\\n".join([f"    - {i}" for i in self.experiments]))\n        logging.info("Experiments failed to import:")\n        logging.info("%s\\n\\n", "\\n".join([f"    - {i}" for i in failed]))\n        # If there are no experiments, then return\n        if not self.experiments:\n            return\n        # # Making diagnostics DataFrame of all the files associated with each experiment that exists\n        # cols_ls = [f.value for f in Folders]\n        # rows_ls = list(self.experiments)\n        # shape = (len(rows_ls), len(cols_ls))\n        # dd_arr = np.apply_along_axis(\n        #     lambda i: os.path.isfile(self.experiments[i[1]].get_fp(i[0])),\n        #     axis=0,\n        #     arr=np.array(np.meshgrid(cols_ls, rows_ls)).reshape((2, np.prod(shape))),\n        # ).reshape(shape)\n        # # Creating the diagnostics DataFrame\n        # dd_df = pd.DataFrame(dd_arr, index=rows_ls, columns=cols_ls)\n        # # Saving the diagnostics DataFrame\n        # self.save_diagnostics("import_experiments", dd_df)\n\n    #####################################################################\n    #                CONFIGS DIAGONOSTICS METHODS\n    #####################################################################\n\n    def collate_configs_auto(self) -&gt; None:\n        """\n        Collates the auto fields of the configs of all experiments into a DataFrame.\n        """\n        # Initialising the process and printing the description\n        description = "Combining binned analysis"\n        logging.info("%s...", description)\n        # Getting all the auto field keys\n        auto_field_keys = ConfigsAuto.get_field_names(ConfigsAuto)\n        # Making a DataFrame to store all the auto fields for each experiment\n        df_configs = pd.DataFrame(\n            index=[exp.name for exp in self.get_experiments()],\n            columns=["_".join(i) for i in auto_field_keys],\n        )\n        # Collating all the auto fields for each experiment\n        for exp in self.get_experiments():\n            configs = ExperimentConfigs.read_json(exp.get_fp(Folders.CONFIGS.value))\n            for i in auto_field_keys:\n                val = configs.auto\n                for j in i:\n                    val = getattr(val, j)\n                df_configs.loc[exp.name, "_".join(i)] = val\n        # Saving the collated auto fields DataFrame to diagnostics folder\n        self.save_diagnostics("collated_configs_auto", df_configs)\n\n        # Making and saving histogram plots of all the auto fields\n        g = sns.FacetGrid(\n            data=df_configs.fillna(-1).melt(), col="variable", sharex=False, col_wrap=4\n        )\n        g.map(sns.histplot, "value", bins=10)\n        g.set_titles("{col_name}")\n        g.savefig(\n            os.path.join(\n                self.root_dir, DIAGNOSTICS_DIR, "collated_configs_auto_hist.png"\n            )\n        )\n        g.figure.clf()\n\n    #####################################################################\n    #            COMBINING ANALYSIS DATA ACROSS EXPS METHODS\n    #####################################################################\n\n    def collate_analysis_binned(self) -&gt; None:\n        """\n        Combines an analysis of all the experiments together to generate combined h5 files for:\n        - Each binned data. The index is (bin) and columns are (expName, indiv, measure).\n        """\n        # Initialising the process and printing the description\n        description = "Combining binned analysis"\n        logging.info("%s...", description)\n        # dd_df = pd.DataFrame()\n\n        # AGGREGATING BINNED DATA\n        # NOTE: need a more robust way of getting the list of bin sizes\n        ANALYSE_DIR = os.path.join(self.root_dir, ANALYSE_DIR)\n        configs = ExperimentConfigs.read_json(\n            self.get_experiments()[0].get_fp(Folders.CONFIGS.value)\n        )\n        bin_sizes_sec = configs.get_ref(configs.user.analyse.bins_sec)\n        bin_sizes_sec = np.append(bin_sizes_sec, "custom")\n        # Searching through all the analysis subdir\n        for i in os.listdir(ANALYSE_DIR):\n            if i == "aggregate_analysis":\n                continue\n            analysis_subdir = os.path.join(ANALYSE_DIR, i)\n            for bin_i in bin_sizes_sec:\n                total_df = pd.DataFrame()\n                out_fp = os.path.join(analysis_subdir, f"__ALL_binned_{bin_i}.feather")\n                for exp in self.get_experiments():\n                    in_fp = os.path.join(\n                        analysis_subdir, f"binned_{bin_i}", f"{exp.name}.feather"\n                    )\n                    if os.path.isfile(in_fp):\n                        # Reading exp summary df\n                        df = DFMixin.read_feather(in_fp)\n                        # Prepending experiment name to column MultiIndex\n                        df = pd.concat(\n                            [df], keys=[exp.name], names=["experiment"], axis=1\n                        )\n                        # Concatenating total_df with df across columns\n                        total_df = pd.concat([total_df, df], axis=1)\n                    DFMixin.write_feather(total_df, out_fp)\n\n    def collate_analysis_summary(self) -&gt; None:\n        """\n        Combines an analysis of all the experiments together to generate combined h5 files for:\n        - The summary data. The index is (expName, indiv, measure) and columns are\n        (statistics -e.g., mean).\n        """\n        # Initialising the process and printing the description\n        description = "Combining summary analysis"\n        logging.info("%s...", description)\n        # dd_df = pd.DataFrame()\n\n        # AGGREGATING SUMMARY DATA\n        ANALYSE_DIR = os.path.join(self.root_dir, ANALYSE_DIR)\n        # Searching through all the analysis subdir\n        for i in os.listdir(ANALYSE_DIR):\n            if i == "aggregate_analysis":\n                continue\n            analysis_subdir = os.path.join(ANALYSE_DIR, i)\n            total_df = pd.DataFrame()\n            out_fp = os.path.join(analysis_subdir, "__ALL_summary.feather")\n            for exp in self.get_experiments():\n                in_fp = os.path.join(analysis_subdir, "summary", f"{exp.name}.feather")\n                if os.path.isfile(in_fp):\n                    # Reading exp summary df\n                    df = DFMixin.read_feather(in_fp)\n                    # Prepending experiment name to index MultiIndex\n                    df = pd.concat([df], keys=[exp.name], names=["experiment"], axis=0)\n                    # Concatenating total_df with df down rows\n                    total_df = pd.concat([total_df, df], axis=0)\n            DFMixin.write_feather(total_df, out_fp)\n            DFMixin.write_feather(total_df, out_fp)\n            DFMixin.write_feather(total_df, out_fp)\n            DFMixin.write_feather(total_df, out_fp)\n            DFMixin.write_feather(total_df, out_fp)\n            DFMixin.write_feather(total_df, out_fp)\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.__init__",
      title: "<code>__init__(root_dir)</code>",
      text: '<p>Make a Project instance.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>The filepath of the project directory. Can be relative to current dir or absolute dir.</p> required Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>def __init__(self, root_dir: str) -&gt; None:\n    """\n    Make a Project instance.\n\n    Parameters\n    ----------\n    root_dir : str\n        The filepath of the project directory. Can be relative to\n        current dir or absolute dir.\n    """\n    # Assertion: project directory must exist\n    if not os.path.isdir(root_dir):\n        raise ValueError(\n            f\'Error: The folder, "{root_dir}" does not exist.\\n\'\n            + "Please specify a folder that exists. Ensure you have the correct"\n            + "forward-slashes or back-slashes for the path name."\n        )\n    self.root_dir = os.path.abspath(root_dir)\n    self.experiments = {}\n    self.nprocs = 4\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.analyse",
      title: "<code>analyse(*args, **kwargs)</code>",
      text: '<p>Batch processing corresponding to behavysis_pipeline.pipeline.experiment.Experiment.analyse</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>tuple</code> <p>args passed to process scaffold method.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>keyword args passed to process scaffold method.</p> <code>{}</code> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>@functools.wraps(Experiment.analyse)\ndef analyse(self, *args, **kwargs) -&gt; None:\n    """\n    Batch processing corresponding to\n    [behavysis_pipeline.pipeline.experiment.Experiment.analyse][]\n\n    Parameters\n    ----------\n    *args : tuple\n        args passed to process scaffold method.\n    **kwargs : dict\n        keyword args passed to process scaffold method.\n    """\n    method = Experiment.analyse\n    self._process_scaffold(method, *args, **kwargs)\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.behav_analyse",
      title: "<code>behav_analyse(*args, **kwargs)</code>",
      text: '<p>Batch processing corresponding to behavysis_pipeline.pipeline.experiment.Experiment.behav_analyse</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>tuple</code> <p>args passed to process scaffold method.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>keyword args passed to process scaffold method.</p> <code>{}</code> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>@functools.wraps(Experiment.behav_analyse)\ndef behav_analyse(self, *args, **kwargs) -&gt; None:\n    """\n    Batch processing corresponding to\n    [behavysis_pipeline.pipeline.experiment.Experiment.behav_analyse][]\n\n    Parameters\n    ----------\n    *args : tuple\n        args passed to process scaffold method.\n    **kwargs : dict\n        keyword args passed to process scaffold method.\n    """\n    method = Experiment.behav_analyse\n    self._process_scaffold(method, *args, **kwargs)\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.calculate_params",
      title: "<code>calculate_params(*args, **kwargs)</code>",
      text: '<p>Batch processing corresponding to behavysis_pipeline.pipeline.experiment.Experiment.calculate_params</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>tuple</code> <p>args passed to process scaffold method.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>keyword args passed to process scaffold method.</p> <code>{}</code> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>@functools.wraps(Experiment.calculate_params)\ndef calculate_params(self, *args, **kwargs) -&gt; None:\n    """\n    Batch processing corresponding to\n    [behavysis_pipeline.pipeline.experiment.Experiment.calculate_params][]\n\n    Parameters\n    ----------\n    *args : tuple\n        args passed to process scaffold method.\n    **kwargs : dict\n        keyword args passed to process scaffold method.\n    """\n    method = Experiment.calculate_params\n    self._process_scaffold(method, *args, **kwargs)\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.classify_behaviours",
      title: "<code>classify_behaviours(*args, **kwargs)</code>",
      text: '<p>Batch processing corresponding to behavysis_pipeline.pipeline.experiment.Experiment.classify_behaviours</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>tuple</code> <p>args passed to process scaffold method.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>keyword args passed to process scaffold method.</p> <code>{}</code> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>@functools.wraps(Experiment.classify_behaviours)\ndef classify_behaviours(self, *args, **kwargs) -&gt; None:\n    """\n    Batch processing corresponding to\n    [behavysis_pipeline.pipeline.experiment.Experiment.classify_behaviours][]\n\n    Parameters\n    ----------\n    *args : tuple\n        args passed to process scaffold method.\n    **kwargs : dict\n        keyword args passed to process scaffold method.\n    """\n    # TODO: handle reading the model file whilst in multiprocessing.\n    # Current fix is single processing.\n    nprocs = self.nprocs\n    self.nprocs = 1\n    method = Experiment.classify_behaviours\n    self._process_scaffold(method, *args, **kwargs)\n    self.nprocs = nprocs\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.collate_analysis_binned",
      title: "<code>collate_analysis_binned()</code>",
      text: '<p>Combines an analysis of all the experiments together to generate combined h5 files for: - Each binned data. The index is (bin) and columns are (expName, indiv, measure).</p> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>def collate_analysis_binned(self) -&gt; None:\n    """\n    Combines an analysis of all the experiments together to generate combined h5 files for:\n    - Each binned data. The index is (bin) and columns are (expName, indiv, measure).\n    """\n    # Initialising the process and printing the description\n    description = "Combining binned analysis"\n    logging.info("%s...", description)\n    # dd_df = pd.DataFrame()\n\n    # AGGREGATING BINNED DATA\n    # NOTE: need a more robust way of getting the list of bin sizes\n    ANALYSE_DIR = os.path.join(self.root_dir, ANALYSE_DIR)\n    configs = ExperimentConfigs.read_json(\n        self.get_experiments()[0].get_fp(Folders.CONFIGS.value)\n    )\n    bin_sizes_sec = configs.get_ref(configs.user.analyse.bins_sec)\n    bin_sizes_sec = np.append(bin_sizes_sec, "custom")\n    # Searching through all the analysis subdir\n    for i in os.listdir(ANALYSE_DIR):\n        if i == "aggregate_analysis":\n            continue\n        analysis_subdir = os.path.join(ANALYSE_DIR, i)\n        for bin_i in bin_sizes_sec:\n            total_df = pd.DataFrame()\n            out_fp = os.path.join(analysis_subdir, f"__ALL_binned_{bin_i}.feather")\n            for exp in self.get_experiments():\n                in_fp = os.path.join(\n                    analysis_subdir, f"binned_{bin_i}", f"{exp.name}.feather"\n                )\n                if os.path.isfile(in_fp):\n                    # Reading exp summary df\n                    df = DFMixin.read_feather(in_fp)\n                    # Prepending experiment name to column MultiIndex\n                    df = pd.concat(\n                        [df], keys=[exp.name], names=["experiment"], axis=1\n                    )\n                    # Concatenating total_df with df across columns\n                    total_df = pd.concat([total_df, df], axis=1)\n                DFMixin.write_feather(total_df, out_fp)\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.collate_analysis_summary",
      title: "<code>collate_analysis_summary()</code>",
      text: '<p>Combines an analysis of all the experiments together to generate combined h5 files for: - The summary data. The index is (expName, indiv, measure) and columns are (statistics -e.g., mean).</p> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>def collate_analysis_summary(self) -&gt; None:\n    """\n    Combines an analysis of all the experiments together to generate combined h5 files for:\n    - The summary data. The index is (expName, indiv, measure) and columns are\n    (statistics -e.g., mean).\n    """\n    # Initialising the process and printing the description\n    description = "Combining summary analysis"\n    logging.info("%s...", description)\n    # dd_df = pd.DataFrame()\n\n    # AGGREGATING SUMMARY DATA\n    ANALYSE_DIR = os.path.join(self.root_dir, ANALYSE_DIR)\n    # Searching through all the analysis subdir\n    for i in os.listdir(ANALYSE_DIR):\n        if i == "aggregate_analysis":\n            continue\n        analysis_subdir = os.path.join(ANALYSE_DIR, i)\n        total_df = pd.DataFrame()\n        out_fp = os.path.join(analysis_subdir, "__ALL_summary.feather")\n        for exp in self.get_experiments():\n            in_fp = os.path.join(analysis_subdir, "summary", f"{exp.name}.feather")\n            if os.path.isfile(in_fp):\n                # Reading exp summary df\n                df = DFMixin.read_feather(in_fp)\n                # Prepending experiment name to index MultiIndex\n                df = pd.concat([df], keys=[exp.name], names=["experiment"], axis=0)\n                # Concatenating total_df with df down rows\n                total_df = pd.concat([total_df, df], axis=0)\n        DFMixin.write_feather(total_df, out_fp)\n        DFMixin.write_feather(total_df, out_fp)\n        DFMixin.write_feather(total_df, out_fp)\n        DFMixin.write_feather(total_df, out_fp)\n        DFMixin.write_feather(total_df, out_fp)\n        DFMixin.write_feather(total_df, out_fp)\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.collate_configs_auto",
      title: "<code>collate_configs_auto()</code>",
      text: '<p>Collates the auto fields of the configs of all experiments into a DataFrame.</p> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>def collate_configs_auto(self) -&gt; None:\n    """\n    Collates the auto fields of the configs of all experiments into a DataFrame.\n    """\n    # Initialising the process and printing the description\n    description = "Combining binned analysis"\n    logging.info("%s...", description)\n    # Getting all the auto field keys\n    auto_field_keys = ConfigsAuto.get_field_names(ConfigsAuto)\n    # Making a DataFrame to store all the auto fields for each experiment\n    df_configs = pd.DataFrame(\n        index=[exp.name for exp in self.get_experiments()],\n        columns=["_".join(i) for i in auto_field_keys],\n    )\n    # Collating all the auto fields for each experiment\n    for exp in self.get_experiments():\n        configs = ExperimentConfigs.read_json(exp.get_fp(Folders.CONFIGS.value))\n        for i in auto_field_keys:\n            val = configs.auto\n            for j in i:\n                val = getattr(val, j)\n            df_configs.loc[exp.name, "_".join(i)] = val\n    # Saving the collated auto fields DataFrame to diagnostics folder\n    self.save_diagnostics("collated_configs_auto", df_configs)\n\n    # Making and saving histogram plots of all the auto fields\n    g = sns.FacetGrid(\n        data=df_configs.fillna(-1).melt(), col="variable", sharex=False, col_wrap=4\n    )\n    g.map(sns.histplot, "value", bins=10)\n    g.set_titles("{col_name}")\n    g.savefig(\n        os.path.join(\n            self.root_dir, DIAGNOSTICS_DIR, "collated_configs_auto_hist.png"\n        )\n    )\n    g.figure.clf()\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.evaluate",
      title: "<code>evaluate(*args, **kwargs)</code>",
      text: '<p>Batch processing corresponding to behavysis_pipeline.pipeline.experiment.Experiment.evaluate</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>tuple</code> <p>args passed to process scaffold method.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>keyword args passed to process scaffold method.</p> <code>{}</code> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>@functools.wraps(Experiment.evaluate)\ndef evaluate(self, *args, **kwargs) -&gt; None:\n    """\n    Batch processing corresponding to\n    [behavysis_pipeline.pipeline.experiment.Experiment.evaluate][]\n\n    Parameters\n    ----------\n    *args : tuple\n        args passed to process scaffold method.\n    **kwargs : dict\n        keyword args passed to process scaffold method.\n    """\n    method = Experiment.evaluate\n    self._process_scaffold(method, *args, **kwargs)\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.export_behaviours",
      title: "<code>export_behaviours(*args, **kwargs)</code>",
      text: '<p>Batch processing corresponding to behavysis_pipeline.pipeline.experiment.Experiment.export_behaviours</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>tuple</code> <p>args passed to process scaffold method.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>keyword args passed to process scaffold method.</p> <code>{}</code> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>@functools.wraps(Experiment.export_behaviours)\ndef export_behaviours(self, *args, **kwargs) -&gt; None:\n    """\n    Batch processing corresponding to\n    [behavysis_pipeline.pipeline.experiment.Experiment.export_behaviours][]\n\n    Parameters\n    ----------\n    *args : tuple\n        args passed to process scaffold method.\n    **kwargs : dict\n        keyword args passed to process scaffold method.\n    """\n    # TODO: handle reading the model file whilst in multiprocessing.\n    # Current fix is single processing.\n    nprocs = self.nprocs\n    self.nprocs = 1\n    method = Experiment.export_behaviours\n    self._process_scaffold(method, *args, **kwargs)\n    self.nprocs = nprocs\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.export_feather",
      title: "<code>export_feather(*args, **kwargs)</code>",
      text: '<p>Batch processing corresponding to behavysis_pipeline.pipeline.experiment.Experiment.export_feather</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>tuple</code> <p>args passed to process scaffold method.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>keyword args passed to process scaffold method.</p> <code>{}</code> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>@functools.wraps(Experiment.export_feather)\ndef export_feather(self, *args, **kwargs) -&gt; None:\n    """\n    Batch processing corresponding to\n    [behavysis_pipeline.pipeline.experiment.Experiment.export_feather][]\n\n    Parameters\n    ----------\n    *args : tuple\n        args passed to process scaffold method.\n    **kwargs : dict\n        keyword args passed to process scaffold method.\n    """\n    method = Experiment.export_feather\n    self._process_scaffold(method, *args, **kwargs)\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.extract_features",
      title: "<code>extract_features(*args, **kwargs)</code>",
      text: '<p>Batch processing corresponding to behavysis_pipeline.pipeline.experiment.Experiment.extract_features</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>tuple</code> <p>args passed to process scaffold method.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>keyword args passed to process scaffold method.</p> <code>{}</code> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>@functools.wraps(Experiment.extract_features)\ndef extract_features(self, *args, **kwargs) -&gt; None:\n    """\n    Batch processing corresponding to\n    [behavysis_pipeline.pipeline.experiment.Experiment.extract_features][]\n\n    Parameters\n    ----------\n    *args : tuple\n        args passed to process scaffold method.\n    **kwargs : dict\n        keyword args passed to process scaffold method.\n    """\n    method = Experiment.extract_features\n    self._process_scaffold(method, *args, **kwargs)\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.format_vid",
      title: "<code>format_vid(*args, **kwargs)</code>",
      text: '<p>Batch processing corresponding to behavysis_pipeline.pipeline.experiment.Experiment.format_vid</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>tuple</code> <p>args passed to process scaffold method.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>keyword args passed to process scaffold method.</p> <code>{}</code> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>@functools.wraps(Experiment.format_vid)\ndef format_vid(self, *args, **kwargs) -&gt; None:\n    """\n    Batch processing corresponding to\n    [behavysis_pipeline.pipeline.experiment.Experiment.format_vid][]\n\n    Parameters\n    ----------\n    *args : tuple\n        args passed to process scaffold method.\n    **kwargs : dict\n        keyword args passed to process scaffold method.\n    """\n    method = Experiment.format_vid\n    self._process_scaffold(method, *args, **kwargs)\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.get_experiment",
      title: "<code>get_experiment(name)</code>",
      text: '<p>Gets the experiment with the given name</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The experiment name.</p> required <p>Returns:</p> Type Description <code>Experiment</code> <p>The experiment.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Experiment with the given name does not exist.</p> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>def get_experiment(self, name: str) -&gt; Experiment:\n    """\n    Gets the experiment with the given name\n\n    Parameters\n    ----------\n    name : str\n        The experiment name.\n\n    Returns\n    -------\n    Experiment\n        The experiment.\n\n    Raises\n    ------\n    ValueError\n        Experiment with the given name does not exist.\n    """\n    if name in self.experiments:\n        return self.experiments[name]\n    raise ValueError(\n        f\'Experiment with the name "{name}" does not exist in the project.\'\n    )\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.get_experiments",
      title: "<code>get_experiments()</code>",
      text: '<p>Gets the ordered (natsorted) list of Experiment instances in the Project.</p> <p>Returns:</p> Type Description <code>list[Experiment]</code> <p>The list of all Experiment instances stored in the Project instance.</p> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>def get_experiments(self) -&gt; list[Experiment]:\n    """\n    Gets the ordered (natsorted) list of Experiment instances in the Project.\n\n    Returns\n    -------\n    list[Experiment]\n        The list of all Experiment instances stored in the Project instance.\n    """\n    return [self.experiments[i] for i in natsorted(self.experiments)]\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.import_experiment",
      title: "<code>import_experiment(name)</code>",
      text: '<p>Adds an experiment with the given name to the .experiments dict. The key of this experiment in the <code>self.experiments</code> dict is "dir/name". If the experiment already exists in the project, it is not added.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The experiment name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the experiment was imported or not. True if imported, False if not.</p> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>def import_experiment(self, name: str) -&gt; bool:\n    """\n    Adds an experiment with the given name to the .experiments dict.\n    The key of this experiment in the `self.experiments` dict is "dir/name".\n    If the experiment already exists in the project, it is not added.\n\n    Parameters\n    ----------\n    name : str\n        The experiment name.\n\n    Returns\n    -------\n    bool\n        Whether the experiment was imported or not.\n        True if imported, False if not.\n    """\n    if name not in self.experiments:\n        self.experiments[name] = Experiment(name, self.root_dir)\n        return True\n    return False\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.import_experiments",
      title: "<code>import_experiments()</code>",
      text: '<p>Add all experiments in the project folder to the experiments dict. The key of each experiment in the .experiments dict is "name". Refer to Project.addExperiment() for details about how each experiment is added.</p> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>def import_experiments(self) -&gt; None:\n    """\n    Add all experiments in the project folder to the experiments dict.\n    The key of each experiment in the .experiments dict is "name".\n    Refer to Project.addExperiment() for details about how each experiment is added.\n    """\n    logging.info("Searching project folder: %s\\n", self.root_dir)\n    # Adding all experiments within given project dir\n    failed = []\n    for f in Folders:\n        folder = os.path.join(self.root_dir, f.value)\n        # If folder does not exist, skip\n        if not os.path.isdir(folder):\n            continue\n        # For each file in the folder\n        for j in natsorted(os.listdir(folder)):\n            if re.search(r"^\\.", j):  # do not add hidden files\n                continue\n            name = IOMixin.get_name(j)\n            try:\n                self.import_experiment(name)\n            except ValueError as e:  # do not add invalid files\n                logging.info("failed: %s    --    %s:\\n%s", f.value, j, e)\n                failed.append(name)\n    # Printing outcome of imported and failed experiments\n    logging.info("Experiments imported successfully:")\n    logging.info("%s\\n\\n", "\\n".join([f"    - {i}" for i in self.experiments]))\n    logging.info("Experiments failed to import:")\n    logging.info("%s\\n\\n", "\\n".join([f"    - {i}" for i in failed]))\n    # If there are no experiments, then return\n    if not self.experiments:\n        return\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.load_diagnostics",
      title: "<code>load_diagnostics(name)</code>",
      text: '<p>Reads the data from the diagnostics file with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the diagnostics file to overwrite and open.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The pandas DataFrame of the diagnostics file.</p> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>def load_diagnostics(self, name: str) -&gt; pd.DataFrame:\n    """\n    Reads the data from the diagnostics file with the given name.\n\n    Parameters\n    ----------\n    name : str\n        The name of the diagnostics file to overwrite and open.\n\n    Returns\n    -------\n    pd.DataFrame\n        The pandas DataFrame of the diagnostics file.\n    """\n    # Getting filepath\n    fp = os.path.join(self.root_dir, DIAGNOSTICS_DIR, f"{name}.csv")\n    # Reading from file\n    return DiagnosticsMixin.load_diagnostics(fp)\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.make_project",
      title:
        "<code>make_project(clf, root_dir, overwrite=False)</code>  <code>classmethod</code>",
      text: '<p>Makes a project instance and imports all experiments in the project folder.</p> <p>Also copies the <code>run.py</code> script and <code>default_configs.json</code> to the root_dir.</p> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>@classmethod\ndef make_project(clf, root_dir, overwrite: bool = False) -&gt; None:\n    """\n    Makes a project instance and imports all experiments in the project folder.\n\n    Also copies the `run.py` script and `default_configs.json` to the root_dir.\n    """\n    # Making the project root folder\n    os.makedirs(root_dir, exist_ok=True)\n    # Making the project instance\n    proj = clf(root_dir)\n    # Making each subfolder\n    for f in Folders:\n        os.makedirs(os.path.join(proj.root_dir, f.value), exist_ok=True)\n    # Copying the default_configs.json and run.py files to the project folder\n    for i in ["default_configs.json", "run.py"]:\n        # Getting the file path\n        dst_fp = os.path.join(proj.root_dir, i)\n        # If not overwrite and file exists, then don\'t overwrite\n        if not overwrite and os.path.exists(dst_fp):\n            continue\n        # Saving the template to the file\n        IOMixin.save_template(\n            i,\n            "behavysis_pipeline",\n            "script_templates",\n            dst_fp,\n        )\n    # returning the project instance\n    return proj\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.preprocess",
      title: "<code>preprocess(*args, **kwargs)</code>",
      text: '<p>Batch processing corresponding to behavysis_pipeline.pipeline.experiment.Experiment.preprocess</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>tuple</code> <p>args passed to process scaffold method.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>keyword args passed to process scaffold method.</p> <code>{}</code> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>@functools.wraps(Experiment.preprocess)\ndef preprocess(self, *args, **kwargs) -&gt; None:\n    """\n    Batch processing corresponding to\n    [behavysis_pipeline.pipeline.experiment.Experiment.preprocess][]\n\n    Parameters\n    ----------\n    *args : tuple\n        args passed to process scaffold method.\n    **kwargs : dict\n        keyword args passed to process scaffold method.\n    """\n    method = Experiment.preprocess\n    self._process_scaffold(method, *args, **kwargs)\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.run_dlc",
      title: "<code>run_dlc(gputouse=None, overwrite=False)</code>",
      text: '<p>Batch processing corresponding to behavysis_pipeline.pipeline.experiment.Experiment.run_dlc</p> <p>Uses a multiprocessing pool to run DLC on each batch of experiments with each GPU natively as batch in the same spawned subprocess (a DLC subprocess is spawned). This is a slight tweak from the regular method of running each experiment separately with multiprocessing.</p> <p>Parameters:</p> Name Type Description Default <code>gputouse</code> <code>int</code> <p>The GPU ID to use for running DLC. If None, all GPUs are used.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the DLC output files if they already exist.</p> <code>False</code> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>@functools.wraps(Experiment.run_dlc)\ndef run_dlc(self, gputouse: int = None, overwrite: bool = False) -&gt; None:\n    """\n    Batch processing corresponding to\n    [behavysis_pipeline.pipeline.experiment.Experiment.run_dlc][]\n\n    Uses a multiprocessing pool to run DLC on each batch of experiments with each GPU\n    natively as batch in the same spawned subprocess (a DLC subprocess is spawned).\n    This is a slight tweak from the regular method of running\n    each experiment separately with multiprocessing.\n\n    Parameters\n    ----------\n    gputouse : int, optional\n        The GPU ID to use for running DLC. If None, all GPUs are used.\n    overwrite : bool, optional\n        Whether to overwrite the DLC output files if they already exist.\n    """\n    # If gputouse is not specified, using all GPUs\n    if gputouse is None:\n        gputouse_ls = MultiprocMixin.get_gpu_ids()\n    else:\n        gputouse_ls = [gputouse]\n    nprocs = len(gputouse_ls)\n    # Getting the experiments to run DLC on\n    exp_ls = self.get_experiments()\n    # If overwrite is False, filtering for only experiments that need processing\n    if not overwrite:\n        exp_ls = [\n            exp\n            for exp in exp_ls\n            if not os.path.isfile(exp.get_fp(Folders.DLC.value))\n        ]\n\n    # Running DLC on each batch of experiments with each GPU (given allocated GPU ID)\n    # TODO: have error handling\n    exp_batches_ls = np.array_split(exp_ls, nprocs)\n    with Pool(processes=nprocs) as p:\n        p.starmap(\n            RunDLC.ma_dlc_analyse_batch,\n            [\n                (\n                    [exp.get_fp(Folders.FORMATTED_VID.value) for exp in exp_batch],\n                    os.path.join(self.root_dir, Folders.DLC.value),\n                    os.path.join(self.root_dir, Folders.CONFIGS.value),\n                    os.path.join(self.root_dir, TEMP_DIR),\n                    gputouse,\n                    overwrite,\n                )\n                for gputouse, exp_batch in zip(gputouse_ls, exp_batches_ls)\n            ],\n        )\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.save_diagnostics",
      title: "<code>save_diagnostics(name, df)</code>",
      text: '<p>Writes the given data to a diagnostics file with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the diagnostics file to overwrite and open.</p> required <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame to write to the diagnostics file.</p> required Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>def save_diagnostics(self, name: str, df: pd.DataFrame) -&gt; None:\n    """\n    Writes the given data to a diagnostics file with the given name.\n\n    Parameters\n    ----------\n    name : str\n        The name of the diagnostics file to overwrite and open.\n    df : pd.DataFrame\n        The pandas DataFrame to write to the diagnostics file.\n    """\n    # Getting filepath\n    fp = os.path.join(self.root_dir, DIAGNOSTICS_DIR, f"{name}.csv")\n    # Writing to file\n    DiagnosticsMixin.save_diagnostics(df, fp)\n</code></pre>',
    },
    {
      location:
        "reference/project.html#behavysis_pipeline.pipeline.project.Project.update_configs",
      title: "<code>update_configs(*args, **kwargs)</code>",
      text: '<p>Batch processing corresponding to behavysis_pipeline.pipeline.experiment.Experiment.update_configs</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>tuple</code> <p>args passed to process scaffold method.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>keyword args passed to process scaffold method.</p> <code>{}</code> Source code in <code>behavysis_pipeline/pipeline/project.py</code> <pre><code>@functools.wraps(Experiment.update_configs)\ndef update_configs(self, *args, **kwargs) -&gt; None:\n    """\n    Batch processing corresponding to\n    [behavysis_pipeline.pipeline.experiment.Experiment.update_configs][]\n\n    Parameters\n    ----------\n    *args : tuple\n        args passed to process scaffold method.\n    **kwargs : dict\n        keyword args passed to process scaffold method.\n    """\n    method = Experiment.update_configs\n    self._process_scaffold(method, *args, **kwargs)\n</code></pre>',
    },
    {
      location: "tutorials/configs_json.html",
      title: "Configs JSON File",
      text: '<p>A configs JSON file is attached to each experiment. This file defines a) how the experiment should be processed (e.g. hyperparameters like the <code>dlc_config_fp</code> to use), and b) the inherent parameters of the experiment (e.g. like the <code>px/mm</code> and <code>start_frame</code> calculations).</p> <p>An example configs file is shown below:</p> <pre><code>{\n  "user": {\n    "format_vid": {\n      "height_px": 540,\n      "width_px": 960,\n      "fps": 15,\n      "start_sec": null,\n      "stop_sec": null\n    },\n    "run_dlc": {\n      "model_fp": "/path/to/dlc_config.yaml"\n    },\n    "calculate_params": {\n      "start_frame": {\n        "window_sec": 1,\n        "pcutoff": 0.9,\n        "bodyparts": "--bodyparts-simba"\n      },\n      "exp_dur": {\n        "window_sec": 1,\n        "pcutoff": 0.9,\n        "bodyparts": "--bodyparts-simba"\n      },\n      "stop_frame": {\n        "dur_sec": 6000\n      },\n      "px_per_mm": {\n        "pt_a": "--tl",\n        "pt_b": "--tr",\n        "dist_mm": 400\n      }\n    },\n    "preprocess": {\n      "interpolate": {\n        "pcutoff": 0.5\n      },\n      "bodycentre": {\n        "bodyparts": "--bodyparts-centre"\n      },\n      "refine_ids": {\n        "marked": "mouse1marked",\n        "unmarked": "mouse2unmarked",\n        "marking": "AnimalColourMark",\n        "window_sec": 0.5,\n        "metric": "rolling",\n        "bodyparts": "--bodyparts-centre"\n      }\n    },\n    "evaluate": {\n      "keypoints_plot": {\n        "bodyparts": ["Nose", "BodyCentre", "TailBase1"]\n      },\n      "eval_vid": {\n        "funcs": ["keypoints", "behavs"],\n        "pcutoff": 0.5,\n        "colour_level": "individuals",\n        "radius": 4,\n        "cmap": "rainbow"\n      }\n    },\n    "extract_features": {\n      "individuals": ["mouse1marked", "mouse2unmarked"],\n      "bodyparts": "--bodyparts-simba"\n    },\n    "classify_behaviours": [\n      {\n        "model_fp": "/path/to/behav_model_1.json",\n        "pcutoff": null,\n        "min_window_frames": "--min_window_frames",\n        "user_behavs": "--user_behavs"\n      },\n      {\n        "model_fp": "/path/to/behav_model_2.json",\n        "pcutoff": null,\n        "min_window_frames": "--min_window_frames",\n        "user_behavs": "--user_behavs"\n      }\n    ],\n    "analyse": {\n      "thigmotaxis": {\n        "thresh_mm": 50,\n        "roi_top_left": "--tl",\n        "roi_top_right": "--tr",\n        "roi_bottom_left": "--bl",\n        "roi_bottom_right": "--br",\n        "bodyparts": "--bodyparts-centre"\n      },\n      "center_crossing": {\n        "thresh_mm": 125,\n        "roi_top_left": "--tl",\n        "roi_top_right": "--tr",\n        "roi_bottom_left": "--bl",\n        "roi_bottom_right": "--br",\n        "bodyparts": "--bodyparts-centre"\n      },\n      "in_roi": {\n        "thresh_mm": 5,\n        "roi_top_left": "--tl",\n        "roi_top_right": "--tr",\n        "roi_bottom_left": "--bl",\n        "roi_bottom_right": "--br",\n        "bodyparts": ["Nose"]\n      },\n      "speed": {\n        "smoothing_sec": 1,\n        "bodyparts": "--bodyparts-centre"\n      },\n      "social_distance": {\n        "smoothing_sec": 1,\n        "bodyparts": "--bodyparts-centre"\n      },\n      "freezing": {\n        "window_sec": 2,\n        "thresh_mm": 5,\n        "smoothing_sec": 0.2,\n        "bodyparts": "--bodyparts-simba"\n      },\n      "bins_sec": [30, 60, 120, 300],\n      "custom_bins_sec": [60, 120, 300, 600]\n    }\n  },\n  "ref": {\n    "bodyparts-centre": [\n      "LeftFlankMid",\n      "BodyCentre",\n      "RightFlankMid",\n      "LeftFlankRear",\n      "RightFlankRear",\n      "TailBase1"\n    ],\n    "bodyparts-simba": [\n      "LeftEar",\n      "RightEar",\n      "Nose",\n      "BodyCentre",\n      "LeftFlankMid",\n      "RightFlankMid",\n      "TailBase1",\n      "TailTip4"\n    ],\n    "tl": "TopLeft",\n    "tr": "TopRight",\n    "bl": "BottomLeft",\n    "br": "BottomRight",\n    "min_window_frames": 2,\n    "user_behavs": ["fight", "aggression"]\n  }\n}\n</code></pre>',
    },
    {
      location: "tutorials/configs_json.html#the-structure",
      title: "The Structure",
      text: '<p>The configs file has three main sections - <code>user</code>: User defined parameters to process the experiment. - <code>auto</code>: Automatically calculated parameters which are used     in later processes for the experiment.     Also gives useful insights into how the experiment "went" (e.g. over/under time, arena is smaller than other videos). - <code>ref</code>: User defined parameters can be referenced from keys defined here.     Useful when the same parameter values are used for many processes     (e.g. bodyparts).</p>',
    },
    {
      location: "tutorials/configs_json.html#understanding-specific-parameters",
      title: "Understanding Specific Parameters",
      text: "<p>Notes</p> <p>To understand specific parameters in the <code>configs.yaml</code>, see each processing function's API documentation.</p> <p>For example, <code>user.calculate_params.px_per_mm</code> requires <code>pt_a</code>, <code>pt_b</code>, and <code>dist_mm</code>, which are described in the API docs.</p>",
    },
    {
      location: "tutorials/configs_json.html#the-ref-section",
      title: "The Ref section",
      text: '<p>The <code>ref</code> section defines values that can be referenced in the <code>user</code> section.</p> <p>To reference a value from the ref section, first define it:</p> <pre><code>{\n    ...\n    "ref": {\n        "example": ["values", "of", "any", "type"]\n    }\n}\n</code></pre> <p>You can now reference <code>example</code> by prepending a double hyphen (<code>--</code>) when referencing it:</p> <pre><code>{\n    "user": {\n        ...\n        "parameter": "--example",\n        ...\n    },\n    ...\n}\n</code></pre>',
    },
    {
      location:
        "tutorials/configs_json.html#setting-the-configs-for-an-experiment-or-all-experiments-in-a-project",
      title:
        "Setting the Configs for an Experiment or all Experiments in a Project",
      text: '<p>Each experiment requires a corresponding configs file.</p> <p>To generate or modify an experiment\'s configs file, first make a <code>default.json</code> file with the configs configured as you\'d like.</p> <p>Tip</p> <p>You can copy the example configs file from here.</p> <p>Just make sure to change the multiple <code>model_fp</code> filepaths.</p> <pre><code>from behavysis_pipeline import Experiment\n\n# Getting the experiment\nexperiment = Experiment("exp_name", "root_dir")\n# Making/overwriting the configs file\nexperiment.update_configs("/path/to/default.json", overwrite="all")\n</code></pre> <p>Note</p> <p>The <code>overwrite</code> keyword can be <code>"all"</code>, or <code>"user"</code>.</p>',
    },
    {
      location: "tutorials/diagnostics_messages.html",
      title: "Diagnostics Messages",
      text: "",
    },
    {
      location: "tutorials/diagnostics_messages.html#the-diagnostics-outputs",
      title: "The Diagnostics Outputs",
      text: "<p>After any process is run, a diagnostics file with the name of the process is generated in the diagnostics folder.</p> <p>An example of what this can look like is shown below.</p> <p></p>",
    },
    {
      location: "tutorials/diagnostics_messages.html#common-errors-and-warning",
      title: "Common Errors and Warning",
      text: '<p>In the diagnostics file, an error is something that has caused the processing of the experiment to fail completely. A warning is something that the program detected as unusual - it won\'t cause the program to fail but it is worth noting because it may affect future processes.</p> <p>Common warnings and errors that may arise are shown below, grouped by the process they usually occur in:</p> <ul> <li>updateConfigFiles</li> <li>The user-given value for <code>config_fp</code> may be an incorrect filepath or that cconfig file itself may not be in a valid JSON format (e.g., a bracket or a comma might be missing). You can use one of the configs templates.</li> <li>formatVideos</li> <li>The raw mp4 file may be missing or corrupted. Please try to open this file to see if this is the case.</li> <li>Sometimes, the resolution is an uncommon value and the downsampling fails. When this happens, the video is instead copied and the diagnostics file notes that the video failed to downsample and was copied instead.</li> <li>runDLC</li> <li>The specified dlc_config_path may be missing or incorrect. Please check that the correct filepath is used.</li> <li>calculateParams</li> <li>Throughout all the processes run within calculateParams, if any necessary parameter in the configs file are missing, then an error is added to the diagnostics file and the specific process stops. Future processes are usually affected by this error.</li> <li>getVideoMetadata<ul> <li>If the video file does not exist or is corrupted, then a warning is added the diagnostics file.</li> </ul> </li> <li>calcStartFrame<ul> <li>If the bodypart listed in the config file is not in the dlc_csv file, then a warning is thrown and the invalid bodypart is ignored.</li> <li>If the subject is not detected in any frames, then a warning is added to the diagnostics file. Please open and check the video to see if this is the case.</li> </ul> </li> <li>calcEndFrame<ul> <li>If the user-specified dur_sec (duration of the experiment in seconds) is larger than the length of the video, then a warning is added to the diagnostics file. Please open and check the video to see if this is the case.</li> </ul> </li> <li>calcPxPerMM<ul> <li>If the labels, "TopLeft", "TopRight", "BottomLeft", or "BottomRight", are missing from the dlc_csv file (i.e., if any of the box corners were not tracked), then an error message is added to the diagnostics file. Please ensure that you are using a DLC model that tracks the arena edges.</li> </ul> </li> <li>preprocessing</li> <li>Please note that some processes run within preprocessing depend on calculateParams. If there were any failed processes in calculateParams, please troubleshoot them before moving onto here.</li> <li>Throughout all the processes run within preprocessing, if any necessary parameter in the configs file are missing, then an error is added to the diagnostics file and the specific process stops. Future processes are usually affected by this error.</li> <li>InterpolatePoints:<ul> <li>If the bodypart listed in the config file is not in the dlc_csv file, then a warning is thrown and the invalid bodypart is ignored.</li> </ul> </li> <li>analysis</li> <li>Please note that most processes run within analysis depend on preprocessing. If there were any failed processes in calculateParams, please troubleshoot them before moving onto here.</li> <li>aggregateAnalysis</li> <li>The analysis process must be successfully run before the files generated from it can be aggregated.</li> </ul>',
    },
    {
      location: "tutorials/explanation.html",
      title: "Explanation",
      text: "<p>The behavysis_pipeline is used to analyse raw mp4 footage of lab mice. Analysis can include:</p> <ul> <li>Open Field</li> <li>Subject thigmotaxis</li> <li>Subject speed</li> <li>Choice (need to implement)</li> <li>etc.</li> </ul> <p>Converting raw mp4 footage to interpretable data and analysis involves the following steps:</p> <ol> <li>Setting up a BA project. This project will perform all the calculations to render analysises.</li> <li>Importing all the raw videos to into the project.</li> <li>Formatting all the raw videos so they can be interpreted by the DeepLabCut (DLC) pose estimation model.</li> <li>Run the formatted videos through the DLC pose estimation model to generate a video with the pose markers and a csv file of x-y coordinates of each marker (e.g., nose, left ear, right ear, body, front right foot, etc.).</li> <li>Preprocess the csv file of x-y marker coordinates so it is ready for analysis.</li> <li>Generate analysis from the preprocessed x-y marker coordinates file.</li> </ol> <p></p>",
    },
    {
      location: "tutorials/explanation.html#making-a-project-to-analyse",
      title: "Making a Project to Analyse",
      text: "<p>The experiment files must be stored in the computer in a certain way so the program can analyse them. Files pertaining to each experiment must be stored in folders with specific names. The overall way that the files are structured in a project are shown below.</p> <p></p> <p>For more information about how to set up experiment files, see setup</p>",
    },
    {
      location: "tutorials/setup.html",
      title: "Setup",
      text: "<p>Before running the behavysis_pipeline analysises, the files that we want to analyse must be set up a certain way for the behavysis_pipeline program to recognise them.</p> <p>There are three important guidelines to set up the project:</p> <ul> <li>Structure of files in folders .</li> <li>Experiment files.</li> <li>Config files for each experiment.</li> </ul>",
    },
    {
      location: "tutorials/setup.html#folder-structure",
      title: "Folder Structure",
      text: "<p>They need to be set up inside specially named folders, as shown below.</p> <p>An example of how this would look on a computer (in this case, a Mac) is shown below.</p>",
    },
    {
      location: "tutorials/setup.html#experiment-files",
      title: "Experiment Files",
      text: '<p>Each experiment must have files that have same name (not including the suffix like <code>.csv</code> or <code>.mp4</code>). An example is "day1_experiment1" must have all files named "day1_experiment1.mp4", "day1_experiment1.csv", "day1_experiment1.json" etc. stored in the corresponding folder.</p>',
    },
    {
      location: "tutorials/setup.html#config-files",
      title: "Config Files",
      text: "<p>The config file for an experiment stores all the parameters for how the experiment was recorded (e.g., the frames per second of the raw video, the experiment duration, etc.), and the parameters for how we want to process the data (e.g., the intended frames per second to format the video to, the DLC model to use to analyse, the likeliness pcutoff to interpolate points, etc.)</p> <p>An example of a config file is shown here.</p>",
    },
    {
      location: "tutorials/setup.html#running-behavysis_pipeline",
      title: "Running behavysis_pipeline",
      text: "<p>To install <code>behavysis_pipeline</code>, follow these instructions.</p> <p>To run <code>behavysis_pipeline</code>, follow these these instructions.</p>",
    },
  ],
};
